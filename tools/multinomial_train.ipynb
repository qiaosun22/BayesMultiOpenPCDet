{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dccef7d-d27c-490c-847f-741a53e85e3f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-21T07:55:47.472775Z",
     "iopub.status.busy": "2023-07-21T07:55:47.472506Z",
     "iopub.status.idle": "2023-07-21T07:55:58.346948Z",
     "shell.execute_reply": "2023-07-21T07:55:58.345694Z",
     "shell.execute_reply.started": "2023-07-21T07:55:47.472748Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### importing ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-21 15:55:57,544   INFO  **********************Start logging**********************\n",
      "2023-07-21 15:55:57,544   INFO  CUDA_VISIBLE_DEVICES=1\n",
      "2023-07-21 15:55:57,544   INFO  cfg_file         ./cfgs/kitti_models/pointpillar_bayes.yaml\n",
      "2023-07-21 15:55:57,545   INFO  batch_size       8\n",
      "2023-07-21 15:55:57,545   INFO  epochs           1\n",
      "2023-07-21 15:55:57,545   INFO  workers          64\n",
      "2023-07-21 15:55:57,546   INFO  extra_tag        multinomial\n",
      "2023-07-21 15:55:57,546   INFO  ckpt             ../output/cfgs/kitti_models/bayes/default/ckpt/checkpoint_epoch_10.pth\n",
      "2023-07-21 15:55:57,547   INFO  pretrained_model True\n",
      "2023-07-21 15:55:57,547   INFO  launcher         none\n",
      "2023-07-21 15:55:57,547   INFO  tcp_port         18888\n",
      "2023-07-21 15:55:57,548   INFO  sync_bn          False\n",
      "2023-07-21 15:55:57,548   INFO  fix_random_seed  True\n",
      "2023-07-21 15:55:57,548   INFO  ckpt_save_interval 1\n",
      "2023-07-21 15:55:57,549   INFO  local_rank       0\n",
      "2023-07-21 15:55:57,549   INFO  max_ckpt_save_num 99999\n",
      "2023-07-21 15:55:57,549   INFO  merge_all_iters_to_one_epoch False\n",
      "2023-07-21 15:55:57,550   INFO  set_cfgs         None\n",
      "2023-07-21 15:55:57,550   INFO  max_waiting_mins 0\n",
      "2023-07-21 15:55:57,550   INFO  start_epoch      0\n",
      "2023-07-21 15:55:57,551   INFO  save_to_file     False\n",
      "2023-07-21 15:55:57,551   INFO  cfg.ROOT_DIR: /mnt/workspace/sunqiao/OpenPCDet\n",
      "2023-07-21 15:55:57,551   INFO  cfg.LOCAL_RANK: 0\n",
      "2023-07-21 15:55:57,552   INFO  cfg.CLASS_NAMES: ['Car', 'Pedestrian', 'Cyclist']\n",
      "2023-07-21 15:55:57,552   INFO  ----------- DATA_CONFIG -----------\n",
      "2023-07-21 15:55:57,552   INFO  cfg.DATA_CONFIG.DATASET: KittiDataset\n",
      "2023-07-21 15:55:57,553   INFO  cfg.DATA_CONFIG.DATA_PATH: ../data/kitti\n",
      "2023-07-21 15:55:57,553   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [0, -39.68, -3, 69.12, 39.68, 1]\n",
      "2023-07-21 15:55:57,554   INFO  ----------- DATA_SPLIT -----------\n",
      "2023-07-21 15:55:57,554   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train\n",
      "2023-07-21 15:55:57,554   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val\n",
      "2023-07-21 15:55:57,555   INFO  ----------- INFO_PATH -----------\n",
      "2023-07-21 15:55:57,555   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['kitti_infos_train.pkl']\n",
      "2023-07-21 15:55:57,555   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['kitti_infos_val.pkl']\n",
      "2023-07-21 15:55:57,556   INFO  cfg.DATA_CONFIG.GET_ITEM_LIST: ['points']\n",
      "2023-07-21 15:55:57,556   INFO  cfg.DATA_CONFIG.FOV_POINTS_ONLY: True\n",
      "2023-07-21 15:55:57,556   INFO  ----------- DATA_AUGMENTOR -----------\n",
      "2023-07-21 15:55:57,557   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']\n",
      "2023-07-21 15:55:57,557   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'USE_ROAD_PLANE': True, 'DB_INFO_PATH': ['kitti_dbinfos_train.pkl'], 'PREPARE': {'filter_by_min_points': ['Car:5', 'Pedestrian:5', 'Cyclist:5'], 'filter_by_difficulty': [-1]}, 'SAMPLE_GROUPS': ['Car:15', 'Pedestrian:15', 'Cyclist:15'], 'NUM_POINT_FEATURES': 4, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': False}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.95, 1.05]}]\n",
      "2023-07-21 15:55:57,557   INFO  ----------- POINT_FEATURE_ENCODING -----------\n",
      "2023-07-21 15:55:57,558   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding\n",
      "2023-07-21 15:55:57,558   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity']\n",
      "2023-07-21 15:55:57,558   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity']\n",
      "2023-07-21 15:55:57,559   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': False}}, {'NAME': 'transform_points_to_voxels', 'VOXEL_SIZE': [0.16, 0.16, 4], 'MAX_POINTS_PER_VOXEL': 32, 'MAX_NUMBER_OF_VOXELS': {'train': 16000, 'test': 40000}}]\n",
      "2023-07-21 15:55:57,559   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/kitti_dataset.yaml\n",
      "2023-07-21 15:55:57,559   INFO  ----------- MODEL -----------\n",
      "2023-07-21 15:55:57,560   INFO  cfg.MODEL.NAME: PointPillar\n",
      "2023-07-21 15:55:57,560   INFO  ----------- VFE -----------\n",
      "2023-07-21 15:55:57,560   INFO  cfg.MODEL.VFE.NAME: PillarVFE\n",
      "2023-07-21 15:55:57,561   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False\n",
      "2023-07-21 15:55:57,561   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True\n",
      "2023-07-21 15:55:57,561   INFO  cfg.MODEL.VFE.USE_NORM: True\n",
      "2023-07-21 15:55:57,562   INFO  cfg.MODEL.VFE.USE_DO: True\n",
      "2023-07-21 15:55:57,562   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64]\n",
      "2023-07-21 15:55:57,562   INFO  ----------- MAP_TO_BEV -----------\n",
      "2023-07-21 15:55:57,563   INFO  cfg.MODEL.MAP_TO_BEV.NAME: PointPillarScatter\n",
      "2023-07-21 15:55:57,563   INFO  cfg.MODEL.MAP_TO_BEV.NUM_BEV_FEATURES: 64\n",
      "2023-07-21 15:55:57,563   INFO  ----------- BACKBONE_2D -----------\n",
      "2023-07-21 15:55:57,564   INFO  cfg.MODEL.BACKBONE_2D.NAME: BaseBEVBackbone\n",
      "2023-07-21 15:55:57,564   INFO  cfg.MODEL.BACKBONE_2D.USE_NORM: True\n",
      "2023-07-21 15:55:57,564   INFO  cfg.MODEL.BACKBONE_2D.USE_DO: True\n",
      "2023-07-21 15:55:57,565   INFO  cfg.MODEL.BACKBONE_2D.LAYER_NUMS: [3, 5, 5]\n",
      "2023-07-21 15:55:57,565   INFO  cfg.MODEL.BACKBONE_2D.LAYER_STRIDES: [2, 2, 2]\n",
      "2023-07-21 15:55:57,565   INFO  cfg.MODEL.BACKBONE_2D.NUM_FILTERS: [64, 128, 256]\n",
      "2023-07-21 15:55:57,566   INFO  cfg.MODEL.BACKBONE_2D.UPSAMPLE_STRIDES: [1, 2, 4]\n",
      "2023-07-21 15:55:57,566   INFO  cfg.MODEL.BACKBONE_2D.NUM_UPSAMPLE_FILTERS: [128, 128, 128]\n",
      "2023-07-21 15:55:57,567   INFO  ----------- DENSE_HEAD -----------\n",
      "2023-07-21 15:55:57,567   INFO  cfg.MODEL.DENSE_HEAD.NAME: AnchorHeadSingle\n",
      "2023-07-21 15:55:57,567   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False\n",
      "2023-07-21 15:55:57,567   INFO  cfg.MODEL.DENSE_HEAD.USE_DIRECTION_CLASSIFIER: True\n",
      "2023-07-21 15:55:57,568   INFO  cfg.MODEL.DENSE_HEAD.DIR_OFFSET: 0.78539\n",
      "2023-07-21 15:55:57,568   INFO  cfg.MODEL.DENSE_HEAD.DIR_LIMIT_OFFSET: 0.0\n",
      "2023-07-21 15:55:57,569   INFO  cfg.MODEL.DENSE_HEAD.NUM_DIR_BINS: 2\n",
      "2023-07-21 15:55:57,569   INFO  cfg.MODEL.DENSE_HEAD.ANCHOR_GENERATOR_CONFIG: [{'class_name': 'Car', 'anchor_sizes': [[3.9, 1.6, 1.56]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-1.78], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.6, 'unmatched_threshold': 0.45}, {'class_name': 'Pedestrian', 'anchor_sizes': [[0.8, 0.6, 1.73]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-0.6], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.5, 'unmatched_threshold': 0.35}, {'class_name': 'Cyclist', 'anchor_sizes': [[1.76, 0.6, 1.73]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-0.6], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.5, 'unmatched_threshold': 0.35}]\n",
      "2023-07-21 15:55:57,569   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------\n",
      "2023-07-21 15:55:57,570   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.NAME: AxisAlignedTargetAssigner\n",
      "2023-07-21 15:55:57,570   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.POS_FRACTION: -1.0\n",
      "2023-07-21 15:55:57,570   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.SAMPLE_SIZE: 512\n",
      "2023-07-21 15:55:57,571   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.NORM_BY_NUM_EXAMPLES: False\n",
      "2023-07-21 15:55:57,571   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MATCH_HEIGHT: False\n",
      "2023-07-21 15:55:57,571   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.BOX_CODER: ResidualCoder\n",
      "2023-07-21 15:55:57,572   INFO  ----------- LOSS_CONFIG -----------\n",
      "2023-07-21 15:55:57,572   INFO  ----------- LOSS_WEIGHTS -----------\n",
      "2023-07-21 15:55:57,573   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0\n",
      "2023-07-21 15:55:57,573   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.loc_weight: 2.0\n",
      "2023-07-21 15:55:57,573   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.dir_weight: 0.2\n",
      "2023-07-21 15:55:57,574   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "2023-07-21 15:55:57,574   INFO  ----------- POST_PROCESSING -----------\n",
      "2023-07-21 15:55:57,574   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]\n",
      "2023-07-21 15:55:57,575   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1\n",
      "2023-07-21 15:55:57,575   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False\n",
      "2023-07-21 15:55:57,575   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti\n",
      "2023-07-21 15:55:57,576   INFO  ----------- NMS_CONFIG -----------\n",
      "2023-07-21 15:55:57,576   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: False\n",
      "2023-07-21 15:55:57,577   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu\n",
      "2023-07-21 15:55:57,577   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.01\n",
      "2023-07-21 15:55:57,577   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 4096\n",
      "2023-07-21 15:55:57,578   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 500\n",
      "2023-07-21 15:55:57,578   INFO  ----------- OPTIMIZATION -----------\n",
      "2023-07-21 15:55:57,578   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 4\n",
      "2023-07-21 15:55:57,579   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 80\n",
      "2023-07-21 15:55:57,579   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle\n",
      "2023-07-21 15:55:57,579   INFO  cfg.OPTIMIZATION.LR: 0.003\n",
      "2023-07-21 15:55:57,580   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2023-07-21 15:55:57,580   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9\n",
      "2023-07-21 15:55:57,581   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]\n",
      "2023-07-21 15:55:57,581   INFO  cfg.OPTIMIZATION.PCT_START: 0.4\n",
      "2023-07-21 15:55:57,581   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10\n",
      "2023-07-21 15:55:57,582   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]\n",
      "2023-07-21 15:55:57,582   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1\n",
      "2023-07-21 15:55:57,582   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07\n",
      "2023-07-21 15:55:57,583   INFO  cfg.OPTIMIZATION.LR_WARMUP: False\n",
      "2023-07-21 15:55:57,583   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1\n",
      "2023-07-21 15:55:57,583   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10\n",
      "2023-07-21 15:55:57,584   INFO  cfg.TAG: pointpillar_bayes\n",
      "2023-07-21 15:55:57,584   INFO  cfg.EXP_GROUP_PATH: cfgs/kitti_models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 514\u001b[0m\n\u001b[1;32m    509\u001b[0m     eval_output_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    510\u001b[0m     args\u001b[38;5;241m.\u001b[39mstart_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Only evaluate the last 10 epochs\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     train_set, train_loader, train_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA_CONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAMES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmerge_all_iters_to_one_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_all_iters_to_one_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     test_set, test_loader, sampler \u001b[38;5;241m=\u001b[39m build_dataloader(\n\u001b[1;32m    526\u001b[0m                                     dataset_cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mDATA_CONFIG,\n\u001b[1;32m    527\u001b[0m                                     class_names\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mCLASS_NAMES,\n\u001b[1;32m    528\u001b[0m                                     batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    529\u001b[0m                                     dist\u001b[38;5;241m=\u001b[39mdist_train, workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers, logger\u001b[38;5;241m=\u001b[39mlogger, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    530\u001b[0m                                 )\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#     # -----------------------start training---------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/__init__.py:57\u001b[0m, in \u001b[0;36mbuild_dataloader\u001b[0;34m(dataset_cfg, class_names, batch_size, dist, root_path, workers, seed, logger, training, merge_all_iters_to_one_epoch, total_epochs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dataloader\u001b[39m(dataset_cfg, class_names, batch_size, dist, root_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m                      logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, merge_all_iters_to_one_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, total_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m__all__\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merge_all_iters_to_one_epoch:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge_all_iters_to_one_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/kitti/kitti_dataset.py:23\u001b[0m, in \u001b[0;36mKittiDataset.__init__\u001b[0;34m(self, dataset_cfg, class_names, training, root_path, logger)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_cfg, class_names, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, root_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        root_path:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        logger:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_cfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_cfg\u001b[38;5;241m.\u001b[39mDATA_SPLIT[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_split_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_path \u001b[38;5;241m/\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/dataset.py:31\u001b[0m, in \u001b[0;36mDatasetTemplate.__init__\u001b[0;34m(self, dataset_cfg, class_names, training, root_path, logger)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_cloud_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_cfg\u001b[38;5;241m.\u001b[39mPOINT_CLOUD_RANGE, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_feature_encoder \u001b[38;5;241m=\u001b[39m PointFeatureEncoder(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_cfg\u001b[38;5;241m.\u001b[39mPOINT_FEATURE_ENCODING,\n\u001b[1;32m     29\u001b[0m     point_cloud_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_cloud_range\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_augmentor \u001b[38;5;241m=\u001b[39m \u001b[43mDataAugmentor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA_AUGMENTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_processor \u001b[38;5;241m=\u001b[39m DataProcessor(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_cfg\u001b[38;5;241m.\u001b[39mDATA_PROCESSOR, point_cloud_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_cloud_range,\n\u001b[1;32m     36\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, num_point_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoint_feature_encoder\u001b[38;5;241m.\u001b[39mnum_point_features\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_processor\u001b[38;5;241m.\u001b[39mgrid_size\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/augmentor/data_augmentor.py:24\u001b[0m, in \u001b[0;36mDataAugmentor.__init__\u001b[0;34m(self, root_path, augmentor_configs, class_names, logger)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_cfg\u001b[38;5;241m.\u001b[39mNAME \u001b[38;5;129;01min\u001b[39;00m augmentor_configs\u001b[38;5;241m.\u001b[39mDISABLE_AUG_LIST:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m cur_augmentor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_augmentor_queue\u001b[38;5;241m.\u001b[39mappend(cur_augmentor)\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/augmentor/data_augmentor.py:40\u001b[0m, in \u001b[0;36mDataAugmentor.gt_sampling\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgt_sampling\u001b[39m(\u001b[38;5;28mself\u001b[39m, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 40\u001b[0m     db_sampler \u001b[38;5;241m=\u001b[39m \u001b[43mdatabase_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataBaseSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampler_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m db_sampler\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/datasets/augmentor/database_sampler.py:41\u001b[0m, in \u001b[0;36mDataBaseSampler.__init__\u001b[0;34m(self, root_path, sampler_cfg, class_names, logger)\u001b[0m\n\u001b[1;32m     38\u001b[0m         sampler_cfg\u001b[38;5;241m.\u001b[39mNUM_POINT_FEATURES \u001b[38;5;241m=\u001b[39m sampler_cfg\u001b[38;5;241m.\u001b[39mBACKUP_DB_INFO[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_POINT_FEATURES\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(db_info_path), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 41\u001b[0m         infos \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb_infos[cur_class]\u001b[38;5;241m.\u001b[39mextend(infos[cur_class]) \u001b[38;5;28;01mfor\u001b[39;00m cur_class \u001b[38;5;129;01min\u001b[39;00m class_names]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func_name, val \u001b[38;5;129;01min\u001b[39;00m sampler_cfg\u001b[38;5;241m.\u001b[39mPREPARE\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, train_loader, model_func, lr_scheduler, accumulated_iter, optim_cfg,\n",
    "                    rank, tbar, total_it_each_epoch, dataloader_iter, tb_log=None, leave_pbar=False):\n",
    "    if total_it_each_epoch == len(train_loader):\n",
    "        dataloader_iter = iter(train_loader)\n",
    "\n",
    "    if rank == 0:\n",
    "        pbar = tqdm.tqdm(total=total_it_each_epoch, leave=leave_pbar, desc='train', dynamic_ncols=True)\n",
    "\n",
    "    for cur_it in range(total_it_each_epoch):\n",
    "        try:\n",
    "            batch = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(train_loader)\n",
    "            batch = next(dataloader_iter)\n",
    "            print('new iters')\n",
    "\n",
    "#         print(len(dataloader_iter))\n",
    "\n",
    "#         print(len(dataloader_iter[0]))\n",
    "\n",
    "#         print(batch.keys())\n",
    "#         exit()\n",
    "\n",
    "        lr_scheduler.step(accumulated_iter)\n",
    "\n",
    "        try:\n",
    "            cur_lr = float(optimizer.lr)\n",
    "        except:\n",
    "            cur_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if tb_log is not None:\n",
    "            tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, tb_dict, disp_dict = model_func(model, batch)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), optim_cfg.GRAD_NORM_CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        accumulated_iter += 1\n",
    "        disp_dict.update({'loss': loss.item(), 'lr': cur_lr})\n",
    "\n",
    "        # log to console and tensorboard\n",
    "        if rank == 0:\n",
    "            pbar.update()\n",
    "            pbar.set_postfix(dict(total_it=accumulated_iter))\n",
    "            tbar.set_postfix(disp_dict)\n",
    "            tbar.refresh()\n",
    "\n",
    "            if tb_log is not None:\n",
    "                tb_log.add_scalar('train/loss', loss, accumulated_iter)\n",
    "                tb_log.add_scalar('meta_data/learning_rate', cur_lr, accumulated_iter)\n",
    "                for key, val in tb_dict.items():\n",
    "                    tb_log.add_scalar('train/' + key, val, accumulated_iter)\n",
    "    if rank == 0:\n",
    "        pbar.close()\n",
    "    return accumulated_iter\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "from noise import add_noise_to_weights\n",
    "import numba\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.models_multinomial import build_network, model_fn_decorator\n",
    "from pcdet.utils import common_utils\n",
    "from train_utils.optimization import build_optimizer, build_scheduler\n",
    "from train_utils.train_utils import train_model, model_save\n",
    "from eval_utils import eval_utils_multinomial\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import torch.optim.lr_scheduler as lr_sched\n",
    "\n",
    "# matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--cfg_file', type=str, default='./cfgs/kitti_models/pointpillar_bayes.yaml', \\\n",
    "                        help='specify the config for training')\n",
    "    # sunqiao/OpenPCDet/tools/cfgs/kitti_models/pointpillar_bayes.yaml\n",
    "    parser.add_argument('--batch_size', type=int, default=8, required=False, help='batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=1, required=False, help='number of epochs to train for')\n",
    "    parser.add_argument('--workers', type=int, default=64, help='number of workers for dataloader')\n",
    "    parser.add_argument('--extra_tag', type=str, default='multinomial', help='extra tag for this experiment')\n",
    "    parser.add_argument('--ckpt', type=str, default='../output/cfgs/kitti_models/bayes/default/ckpt/checkpoint_epoch_10.pth', \n",
    "                        help='checkpoint to start from')\n",
    "    \n",
    "    # ./checkpoint_epoch_80.pth\n",
    "    parser.add_argument('--pretrained_model', type=str, default=True, help='pretrained_model')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    parser.add_argument('--fix_random_seed', action='store_true', default=True, help='')\n",
    "    parser.add_argument('--ckpt_save_interval', type=int, default=1, help='number of training epochs')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')\n",
    "    parser.add_argument('--max_ckpt_save_num', type=int, default=99999, help='max number of saved checkpoint')\n",
    "    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "                        help='set extra config keys if needed')\n",
    "\n",
    "    parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    cfg_from_yaml_file(args.cfg_file, cfg)\n",
    "    cfg.TAG = Path(args.cfg_file).stem\n",
    "    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1: -1])  # remove 'cfgs' and 'xxxx.yaml'\n",
    "\n",
    "    if args.set_cfgs is not None:\n",
    "        cfg_from_list(args.set_cfgs, cfg)\n",
    "\n",
    "    return args, cfg\n",
    "\n",
    "class Opt():\n",
    "    def __init__(self, sigma, train_set, train_loader, train_sampler, test_set, test_loader, sampler):\n",
    "        self.sigma = sigma\n",
    "        # self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_loader = train_loader\n",
    "        self.train_sampler = train_sampler\n",
    "        self.test_set = test_set\n",
    "        self.test_loader = test_loader\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def opt_function(self, p1, p2, p3):\n",
    "        \n",
    "        sigma = self.sigma\n",
    "        # model = self.model\n",
    "        train_set = self.train_set\n",
    "        train_loader = self.train_loader\n",
    "        train_sampler = self.train_sampler\n",
    "        test_set = self.test_set\n",
    "        test_loader = self.test_loader\n",
    "        sampler = self.sampler\n",
    "        \n",
    "        model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), \n",
    "                            p1=p1, \n",
    "                            p2=p2, \n",
    "                            p3=p3,\n",
    "                            dataset=train_set)\n",
    "        model.cuda()\n",
    "        \n",
    "        optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "        model.load_params_with_optimizer(args.ckpt, to_cpu=dist, optimizer=optimizer, logger=logger)\n",
    "        \n",
    "        \n",
    "        global n\n",
    "        n += 1\n",
    "        \n",
    "        print(\"=============\")\n",
    "        print(p1, p2, p3)\n",
    "        print(\"=============\")\n",
    "\n",
    "        global best_accu\n",
    "\n",
    "        # p1 = round(p1, 2)\n",
    "        # p2 = round(p2, 2)\n",
    "\n",
    "#         train_set, train_loader, train_sampler = build_dataloader(\n",
    "#             dataset_cfg=cfg.DATA_CONFIG,\n",
    "#             class_names=cfg.CLASS_NAMES,\n",
    "#             batch_size=args.batch_size,\n",
    "#             dist=dist_train, workers=args.workers,\n",
    "#             logger=logger,\n",
    "#             training=True,\n",
    "#             merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "#             total_epochs=args.epochs\n",
    "#         )\n",
    "\n",
    "#         model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), \n",
    "#                             p1=p1, \n",
    "#                             p2=p2, \n",
    "#                             p3=p3,\n",
    "#                             dataset=train_set)\n",
    "#         model.cuda()\n",
    "        # print(model.state_dict())\n",
    "        # print(\"???????????\")\n",
    "\n",
    "        # if args.sync_bn:\n",
    "        #     model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        \n",
    "\n",
    "        \n",
    "        # # load checkpoint if it is possible\n",
    "        start_epoch = it = 0\n",
    "        last_epoch = -1\n",
    "        # if args.pretrained_model is True:\n",
    "        #     model.load_params_from_file(filename=args.ckpt, to_cpu=dist, logger=logger)\n",
    "\n",
    "        # if args.ckpt is not None:\n",
    "            # it, start_epoch = model.load_params_with_optimizer(args.ckpt, to_cpu=dist, optimizer=optimizer, logger=logger)\n",
    "            # last_epoch = start_epoch + 1\n",
    "            \n",
    "            \n",
    "        # model.load_params_with_optimizer(args.ckpt, to_cpu=dist, optimizer=optimizer, logger=logger)\n",
    "        \n",
    "        \n",
    "        # else:\n",
    "        #     ckpt_list = glob.glob(str(ckpt_dir / '*checkpoint_epoch_*.pth'))\n",
    "        #     if len(ckpt_list) > 0:\n",
    "        #         ckpt_list.sort(key=os.path.getmtime)\n",
    "        #         it, start_epoch = model.load_params_with_optimizer(\n",
    "        #             ckpt_list[-1], to_cpu=dist, optimizer=optimizer, logger=logger\n",
    "        #         )\n",
    "        #         last_epoch = start_epoch + 1\n",
    "\n",
    "        model.train()  # before wrap to DistributedDataParallel to support fixed some parameters\n",
    "        model.cuda()\n",
    "        # if dist_train:\n",
    "        #     model = nn.parallel.DistributedDataParallel(model, device_ids=[cfg.LOCAL_RANK % torch.cuda.device_count()])\n",
    "        # logger.info(model)\n",
    "\n",
    "#         lr_scheduler, lr_warmup_scheduler = build_scheduler(\n",
    "#             optimizer, total_iters_each_epoch=len(train_loader), total_epochs=args.epochs,\n",
    "#             last_epoch=last_epoch, optim_cfg=cfg.OPTIMIZATION\n",
    "#         )\n",
    "        \n",
    "        \n",
    "# def build_scheduler(optimizer, total_iters_each_epoch, total_epochs, last_epoch, optim_cfg):\n",
    "#     decay_steps = [x * total_iters_each_epoch for x in optim_cfg.DECAY_STEP_LIST]\n",
    "#     def lr_lbmd(cur_epoch):\n",
    "#         cur_decay = 1\n",
    "#         for decay_step in decay_steps:\n",
    "#             if cur_epoch >= decay_step:\n",
    "#                 cur_decay = cur_decay * optim_cfg.LR_DECAY\n",
    "#         return max(cur_decay, optim_cfg.LR_CLIP / optim_cfg.LR)\n",
    "\n",
    "        lr_warmup_scheduler = None\n",
    "        # total_steps = total_iters_each_epoch * total_epochs\n",
    "        # if optim_cfg.OPTIMIZER == 'adam_onecycle':\n",
    "        #     lr_scheduler = OneCycle(\n",
    "        #         optimizer, total_steps, optim_cfg.LR, list(optim_cfg.MOMS), optim_cfg.DIV_FACTOR, optim_cfg.PCT_START\n",
    "        #     )\n",
    "        # else:\n",
    "        # lr_scheduler = lr_sched.LambdaLR(optimizer, lr_lbmd, last_epoch=last_epoch)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 20, eta_min=0, last_epoch=-1)\n",
    "\n",
    "            # if optim_cfg.LR_WARMUP:\n",
    "            #     lr_warmup_scheduler = CosineWarmupLR(\n",
    "            #         optimizer, T_max=optim_cfg.WARMUP_EPOCH * len(total_iters_each_epoch),\n",
    "            #         eta_min=optim_cfg.LR / optim_cfg.DIV_FACTOR\n",
    "            #     )\n",
    "\n",
    "        # return lr_scheduler, lr_warmup_scheduler\n",
    "\n",
    "\n",
    "\n",
    "        # -----------------------start training---------------------------\n",
    "        logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                    % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "        \n",
    "        output_dir = cfg.ROOT_DIR / 'tools' / 'save_path' / args.extra_tag\n",
    "        ckpt_dir = output_dir / 'ckpt'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "        # print(ckpt_dir)\n",
    "        # ckpt_dir = './save_path/ckpts'\n",
    "        print(ckpt_dir)\n",
    "\n",
    "        # train_model(\n",
    "        #     model,\n",
    "        #     optimizer,\n",
    "        #     train_loader,\n",
    "        #     model_func=model_fn_decorator(),\n",
    "        #     lr_scheduler=lr_scheduler,\n",
    "        #     optim_cfg=cfg.OPTIMIZATION,\n",
    "        #     start_epoch=start_epoch,\n",
    "        #     total_epochs=args.epochs,\n",
    "        #     start_iter=it,\n",
    "        #     rank=cfg.LOCAL_RANK,\n",
    "        #     tb_log=tb_log,\n",
    "        #     ckpt_save_dir=ckpt_dir,\n",
    "        #     train_sampler=train_sampler,\n",
    "        #     lr_warmup_scheduler=lr_warmup_scheduler,\n",
    "        #     ckpt_save_interval=args.ckpt_save_interval,\n",
    "        #     max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        #     merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch\n",
    "        # )\n",
    "\n",
    "# def train_model(model, optimizer, train_loader, model_func, lr_scheduler, optim_cfg,\n",
    "#                 start_epoch, total_epochs, start_iter, rank, tb_log, ckpt_save_dir, train_sampler=None,\n",
    "#                 lr_warmup_scheduler=None, ckpt_save_interval=1, max_ckpt_save_num=50,\n",
    "#                 merge_all_iters_to_one_epoch=False):\n",
    "        model_func=model_fn_decorator(),\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        optim_cfg=cfg.OPTIMIZATION,\n",
    "        start_epoch=start_epoch,\n",
    "        total_epochs=args.epochs,\n",
    "        start_iter=it,\n",
    "        rank=cfg.LOCAL_RANK,\n",
    "        tb_log=tb_log,\n",
    "        ckpt_save_dir=ckpt_dir,\n",
    "        train_sampler=train_sampler,\n",
    "        lr_warmup_scheduler=lr_warmup_scheduler,\n",
    "        ckpt_save_interval=args.ckpt_save_interval,\n",
    "        max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch\n",
    "        accumulated_iter = start_iter\n",
    "        with tqdm.trange(start_epoch, total_epochs, desc='epochs', dynamic_ncols=True, leave=(rank == 0)) as tbar:\n",
    "            total_it_each_epoch = len(train_loader)\n",
    "            if merge_all_iters_to_one_epoch:\n",
    "                assert hasattr(train_loader.dataset, 'merge_all_iters_to_one_epoch')\n",
    "                train_loader.dataset.merge_all_iters_to_one_epoch(merge=True, epochs=total_epochs)\n",
    "                total_it_each_epoch = len(train_loader) // max(total_epochs, 1)\n",
    "\n",
    "            dataloader_iter = iter(train_loader)\n",
    "            for cur_epoch in tbar:\n",
    "                if train_sampler is not None:\n",
    "                    train_sampler.set_epoch(cur_epoch)\n",
    "\n",
    "                # train one epoch\n",
    "                if lr_warmup_scheduler is not None and cur_epoch < optim_cfg.WARMUP_EPOCH:\n",
    "                    cur_scheduler = lr_warmup_scheduler\n",
    "                else:\n",
    "                    cur_scheduler = lr_scheduler\n",
    "                accumulated_iter = train_one_epoch(\n",
    "                    model, optimizer, train_loader, model_func,\n",
    "                    lr_scheduler=cur_scheduler,\n",
    "                    accumulated_iter=accumulated_iter, optim_cfg=optim_cfg,\n",
    "                    rank=rank, tbar=tbar, tb_log=tb_log,\n",
    "                    leave_pbar=(cur_epoch + 1 == total_epochs),\n",
    "                    total_it_each_epoch=total_it_each_epoch,\n",
    "                    dataloader_iter=dataloader_iter\n",
    "                )\n",
    "\n",
    "        # save trained model\n",
    "        trained_epoch = cur_epoch + 1\n",
    "        if trained_epoch % ckpt_save_interval == 0 and rank == 0:\n",
    "\n",
    "            ckpt_list = glob.glob(str(ckpt_save_dir / 'checkpoint_epoch_*.pth'))\n",
    "            ckpt_list.sort(key=os.path.getmtime)\n",
    "\n",
    "            if ckpt_list.__len__() >= max_ckpt_save_num:\n",
    "                for cur_file_idx in range(0, len(ckpt_list) - max_ckpt_save_num + 1):\n",
    "                    os.remove(ckpt_list[cur_file_idx])\n",
    "\n",
    "            ckpt_name = ckpt_save_dir / ('checkpoint_epoch_%d' % trained_epoch)\n",
    "            save_checkpoint(\n",
    "                checkpoint_state(model, optimizer, trained_epoch, accumulated_iter), filename=ckpt_name,\n",
    "            )\n",
    "\n",
    "\n",
    "        # model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), p1=0.42, p2=0.23, p3=0.11, dataset=test_set)\n",
    "\n",
    "        # optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "\n",
    "        # if dist_train: \n",
    "        #     model = model.module\n",
    "\n",
    "        # if args.pretrained_model is True:\n",
    "        # model.load_params_from_file(filename=args.ckpt, to_cpu=dist, logger=logger)\n",
    "\n",
    "        # model.load_params_from_file(filename='./checkpoint_epoch_80.pth', logger=logger, to_cpu=dist_train)\n",
    "        # model.cuda()\n",
    "\n",
    "        ckpt_pth = save_path+'bayes_model-{}-{}-{}'.format(p1, p2, p3)\n",
    "        ckpt_name = ckpt_pth+'.pth'\n",
    "\n",
    "        # if cfg.LOCAL_RANK == 0:\n",
    "        #     model_save(model, ckpt_pth, optimizer, args.epochs, args.epochs)\n",
    "\n",
    "        logger.info('**********************End training**********************')\n",
    "\n",
    "        # time.sleep(30)\n",
    "\n",
    "\n",
    "\n",
    "        # if dist_train: \n",
    "        #     model = model.module\n",
    "\n",
    "        sigma = self.sigma\n",
    "        f = open(save_path+'result.txt', \"a+\")\n",
    "        # f.write('----------------Noise-{}-evaluate----------------'.format(sigma))\n",
    "        f.write('----------------{}-{}-{}---------------\\n'.format(p1, p2, p3))\n",
    "        f.close()\n",
    "\n",
    "        logger.info('---------------Epoch-{}-Noise-{}-evaluate----------------'.format(n, sigma))\n",
    "        # model.load_params_from_file(filename=ckpt_name, logger=logger, to_cpu=dist_train)\n",
    "        # model.cuda()\n",
    "        model = add_noise_to_weights(0, sigma, model)\n",
    "\n",
    "\n",
    "        acc1 = eval_utils_multinomial.eval_simple(args.ckpt, p1, p2, p3, sigma, n, cfg, model, test_loader, logger, save_path, dist_test=dist_train, save_to_file=args.save_to_file, result_dir=eval_output_dir)\n",
    "        print(\"----------\")\n",
    "        print(acc1)\n",
    "        print(\"----------\")\n",
    "\n",
    "\n",
    "        logger.info('**********************End evaluation**********************')\n",
    "\n",
    "            # best_accu = acc\n",
    "\n",
    "        return acc1  #+acc2+acc3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    torch.cuda.set_device(1)\n",
    "    # best_accu = 0\n",
    "\n",
    "    args, cfg = parse_config()\n",
    "    # if args.launcher == 'none':\n",
    "    dist_train = False\n",
    "    total_gpus = 1\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    print('Using GPU:' + str(np.argmax(memory_gpu)))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(np.argmax(memory_gpu))\n",
    "    os.system('rm tmp')\n",
    "    # else:\n",
    "    #     total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "    #         args.tcp_port, args.local_rank, backend='nccl'\n",
    "    #     )\n",
    "    #     dist_train = True\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "    else:\n",
    "        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "        args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "    if args.fix_random_seed:\n",
    "        common_utils.set_random_seed(666)\n",
    "\n",
    "    output_dir = cfg.ROOT_DIR / 'output' / cfg.EXP_GROUP_PATH / 'bayes' / args.extra_tag\n",
    "    ckpt_dir = output_dir / 'ckpt'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    save_path = './save_path/bayes/'#/bayes/pointpillar/'+time.strftime('%m%d-%H%M',time.localtime(time.time()))+'/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True) \n",
    "\n",
    "    logger = common_utils.create_logger(save_path+'log.txt', rank=cfg.LOCAL_RANK)\n",
    "\n",
    "    file = open(save_path+'result.txt','w')\n",
    "    file.write('results\\n')\n",
    "    file.close()\n",
    "\n",
    "    # head = ''\n",
    "    # logging.basicConfig(filename='./baseline/pointpillar/log.txt',\n",
    "    #                     format=head)\n",
    "    # logger_result = logging.getLogger()\n",
    "    # logger_result.setLevel(logging.INFO)\n",
    "    # console = logging.StreamHandler()\n",
    "    # logging.getLogger('').addHandler(console)\n",
    "\n",
    "    # log to file\n",
    "    logger.info('**********************Start logging**********************')\n",
    "    gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "    logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "    if dist_train:\n",
    "        logger.info('total_batch_size: %d' % (total_gpus * args.batch_size))\n",
    "    for key, val in vars(args).items():\n",
    "        logger.info('{:16} {}'.format(key, val))\n",
    "    log_config_to_file(cfg, logger=logger)\n",
    "    if cfg.LOCAL_RANK == 0:\n",
    "        os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "\n",
    "    tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "    eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "    eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    args.start_epoch = max(args.epochs - 10, 0)  # Only evaluate the last 10 epochs\n",
    "    \n",
    "\n",
    "\n",
    "    train_set, train_loader, train_sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        class_names=cfg.CLASS_NAMES,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers,\n",
    "        logger=logger,\n",
    "        training=True,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        total_epochs=args.epochs\n",
    "    )\n",
    "    \n",
    "    test_set, test_loader, sampler = build_dataloader(\n",
    "                                    dataset_cfg=cfg.DATA_CONFIG,\n",
    "                                    class_names=cfg.CLASS_NAMES,\n",
    "                                    batch_size=args.batch_size,\n",
    "                                    dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "                                )\n",
    "\n",
    "    \n",
    "#     # -----------------------start training---------------------------\n",
    "    logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "\n",
    "\n",
    "    logger.info('----------------Bayes Optimization----------------')\n",
    "    for sigma in np.linspace(1e-31, 1.0, 21):\n",
    "\n",
    "        # opt_function(0.11, 0.11)\n",
    "        print(\"=============\")\n",
    "        p1 = 0.42\n",
    "        p2 = 0.23\n",
    "        p3 = 0.11\n",
    "        print(p1, p2, p3)\n",
    "        print(\"=============\")\n",
    "        \n",
    "        opt= Opt(sigma, train_set, train_loader, train_sampler, test_set, test_loader, sampler)\n",
    "        opt_function = opt.opt_function\n",
    "        \n",
    "        # Bounded region of parameter space\n",
    "        pbounds = {'p1': (0.1, 0.9), 'p2': (0.1, 0.9), 'p3': (0.1, 0.9)}\n",
    "\n",
    "\n",
    "        optimizer_bayes = BayesianOptimization(\n",
    "            f=opt_function,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "            random_state=1,\n",
    "        )\n",
    "        optimizer_bayes.probe(\n",
    "            params={'p1': 0.68, 'p2': 0.77, 'p3': 0.23},\n",
    "            lazy=True,\n",
    "        )\n",
    "\n",
    "        logger_bayes = JSONLogger(path=save_path+\"logs2.json\")\n",
    "        optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger_bayes)\n",
    "\n",
    "\n",
    "        n = 0\n",
    "        optimizer_bayes.maximize(\n",
    "            init_points=3,\n",
    "            n_iter=1,\n",
    "        )\n",
    "    print(\"=======end========\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da2618-a281-45da-a5cd-cc7890e2fdec",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-21T07:55:58.348014Z",
     "iopub.status.idle": "2023-07-21T07:55:58.348300Z",
     "shell.execute_reply": "2023-07-21T07:55:58.348178Z",
     "shell.execute_reply.started": "2023-07-21T07:55:58.348164Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_noise_to_weights(mean, std, model):\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        if hasattr(m, 'weight'):\n",
    "            m.weight.add_(torch.randn(m.weight.size()) * 0.1)\n",
    "    \"\"\"\n",
    "    model = copy.deepcopy(model)\n",
    "    gassian_kernel = torch.distributions.Normal(mean, std)\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():                  \n",
    "            param.mul_(torch.exp(gassian_kernel.sample(param.size())).cuda())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6477c020-ffd5-4655-9b9a-0bc09fa55d8b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-21T07:55:58.349390Z",
     "iopub.status.idle": "2023-07-21T07:55:58.349924Z",
     "shell.execute_reply": "2023-07-21T07:55:58.349771Z",
     "shell.execute_reply.started": "2023-07-21T07:55:58.349756Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    class_names=cfg.CLASS_NAMES,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=dist_train, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs\n",
    ")\n",
    "\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "                                dataset_cfg=cfg.DATA_CONFIG,\n",
    "                                class_names=cfg.CLASS_NAMES,\n",
    "                                batch_size=args.batch_size,\n",
    "                                dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "                            )\n",
    "\n",
    "model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), \n",
    "                    p1=p1, \n",
    "                    p2=p2, \n",
    "                    p3=p3,\n",
    "                    dataset=train_set)\n",
    "\n",
    "optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "model.load_params_with_optimizer(args.ckpt, to_cpu=dist, optimizer=optimizer, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95292c-a1c8-4cf6-9f2e-bf3ad2f6081b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-21T07:55:58.350777Z",
     "iopub.status.idle": "2023-07-21T07:55:58.351284Z",
     "shell.execute_reply": "2023-07-21T07:55:58.351118Z",
     "shell.execute_reply.started": "2023-07-21T07:55:58.351102Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy.deepcopy\n",
    "import torch.nn\n",
    "# import copy\n",
    "# def cpmodel(model):\n",
    "#     model = copy.deepcopy(model)\n",
    "#     with torch.no_grad():\n",
    "#         for param in model.parameters():                  \n",
    "#             param.mul_(torch.tensor(2).cuda())\n",
    "            \n",
    "#     return model\n",
    "# model = nn.Linear(4,2).cuda()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "for i in model.parameters():\n",
    "    print(i)\n",
    "    break\n",
    "\n",
    "model1 = add_noise_to_weights(0.0, 0.3, model)\n",
    "\n",
    "# cpmodel(model)\n",
    "\n",
    "for i in model.parameters():\n",
    "    print(i)\n",
    "    break\n",
    "    \n",
    "for i in model1.parameters():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a4cb2-5c52-40dd-847d-a166b840f758",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-21T07:55:58.352330Z",
     "iopub.status.idle": "2023-07-21T07:55:58.352955Z",
     "shell.execute_reply": "2023-07-21T07:55:58.352807Z",
     "shell.execute_reply.started": "2023-07-21T07:55:58.352791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linspace(1e-31, 1.0, 21)[6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
