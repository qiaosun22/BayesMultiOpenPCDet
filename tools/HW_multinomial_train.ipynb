{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e48621f-a4ac-4a23-b6f0-553e0eb979d5",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2023-07-05T07:07:13.299578Z",
     "iopub.status.busy": "2023-07-05T07:07:13.299206Z",
     "iopub.status.idle": "2023-07-05T07:08:01.497100Z",
     "shell.execute_reply": "2023-07-05T07:08:01.495934Z",
     "shell.execute_reply.started": "2023-07-05T07:07:13.299560Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### importing ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 15:07:19,261   INFO  **********************Start logging**********************\n",
      "2023-07-05 15:07:19,262   INFO  CUDA_VISIBLE_DEVICES=0\n",
      "2023-07-05 15:07:19,262   INFO  cfg_file         cfgs/kitti_models/pointpillar.yaml\n",
      "2023-07-05 15:07:19,263   INFO  batch_size       4\n",
      "2023-07-05 15:07:19,263   INFO  epochs           80\n",
      "2023-07-05 15:07:19,264   INFO  workers          32\n",
      "2023-07-05 15:07:19,264   INFO  extra_tag        default\n",
      "2023-07-05 15:07:19,264   INFO  ckpt             checkpoint_epoch_80.pth\n",
      "2023-07-05 15:07:19,265   INFO  pretrained_model None\n",
      "2023-07-05 15:07:19,265   INFO  launcher         none\n",
      "2023-07-05 15:07:19,265   INFO  tcp_port         29051\n",
      "2023-07-05 15:07:19,266   INFO  sync_bn          False\n",
      "2023-07-05 15:07:19,266   INFO  fix_random_seed  True\n",
      "2023-07-05 15:07:19,266   INFO  ckpt_save_interval 1\n",
      "2023-07-05 15:07:19,267   INFO  local_rank       0\n",
      "2023-07-05 15:07:19,267   INFO  max_ckpt_save_num 30\n",
      "2023-07-05 15:07:19,268   INFO  merge_all_iters_to_one_epoch False\n",
      "2023-07-05 15:07:19,268   INFO  set_cfgs         None\n",
      "2023-07-05 15:07:19,268   INFO  max_waiting_mins 0\n",
      "2023-07-05 15:07:19,269   INFO  start_epoch      0\n",
      "2023-07-05 15:07:19,269   INFO  save_to_file     False\n",
      "2023-07-05 15:07:19,270   INFO  cfg.ROOT_DIR: /mnt/workspace/sunqiao/OpenPCDet\n",
      "2023-07-05 15:07:19,270   INFO  cfg.LOCAL_RANK: 0\n",
      "2023-07-05 15:07:19,270   INFO  cfg.CLASS_NAMES: ['Car', 'Pedestrian', 'Cyclist']\n",
      "2023-07-05 15:07:19,271   INFO  ----------- DATA_CONFIG -----------\n",
      "2023-07-05 15:07:19,271   INFO  cfg.DATA_CONFIG.DATASET: KittiDataset\n",
      "2023-07-05 15:07:19,271   INFO  cfg.DATA_CONFIG.DATA_PATH: ../data/kitti\n",
      "2023-07-05 15:07:19,272   INFO  cfg.DATA_CONFIG.POINT_CLOUD_RANGE: [0, -39.68, -3, 69.12, 39.68, 1]\n",
      "2023-07-05 15:07:19,272   INFO  ----------- DATA_SPLIT -----------\n",
      "2023-07-05 15:07:19,272   INFO  cfg.DATA_CONFIG.DATA_SPLIT.train: train\n",
      "2023-07-05 15:07:19,273   INFO  cfg.DATA_CONFIG.DATA_SPLIT.test: val\n",
      "2023-07-05 15:07:19,273   INFO  ----------- INFO_PATH -----------\n",
      "2023-07-05 15:07:19,274   INFO  cfg.DATA_CONFIG.INFO_PATH.train: ['kitti_infos_train.pkl']\n",
      "2023-07-05 15:07:19,274   INFO  cfg.DATA_CONFIG.INFO_PATH.test: ['kitti_infos_val.pkl']\n",
      "2023-07-05 15:07:19,274   INFO  cfg.DATA_CONFIG.GET_ITEM_LIST: ['points']\n",
      "2023-07-05 15:07:19,275   INFO  cfg.DATA_CONFIG.FOV_POINTS_ONLY: True\n",
      "2023-07-05 15:07:19,275   INFO  ----------- DATA_AUGMENTOR -----------\n",
      "2023-07-05 15:07:19,275   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.DISABLE_AUG_LIST: ['placeholder']\n",
      "2023-07-05 15:07:19,276   INFO  cfg.DATA_CONFIG.DATA_AUGMENTOR.AUG_CONFIG_LIST: [{'NAME': 'gt_sampling', 'USE_ROAD_PLANE': True, 'DB_INFO_PATH': ['kitti_dbinfos_train.pkl'], 'PREPARE': {'filter_by_min_points': ['Car:5', 'Pedestrian:5', 'Cyclist:5'], 'filter_by_difficulty': [-1]}, 'SAMPLE_GROUPS': ['Car:15', 'Pedestrian:15', 'Cyclist:15'], 'NUM_POINT_FEATURES': 4, 'DATABASE_WITH_FAKELIDAR': False, 'REMOVE_EXTRA_WIDTH': [0.0, 0.0, 0.0], 'LIMIT_WHOLE_SCENE': False}, {'NAME': 'random_world_flip', 'ALONG_AXIS_LIST': ['x']}, {'NAME': 'random_world_rotation', 'WORLD_ROT_ANGLE': [-0.78539816, 0.78539816]}, {'NAME': 'random_world_scaling', 'WORLD_SCALE_RANGE': [0.95, 1.05]}]\n",
      "2023-07-05 15:07:19,276   INFO  ----------- POINT_FEATURE_ENCODING -----------\n",
      "2023-07-05 15:07:19,276   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.encoding_type: absolute_coordinates_encoding\n",
      "2023-07-05 15:07:19,277   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.used_feature_list: ['x', 'y', 'z', 'intensity']\n",
      "2023-07-05 15:07:19,277   INFO  cfg.DATA_CONFIG.POINT_FEATURE_ENCODING.src_feature_list: ['x', 'y', 'z', 'intensity']\n",
      "2023-07-05 15:07:19,277   INFO  cfg.DATA_CONFIG.DATA_PROCESSOR: [{'NAME': 'mask_points_and_boxes_outside_range', 'REMOVE_OUTSIDE_BOXES': True}, {'NAME': 'shuffle_points', 'SHUFFLE_ENABLED': {'train': True, 'test': False}}, {'NAME': 'transform_points_to_voxels', 'VOXEL_SIZE': [0.16, 0.16, 4], 'MAX_POINTS_PER_VOXEL': 32, 'MAX_NUMBER_OF_VOXELS': {'train': 16000, 'test': 40000}}]\n",
      "2023-07-05 15:07:19,278   INFO  cfg.DATA_CONFIG._BASE_CONFIG_: cfgs/dataset_configs/kitti_dataset.yaml\n",
      "2023-07-05 15:07:19,278   INFO  ----------- MODEL -----------\n",
      "2023-07-05 15:07:19,279   INFO  cfg.MODEL.NAME: PointPillar\n",
      "2023-07-05 15:07:19,279   INFO  ----------- VFE -----------\n",
      "2023-07-05 15:07:19,279   INFO  cfg.MODEL.VFE.NAME: PillarVFE\n",
      "2023-07-05 15:07:19,280   INFO  cfg.MODEL.VFE.WITH_DISTANCE: False\n",
      "2023-07-05 15:07:19,280   INFO  cfg.MODEL.VFE.USE_ABSLOTE_XYZ: True\n",
      "2023-07-05 15:07:19,280   INFO  cfg.MODEL.VFE.USE_NORM: True\n",
      "2023-07-05 15:07:19,281   INFO  cfg.MODEL.VFE.USE_DO: False\n",
      "2023-07-05 15:07:19,281   INFO  cfg.MODEL.VFE.NUM_FILTERS: [64]\n",
      "2023-07-05 15:07:19,282   INFO  ----------- MAP_TO_BEV -----------\n",
      "2023-07-05 15:07:19,282   INFO  cfg.MODEL.MAP_TO_BEV.NAME: PointPillarScatter\n",
      "2023-07-05 15:07:19,282   INFO  cfg.MODEL.MAP_TO_BEV.NUM_BEV_FEATURES: 64\n",
      "2023-07-05 15:07:19,283   INFO  ----------- BACKBONE_2D -----------\n",
      "2023-07-05 15:07:19,283   INFO  cfg.MODEL.BACKBONE_2D.NAME: BaseBEVBackbone\n",
      "2023-07-05 15:07:19,283   INFO  cfg.MODEL.BACKBONE_2D.USE_NORM: True\n",
      "2023-07-05 15:07:19,284   INFO  cfg.MODEL.BACKBONE_2D.USE_DO: False\n",
      "2023-07-05 15:07:19,284   INFO  cfg.MODEL.BACKBONE_2D.LAYER_NUMS: [3, 5, 5]\n",
      "2023-07-05 15:07:19,284   INFO  cfg.MODEL.BACKBONE_2D.LAYER_STRIDES: [2, 2, 2]\n",
      "2023-07-05 15:07:19,285   INFO  cfg.MODEL.BACKBONE_2D.NUM_FILTERS: [64, 128, 256]\n",
      "2023-07-05 15:07:19,285   INFO  cfg.MODEL.BACKBONE_2D.UPSAMPLE_STRIDES: [1, 2, 4]\n",
      "2023-07-05 15:07:19,286   INFO  cfg.MODEL.BACKBONE_2D.NUM_UPSAMPLE_FILTERS: [128, 128, 128]\n",
      "2023-07-05 15:07:19,286   INFO  ----------- DENSE_HEAD -----------\n",
      "2023-07-05 15:07:19,286   INFO  cfg.MODEL.DENSE_HEAD.NAME: AnchorHeadSingle\n",
      "2023-07-05 15:07:19,286   INFO  cfg.MODEL.DENSE_HEAD.CLASS_AGNOSTIC: False\n",
      "2023-07-05 15:07:19,287   INFO  cfg.MODEL.DENSE_HEAD.USE_DIRECTION_CLASSIFIER: True\n",
      "2023-07-05 15:07:19,287   INFO  cfg.MODEL.DENSE_HEAD.DIR_OFFSET: 0.78539\n",
      "2023-07-05 15:07:19,288   INFO  cfg.MODEL.DENSE_HEAD.DIR_LIMIT_OFFSET: 0.0\n",
      "2023-07-05 15:07:19,288   INFO  cfg.MODEL.DENSE_HEAD.NUM_DIR_BINS: 2\n",
      "2023-07-05 15:07:19,288   INFO  cfg.MODEL.DENSE_HEAD.ANCHOR_GENERATOR_CONFIG: [{'class_name': 'Car', 'anchor_sizes': [[3.9, 1.6, 1.56]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-1.78], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.6, 'unmatched_threshold': 0.45}, {'class_name': 'Pedestrian', 'anchor_sizes': [[0.8, 0.6, 1.73]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-0.6], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.5, 'unmatched_threshold': 0.35}, {'class_name': 'Cyclist', 'anchor_sizes': [[1.76, 0.6, 1.73]], 'anchor_rotations': [0, 1.57], 'anchor_bottom_heights': [-0.6], 'align_center': False, 'feature_map_stride': 2, 'matched_threshold': 0.5, 'unmatched_threshold': 0.35}]\n",
      "2023-07-05 15:07:19,289   INFO  ----------- TARGET_ASSIGNER_CONFIG -----------\n",
      "2023-07-05 15:07:19,289   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.NAME: AxisAlignedTargetAssigner\n",
      "2023-07-05 15:07:19,289   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.POS_FRACTION: -1.0\n",
      "2023-07-05 15:07:19,290   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.SAMPLE_SIZE: 512\n",
      "2023-07-05 15:07:19,290   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.NORM_BY_NUM_EXAMPLES: False\n",
      "2023-07-05 15:07:19,291   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.MATCH_HEIGHT: False\n",
      "2023-07-05 15:07:19,291   INFO  cfg.MODEL.DENSE_HEAD.TARGET_ASSIGNER_CONFIG.BOX_CODER: ResidualCoder\n",
      "2023-07-05 15:07:19,291   INFO  ----------- LOSS_CONFIG -----------\n",
      "2023-07-05 15:07:19,292   INFO  ----------- LOSS_WEIGHTS -----------\n",
      "2023-07-05 15:07:19,292   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.cls_weight: 1.0\n",
      "2023-07-05 15:07:19,292   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.loc_weight: 2.0\n",
      "2023-07-05 15:07:19,293   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.dir_weight: 0.2\n",
      "2023-07-05 15:07:19,293   INFO  cfg.MODEL.DENSE_HEAD.LOSS_CONFIG.LOSS_WEIGHTS.code_weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "2023-07-05 15:07:19,293   INFO  ----------- POST_PROCESSING -----------\n",
      "2023-07-05 15:07:19,294   INFO  cfg.MODEL.POST_PROCESSING.RECALL_THRESH_LIST: [0.3, 0.5, 0.7]\n",
      "2023-07-05 15:07:19,294   INFO  cfg.MODEL.POST_PROCESSING.SCORE_THRESH: 0.1\n",
      "2023-07-05 15:07:19,295   INFO  cfg.MODEL.POST_PROCESSING.OUTPUT_RAW_SCORE: False\n",
      "2023-07-05 15:07:19,295   INFO  cfg.MODEL.POST_PROCESSING.EVAL_METRIC: kitti\n",
      "2023-07-05 15:07:19,295   INFO  ----------- NMS_CONFIG -----------\n",
      "2023-07-05 15:07:19,296   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.MULTI_CLASSES_NMS: False\n",
      "2023-07-05 15:07:19,296   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_TYPE: nms_gpu\n",
      "2023-07-05 15:07:19,296   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_THRESH: 0.01\n",
      "2023-07-05 15:07:19,297   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_PRE_MAXSIZE: 4096\n",
      "2023-07-05 15:07:19,297   INFO  cfg.MODEL.POST_PROCESSING.NMS_CONFIG.NMS_POST_MAXSIZE: 500\n",
      "2023-07-05 15:07:19,298   INFO  ----------- OPTIMIZATION -----------\n",
      "2023-07-05 15:07:19,298   INFO  cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU: 4\n",
      "2023-07-05 15:07:19,298   INFO  cfg.OPTIMIZATION.NUM_EPOCHS: 80\n",
      "2023-07-05 15:07:19,299   INFO  cfg.OPTIMIZATION.OPTIMIZER: adam_onecycle\n",
      "2023-07-05 15:07:19,299   INFO  cfg.OPTIMIZATION.LR: 0.003\n",
      "2023-07-05 15:07:19,299   INFO  cfg.OPTIMIZATION.WEIGHT_DECAY: 0.01\n",
      "2023-07-05 15:07:19,300   INFO  cfg.OPTIMIZATION.MOMENTUM: 0.9\n",
      "2023-07-05 15:07:19,300   INFO  cfg.OPTIMIZATION.MOMS: [0.95, 0.85]\n",
      "2023-07-05 15:07:19,300   INFO  cfg.OPTIMIZATION.PCT_START: 0.4\n",
      "2023-07-05 15:07:19,301   INFO  cfg.OPTIMIZATION.DIV_FACTOR: 10\n",
      "2023-07-05 15:07:19,301   INFO  cfg.OPTIMIZATION.DECAY_STEP_LIST: [35, 45]\n",
      "2023-07-05 15:07:19,301   INFO  cfg.OPTIMIZATION.LR_DECAY: 0.1\n",
      "2023-07-05 15:07:19,302   INFO  cfg.OPTIMIZATION.LR_CLIP: 1e-07\n",
      "2023-07-05 15:07:19,302   INFO  cfg.OPTIMIZATION.LR_WARMUP: False\n",
      "2023-07-05 15:07:19,302   INFO  cfg.OPTIMIZATION.WARMUP_EPOCH: 1\n",
      "2023-07-05 15:07:19,303   INFO  cfg.OPTIMIZATION.GRAD_NORM_CLIP: 10\n",
      "2023-07-05 15:07:19,303   INFO  cfg.TAG: pointpillar\n",
      "2023-07-05 15:07:19,304   INFO  cfg.EXP_GROUP_PATH: kitti_models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-05 15:07:19.210503\n",
      "Using GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 15:07:19,423   INFO  Database filter by min points Car: 14357 => 13532\n",
      "2023-07-05 15:07:19,424   INFO  Database filter by min points Pedestrian: 2207 => 2168\n",
      "2023-07-05 15:07:19,425   INFO  Database filter by min points Cyclist: 734 => 705\n",
      "2023-07-05 15:07:19,440   INFO  Database filter by difficulty Car: 13532 => 10759\n",
      "2023-07-05 15:07:19,442   INFO  Database filter by difficulty Pedestrian: 2168 => 2075\n",
      "2023-07-05 15:07:19,443   INFO  Database filter by difficulty Cyclist: 705 => 581\n",
      "2023-07-05 15:07:19,447   INFO  Loading KITTI dataset\n",
      "2023-07-05 15:07:19,530   INFO  Total samples for KITTI dataset: 3712\n",
      "2023-07-05 15:07:19,531   INFO  **********************Start evaluation kitti_models/pointpillar(default)**********************\n",
      "2023-07-05 15:07:19,533   INFO  Loading KITTI dataset\n",
      "2023-07-05 15:07:19,616   INFO  Total samples for KITTI dataset: 3769\n",
      "2023-07-05 15:07:19,618   INFO  ----------------Noise Experiment----------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I-V_data_2um_length_200nm_diameter_NA_third_etch_18min_Pb_ED_3h_180C_MABr_no_200nm_Ag_memory_8V_probe.xlsx\n",
      "=============\n",
      "0.42 0.23 0.11\n",
      "=============\n",
      "#################### imported! ##########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2023-07-05 15:07:21,117   INFO  ==> Loading parameters from checkpoint checkpoint_epoch_80.pth to CPU\n",
      "2023-07-05 15:07:21,196   INFO  ==> Loading optimizer parameters from checkpoint checkpoint_epoch_80.pth to CPU\n",
      "2023-07-05 15:07:21,203   INFO  ==> Done\n",
      "2023-07-05 15:07:21,205   INFO  **********************Start training kitti_models/pointpillar(default)**********************\n",
      "2023-07-05 15:07:21,206   INFO  **********************End training**********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Checkpoint trained from version: pcdet+0.3.0+26a1612\n",
      "=============\n",
      "0.42 0.23 0.11\n",
      "=============\n",
      "/mnt/workspace/sunqiao/OpenPCDet/tools/save_path/default/ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 15:07:51,237   INFO  ---------------Epoch-1-Noise-I-V_data_2um_length_200nm_diameter_NA_third_etch_18min_Pb_ED_3h_180C_MABr_no_200nm_Ag_memory_8V_probe.xlsx-evaluate----------------\n",
      "eval:   0%|          | 0/3769 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 353\u001b[0m\n\u001b[1;32m    349\u001b[0m         optimizer_bayes\u001b[38;5;241m.\u001b[39msubscribe(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP, logger_bayes)\n\u001b[1;32m    352\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 353\u001b[0m         \u001b[43moptimizer_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m            \u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**********************End evaluation \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)**********************\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constraint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[1], line 187\u001b[0m, in \u001b[0;36mOpt.opt_function\u001b[0;34m(self, p1, p2, p3)\u001b[0m\n\u001b[1;32m    184\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m---------------Epoch-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-Noise-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-evaluate----------------\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n, f_name))\n\u001b[1;32m    185\u001b[0m model \u001b[38;5;241m=\u001b[39m add_noise_to_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./hardware_noise/hardware_data/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mf_name, \u001b[38;5;241m0\u001b[39m, f_name, model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 187\u001b[0m acc1 \u001b[38;5;241m=\u001b[39m \u001b[43meval_utils_multinomial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc1)\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/tools/eval_utils/eval_utils_multinomial.py:164\u001b[0m, in \u001b[0;36meval_simple\u001b[0;34m(ckpt, p1, p2, p3, f, n, cfg, model, dataloader, logger, save_path, dist_test, save_to_file, result_dir)\u001b[0m\n\u001b[1;32m    162\u001b[0m load_data_to_gpu(batch_dict)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 164\u001b[0m     pred_dicts, ret_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m disp_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    167\u001b[0m statistics_info(cfg, ret_dict, metric, disp_dict)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/models_multinomial/detectors/pointpillar.py:12\u001b[0m, in \u001b[0;36mPointPillar.forward\u001b[0;34m(self, batch_dict)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_dict):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cur_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_list:\n\u001b[0;32m---> 12\u001b[0m         batch_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcur_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     15\u001b[0m         loss, tb_dict, disp_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_training_loss()\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/pcdet/models_multinomial/dense_heads/anchor_head_single.py:68\u001b[0m, in \u001b[0;36mAnchorHeadSingle.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_ret_dict\u001b[38;5;241m.\u001b[39mupdate(targets_dict)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_boxes_when_training:\n\u001b[1;32m     67\u001b[0m     batch_cls_preds, batch_box_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_predicted_boxes(\n\u001b[0;32m---> 68\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mdata_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     69\u001b[0m         cls_preds\u001b[38;5;241m=\u001b[39mcls_preds, box_preds\u001b[38;5;241m=\u001b[39mbox_preds, dir_cls_preds\u001b[38;5;241m=\u001b[39mdir_cls_preds\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_cls_preds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_cls_preds\n\u001b[1;32m     72\u001b[0m     data_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_box_preds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m batch_box_preds\n",
      "\u001b[0;31mKeyError\u001b[0m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "from noise import add_noise_to_weights\n",
    "import numba\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.models_multinomial import build_network, model_fn_decorator\n",
    "from pcdet.utils import common_utils\n",
    "from train_utils.optimization import build_optimizer, build_scheduler\n",
    "from train_utils.train_utils import train_model, model_save\n",
    "from eval_utils import eval_utils_multinomial\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from hardware_noise.new_weight_mapping import weights_mapping as add_noise_to_weights\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--cfg_file', type=str, default='cfgs/kitti_models/pointpillar.yaml', \\\n",
    "                        help='specify the config for training')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "    parser.add_argument('--workers', type=int, default=32, help='number of workers for dataloader')\n",
    "    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    parser.add_argument('--ckpt', type=str, default='checkpoint_epoch_80.pth', \\\n",
    "                        help='checkpoint to start from')\n",
    "    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    parser.add_argument('--tcp_port', type=int, default=29051, help='tcp port for distrbuted training')\n",
    "    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    parser.add_argument('--fix_random_seed', action='store_true', default=True, help='')\n",
    "    parser.add_argument('--ckpt_save_interval', type=int, default=1, help='number of training epochs')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')\n",
    "    parser.add_argument('--max_ckpt_save_num', type=int, default=30, help='max number of saved checkpoint')\n",
    "    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "                        help='set extra config keys if needed')\n",
    "\n",
    "    parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "    \n",
    "    # parser.add_argument('--local_rank', type=int, default=-1, metavar='N', help='Local process rank.')\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    cfg_from_yaml_file(args.cfg_file, cfg)\n",
    "    cfg.TAG = Path(args.cfg_file).stem\n",
    "    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])  # remove 'cfgs' and 'xxxx.yaml'\n",
    "\n",
    "    if args.set_cfgs is not None:\n",
    "        cfg_from_list(args.set_cfgs, cfg)\n",
    "\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "args, cfg = parse_config()\n",
    "\n",
    "\n",
    "class Opt():\n",
    "    def __init__(self, f_name, train_set, train_loader, train_sampler, test_set, test_loader, sampler):\n",
    "        self.f_name = f_name\n",
    "        # self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.train_loader = train_loader\n",
    "        self.train_sampler = train_sampler\n",
    "        self.test_set = test_set\n",
    "        self.test_loader = test_loader\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def opt_function(self, p1, p2, p3):\n",
    "        # f_name = self.f_name\n",
    "        # model = self.model\n",
    "        train_set = self.train_set\n",
    "        train_loader = self.train_loader\n",
    "        train_sampler = self.train_sampler\n",
    "        test_set = self.test_set\n",
    "        test_loader = self.test_loader\n",
    "        sampler = self.sampler\n",
    "        \n",
    "        model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), \n",
    "                            p1=p1, \n",
    "                            p2=p2, \n",
    "                            p3=p3,\n",
    "                            dataset=train_set)\n",
    "        model.cuda()\n",
    "        \n",
    "        optimizer = build_optimizer(model, cfg.OPTIMIZATION)\n",
    "        model.load_params_with_optimizer(args.ckpt, to_cpu=dist, optimizer=optimizer, logger=logger)\n",
    "        \n",
    "        \n",
    "        global n\n",
    "        n += 1\n",
    "        \n",
    "        print(\"=============\")\n",
    "        print(p1, p2, p3)\n",
    "        print(\"=============\")\n",
    "\n",
    "        global best_accu\n",
    "        \n",
    "        # # load checkpoint if it is possible\n",
    "        start_epoch = it = 0\n",
    "        last_epoch = -1\n",
    "\n",
    "        model.train()  # before wrap to DistributedDataParallel to support fixed some parameters\n",
    "        model.cuda()\n",
    "\n",
    "        lr_scheduler, lr_warmup_scheduler = build_scheduler(\n",
    "            optimizer, total_iters_each_epoch=len(train_loader), total_epochs=args.epochs,\n",
    "            last_epoch=last_epoch, optim_cfg=cfg.OPTIMIZATION\n",
    "        )\n",
    "\n",
    "        # -----------------------start training---------------------------\n",
    "        logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                    % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "        \n",
    "        output_dir = cfg.ROOT_DIR / 'tools' / 'save_path' / args.extra_tag\n",
    "        ckpt_dir = output_dir / 'ckpt'\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(ckpt_dir)\n",
    "\n",
    "        # train_model(\n",
    "        #     model,\n",
    "        #     optimizer,\n",
    "        #     train_loader,\n",
    "        #     model_func=model_fn_decorator(),\n",
    "        #     lr_scheduler=lr_scheduler,\n",
    "        #     optim_cfg=cfg.OPTIMIZATION,\n",
    "        #     start_epoch=start_epoch,\n",
    "        #     total_epochs=args.epochs,\n",
    "        #     start_iter=it,\n",
    "        #     rank=cfg.LOCAL_RANK,\n",
    "        #     tb_log=tb_log,\n",
    "        #     ckpt_save_dir=ckpt_dir,\n",
    "        #     train_sampler=train_sampler,\n",
    "        #     lr_warmup_scheduler=lr_warmup_scheduler,\n",
    "        #     ckpt_save_interval=args.ckpt_save_interval,\n",
    "        #     max_ckpt_save_num=args.max_ckpt_save_num,\n",
    "        #     merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch\n",
    "        # )\n",
    "\n",
    "\n",
    "        ckpt_pth = save_path+'bayes_model-{}-{}-{}'.format(p1, p2, p3)\n",
    "        ckpt_name = ckpt_pth+'.pth'\n",
    "\n",
    "        logger.info('**********************End training**********************')\n",
    "\n",
    "        time.sleep(30)\n",
    "\n",
    "\n",
    "        f_name = self.f_name\n",
    "        f = open(save_path+'result.txt', \"a+\")\n",
    "        f.write('----------------Noise-{}-evaluate----------------'.format(f_name))\n",
    "        f.write('----------------{}-{}-{}---------------\\n'.format(p1, p2, p3))\n",
    "        f.close()\n",
    "\n",
    "        logger.info('---------------Epoch-{}-Noise-{}-evaluate----------------'.format(n, f_name))\n",
    "        model = add_noise_to_weights('./hardware_noise/hardware_data/'+f_name, 0, f_name, model, 'cuda')\n",
    "\n",
    "        acc1 = eval_utils_multinomial.eval_simple(args.ckpt, p1, p2, p3, f_name, n, cfg, model, test_loader, logger, save_path, dist_test=dist_train, save_to_file=args.save_to_file, result_dir=eval_output_dir)\n",
    "        print(\"----------\")\n",
    "        print(acc1)\n",
    "        print(\"----------\")\n",
    "        logger.info('**********************End evaluation**********************')\n",
    "\n",
    "            # best_accu = acc\n",
    "\n",
    "        return acc1  #+acc2+acc3\n",
    "\n",
    "\n",
    "if args.launcher == 'none':\n",
    "    dist_train = False  # True\n",
    "    total_gpus = 1\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    print('Using GPU:' + str(np.argmax(memory_gpu)))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(np.argmax(memory_gpu))\n",
    "    os.system('rm tmp')\n",
    "else:\n",
    "    args.is_master = args.local_rank == 0\n",
    "    args.device = torch.cuda.device(args.local_rank)\n",
    "    torch.cuda.manual_seed_all(666)\n",
    "    # dist.init_process_group(backend='nccl', init_method='env://', rank = 0, world_size = 1)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '5678'\n",
    "    # dist.init_process_group(backend='nccl', init_method='env://', rank = 0, world_size = 1)\n",
    "    torch.cuda.set_device(1)\n",
    "    device = torch.device('cuda', cfg.LOCAL_RANK)\n",
    "    total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "        args.tcp_port, args.local_rank, backend='nccl'\n",
    "    )\n",
    "    dist_train = True\n",
    "\n",
    "\n",
    "if args.batch_size is None:\n",
    "    args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "else:\n",
    "    assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "    args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "if args.fix_random_seed:\n",
    "    common_utils.set_random_seed(666)\n",
    "\n",
    "output_dir = cfg.ROOT_DIR / 'output' / cfg.EXP_GROUP_PATH / cfg.TAG / args.extra_tag\n",
    "ckpt_dir = output_dir / 'ckpt'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "# logger = common_utils.create_logger(log_file, rank=cfg.LOCAL_RANK)\n",
    "logger = common_utils.create_logger('./baseline/pointpillar/log.txt', rank=cfg.LOCAL_RANK)\n",
    "\n",
    "file = open('./baseline/pointpillar/result.txt','w')\n",
    "file.write('results\\n')\n",
    "file.close()\n",
    "\n",
    "\n",
    "# log to file\n",
    "logger.info('**********************Start logging**********************')\n",
    "gpu_list = os.environ['CUDA_VISIBLE_DEVICES'] if 'CUDA_VISIBLE_DEVICES' in os.environ.keys() else 'ALL'\n",
    "logger.info('CUDA_VISIBLE_DEVICES=%s' % gpu_list)\n",
    "\n",
    "\n",
    "for key, val in vars(args).items():\n",
    "    logger.info('{:16} {}'.format(key, val))\n",
    "log_config_to_file(cfg, logger=logger)\n",
    "if cfg.LOCAL_RANK == 0:\n",
    "    os.system('cp %s %s' % (args.cfg_file, output_dir))\n",
    "\n",
    "tb_log = SummaryWriter(log_dir=str(output_dir / 'tensorboard')) if cfg.LOCAL_RANK == 0 else None\n",
    "\n",
    "# -----------------------create dataloader & network & optimizer---------------------------\n",
    "train_set, train_loader, train_sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    class_names=cfg.CLASS_NAMES,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=dist_train, workers=args.workers,\n",
    "    logger=logger,\n",
    "    training=True,\n",
    "    merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "    total_epochs=args.epochs\n",
    ")\n",
    "\n",
    "\n",
    "logger.info('**********************Start evaluation %s/%s(%s)**********************' %\n",
    "            (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "    dataset_cfg=cfg.DATA_CONFIG,\n",
    "    class_names=cfg.CLASS_NAMES,\n",
    "    batch_size=args.batch_size,\n",
    "    dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    ")\n",
    "eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "args.start_epoch = max(args.epochs - 10, 0)  # Only evaluate the last 10 epochs\n",
    "\n",
    "\n",
    "logger.info('----------------Noise Experiment----------------')\n",
    "\n",
    "save_path = './save_path/'\n",
    "f = open(save_path+'3d.txt', \"a+\")\n",
    "f.write(str(datetime.datetime.now())+'\\n')\n",
    "f.close()\n",
    "\n",
    "f = open(save_path+'3d.txt', \"a+\")\n",
    "f.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format('s', 'n', 'mthd',\n",
    "                                    'Car/easy_R40',\n",
    "                                    'Car/moderate_R40',\n",
    "                                    'Car/hard_R40',\n",
    "                                    'Pedestrian/easy_R40',\n",
    "                                    'Pedestrian/moderate_R40',\n",
    "                                    'Pedestrian/hard_R40',\n",
    "                                    'Cyclist/easy_R40',\n",
    "                                    'Cyclist/moderate_R40',\n",
    "                                    'Cyclist/hard_R40',\n",
    "                                    'easy_R40',\n",
    "                                    'moderate_R40',\n",
    "                                    'hard_R40',\n",
    "                                    'avg', 'time'\n",
    "                                    ))\n",
    "f.close()\n",
    "\n",
    "\n",
    "hw_data_files = os.listdir('./hardware_noise/hardware_data/')\n",
    "\n",
    "\n",
    "file2ap_dict = {}\n",
    "for f_name in sorted(hw_data_files)[-5:]:\n",
    "    if f_name.endswith('xlsx'):\n",
    "        file2ap_dict[f_name] = {}\n",
    "        print(f_name)\n",
    "        \n",
    "        print(\"=============\")\n",
    "        p1 = 0.42\n",
    "        p2 = 0.23\n",
    "        p3 = 0.11\n",
    "        print(p1, p2, p3)\n",
    "        print(\"=============\")\n",
    "        \n",
    "        opt= Opt(f_name, train_set, train_loader, train_sampler, test_set, test_loader, sampler)\n",
    "        opt_function = opt.opt_function\n",
    "        \n",
    "        # Bounded region of parameter space\n",
    "        pbounds = {'p1': (0.1, 0.9), 'p2': (0.1, 0.9), 'p3': (0.1, 0.9)}\n",
    "\n",
    "\n",
    "        optimizer_bayes = BayesianOptimization(\n",
    "            f=opt_function,\n",
    "            pbounds=pbounds,\n",
    "            verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "            random_state=1,\n",
    "        )\n",
    "        optimizer_bayes.probe(\n",
    "            params={'p1': 0.42, 'p2': 0.23, 'p3': 0.11},\n",
    "            lazy=True,\n",
    "        )\n",
    "\n",
    "        logger_bayes = JSONLogger(path=save_path+\"logs2.json\")\n",
    "        optimizer_bayes.subscribe(Events.OPTIMIZATION_STEP, logger_bayes)\n",
    "\n",
    "\n",
    "        n = 0\n",
    "        optimizer_bayes.maximize(\n",
    "            init_points=3,\n",
    "            n_iter=10,\n",
    "        )\n",
    "logger.info('**********************End evaluation %s/%s(%s)**********************')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403512cc-bdb0-421f-babc-bc8b4d54adee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d69ba-65f2-4bfb-8b31-85477599fc80",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-05T07:08:01.497950Z",
     "iopub.status.idle": "2023-07-05T07:08:01.498176Z",
     "shell.execute_reply": "2023-07-05T07:08:01.498079Z",
     "shell.execute_reply.started": "2023-07-05T07:08:01.498069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file2ap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312d5bf-cc2f-48ed-86d6-8fec54dea795",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-05T07:08:01.498826Z",
     "iopub.status.idle": "2023-07-05T07:08:01.499020Z",
     "shell.execute_reply": "2023-07-05T07:08:01.498930Z",
     "shell.execute_reply.started": "2023-07-05T07:08:01.498922Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f_name in sorted(hw_data_files):\n",
    "    print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81a982-fd22-4942-8817-fc12fd7c6e7b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-05T07:08:01.499742Z",
     "iopub.status.idle": "2023-07-05T07:08:01.499926Z",
     "shell.execute_reply": "2023-07-05T07:08:01.499841Z",
     "shell.execute_reply.started": "2023-07-05T07:08:01.499832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b87fd-41dc-4733-bdb2-7a2d80e21c8b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.status.busy": "2023-07-05T07:08:01.500540Z",
     "iopub.status.idle": "2023-07-05T07:08:01.500716Z",
     "shell.execute_reply": "2023-07-05T07:08:01.500634Z",
     "shell.execute_reply.started": "2023-07-05T07:08:01.500626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # my_dict = file2sigma  # { 'Apple': 4, 'Banana': 2, 'Orange': 6, 'Grapes': 11}\n",
    "# # # 保存文件\n",
    "# # np.save('file2sigma.npy', my_dict)\n",
    "# # 读取文件 \n",
    "# # new_dict = np.load('./hardware_noise/file2sigma.npy', allow_pickle='TRUE').item()\n",
    "# # print(new_dict)\n",
    "\n",
    "# # for i, f_name in enumerate(new_dict.keys()):\n",
    "# #     # if ~f_name.endswith('xlsx'):\n",
    "# #     #     pass\n",
    "# #     # if f_name != sorted(hw_data_files)[i+1]:\n",
    "# #     #     print('!')\n",
    "# #     for u in range(10):\n",
    "# #         with open('sigma.txt', 'a+') as f:\n",
    "# #             print(new_dict[f_name], file=f)\n",
    "            \n",
    "# new_dict = np.load('./hardware_noise/file2usability.npy', allow_pickle='TRUE').item()\n",
    "# # print(new_dict)\n",
    "\n",
    "# for i, f_name in enumerate(new_dict.keys()):\n",
    "#     # if ~f_name.endswith('xlsx'):\n",
    "#     #     pass\n",
    "#     # if f_name != sorted(hw_data_files)[i+1]:\n",
    "#     #     print('!')\n",
    "#     for u in range(10):\n",
    "#         with open('usability.txt', 'a+') as f:\n",
    "#             print(new_dict[f_name], file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
