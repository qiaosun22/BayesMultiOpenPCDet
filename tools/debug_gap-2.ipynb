{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cfef76-63f6-45a2-9fdd-234058610d84",
   "metadata": {},
   "source": [
    "检查 gap 产生的原因\n",
    "\n",
    "ERM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e48621f-a4ac-4a23-b6f0-553e0eb979d5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-20T11:17:11.454395Z",
     "iopub.status.busy": "2023-07-20T11:17:11.453880Z",
     "iopub.status.idle": "2023-07-20T11:20:57.805493Z",
     "shell.execute_reply": "2023-07-20T11:20:57.804947Z",
     "shell.execute_reply.started": "2023-07-20T11:17:11.454378Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 19:17:11,526   INFO  Loading KITTI dataset\n",
      "2023-07-20 19:17:11,526   INFO  Loading KITTI dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-20 19:17:11.472269\n",
      "Using GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 19:17:11,749   INFO  Total samples for KITTI dataset: 3769\n",
      "2023-07-20 19:17:11,749   INFO  Total samples for KITTI dataset: 3769\n",
      "2023-07-20 19:17:11,839   INFO  **********************Start evaluation kitti_models/pointpillar(default)**********************\n",
      "2023-07-20 19:17:11,839   INFO  **********************Start evaluation kitti_models/pointpillar(default)**********************\n",
      "2023-07-20 19:17:11,840   INFO  ----------------Noise Experiment----------------\n",
      "2023-07-20 19:17:11,840   INFO  ----------------Noise Experiment----------------\n",
      "2023-07-20 19:17:11,847   INFO  ==> Loading parameters from checkpoint checkpoint_epoch_30__.pth to CPU\n",
      "2023-07-20 19:17:11,847   INFO  ==> Loading parameters from checkpoint checkpoint_epoch_30__.pth to CPU\n",
      "2023-07-20 19:17:11,882   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+02ac3e1\n",
      "2023-07-20 19:17:11,882   INFO  ==> Checkpoint trained from version: pcdet+0.6.0+02ac3e1\n",
      "2023-07-20 19:17:11,887   INFO  Not updated weight backbone_2d.blocks.0.4.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,887   INFO  Not updated weight backbone_2d.blocks.0.4.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,887   INFO  Not updated weight backbone_2d.blocks.0.5.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,887   INFO  Not updated weight backbone_2d.blocks.0.5.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,888   INFO  Not updated weight backbone_2d.blocks.0.5.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,888   INFO  Not updated weight backbone_2d.blocks.0.5.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,889   INFO  Not updated weight backbone_2d.blocks.0.5.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,889   INFO  Not updated weight backbone_2d.blocks.0.5.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,889   INFO  Not updated weight backbone_2d.blocks.0.5.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,889   INFO  Not updated weight backbone_2d.blocks.0.5.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,890   INFO  Not updated weight backbone_2d.blocks.0.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,890   INFO  Not updated weight backbone_2d.blocks.0.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,891   INFO  Not updated weight backbone_2d.blocks.0.7.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,891   INFO  Not updated weight backbone_2d.blocks.0.7.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,891   INFO  Not updated weight backbone_2d.blocks.0.8.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,891   INFO  Not updated weight backbone_2d.blocks.0.8.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,892   INFO  Not updated weight backbone_2d.blocks.0.8.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,892   INFO  Not updated weight backbone_2d.blocks.0.8.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,892   INFO  Not updated weight backbone_2d.blocks.0.8.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,892   INFO  Not updated weight backbone_2d.blocks.0.8.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,893   INFO  Not updated weight backbone_2d.blocks.0.8.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,893   INFO  Not updated weight backbone_2d.blocks.0.8.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,894   INFO  Not updated weight backbone_2d.blocks.0.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,894   INFO  Not updated weight backbone_2d.blocks.0.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,894   INFO  Not updated weight backbone_2d.blocks.0.10.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,894   INFO  Not updated weight backbone_2d.blocks.0.10.weight: torch.Size([64, 64, 3, 3])\n",
      "2023-07-20 19:17:11,895   INFO  Not updated weight backbone_2d.blocks.0.11.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,895   INFO  Not updated weight backbone_2d.blocks.0.11.weight: torch.Size([64])\n",
      "2023-07-20 19:17:11,896   INFO  Not updated weight backbone_2d.blocks.0.11.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,896   INFO  Not updated weight backbone_2d.blocks.0.11.bias: torch.Size([64])\n",
      "2023-07-20 19:17:11,896   INFO  Not updated weight backbone_2d.blocks.0.11.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,896   INFO  Not updated weight backbone_2d.blocks.0.11.running_mean: torch.Size([64])\n",
      "2023-07-20 19:17:11,897   INFO  Not updated weight backbone_2d.blocks.0.11.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,897   INFO  Not updated weight backbone_2d.blocks.0.11.running_var: torch.Size([64])\n",
      "2023-07-20 19:17:11,897   INFO  Not updated weight backbone_2d.blocks.0.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,897   INFO  Not updated weight backbone_2d.blocks.0.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,898   INFO  Not updated weight backbone_2d.blocks.1.4.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,898   INFO  Not updated weight backbone_2d.blocks.1.4.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,899   INFO  Not updated weight backbone_2d.blocks.1.5.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,899   INFO  Not updated weight backbone_2d.blocks.1.5.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,899   INFO  Not updated weight backbone_2d.blocks.1.5.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,899   INFO  Not updated weight backbone_2d.blocks.1.5.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,900   INFO  Not updated weight backbone_2d.blocks.1.5.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,900   INFO  Not updated weight backbone_2d.blocks.1.5.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,901   INFO  Not updated weight backbone_2d.blocks.1.5.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,901   INFO  Not updated weight backbone_2d.blocks.1.5.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,901   INFO  Not updated weight backbone_2d.blocks.1.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,901   INFO  Not updated weight backbone_2d.blocks.1.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,902   INFO  Not updated weight backbone_2d.blocks.1.7.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,902   INFO  Not updated weight backbone_2d.blocks.1.7.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,902   INFO  Not updated weight backbone_2d.blocks.1.8.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,902   INFO  Not updated weight backbone_2d.blocks.1.8.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,903   INFO  Not updated weight backbone_2d.blocks.1.8.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,903   INFO  Not updated weight backbone_2d.blocks.1.8.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,904   INFO  Not updated weight backbone_2d.blocks.1.8.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,904   INFO  Not updated weight backbone_2d.blocks.1.8.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,904   INFO  Not updated weight backbone_2d.blocks.1.8.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,904   INFO  Not updated weight backbone_2d.blocks.1.8.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,905   INFO  Not updated weight backbone_2d.blocks.1.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,905   INFO  Not updated weight backbone_2d.blocks.1.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,905   INFO  Not updated weight backbone_2d.blocks.1.10.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,905   INFO  Not updated weight backbone_2d.blocks.1.10.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,906   INFO  Not updated weight backbone_2d.blocks.1.11.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,906   INFO  Not updated weight backbone_2d.blocks.1.11.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,907   INFO  Not updated weight backbone_2d.blocks.1.11.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,907   INFO  Not updated weight backbone_2d.blocks.1.11.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,907   INFO  Not updated weight backbone_2d.blocks.1.11.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,907   INFO  Not updated weight backbone_2d.blocks.1.11.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,908   INFO  Not updated weight backbone_2d.blocks.1.11.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,908   INFO  Not updated weight backbone_2d.blocks.1.11.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,909   INFO  Not updated weight backbone_2d.blocks.1.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,909   INFO  Not updated weight backbone_2d.blocks.1.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,909   INFO  Not updated weight backbone_2d.blocks.1.16.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,909   INFO  Not updated weight backbone_2d.blocks.1.16.weight: torch.Size([128, 128, 3, 3])\n",
      "2023-07-20 19:17:11,910   INFO  Not updated weight backbone_2d.blocks.1.17.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,910   INFO  Not updated weight backbone_2d.blocks.1.17.weight: torch.Size([128])\n",
      "2023-07-20 19:17:11,910   INFO  Not updated weight backbone_2d.blocks.1.17.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,910   INFO  Not updated weight backbone_2d.blocks.1.17.bias: torch.Size([128])\n",
      "2023-07-20 19:17:11,911   INFO  Not updated weight backbone_2d.blocks.1.17.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,911   INFO  Not updated weight backbone_2d.blocks.1.17.running_mean: torch.Size([128])\n",
      "2023-07-20 19:17:11,912   INFO  Not updated weight backbone_2d.blocks.1.17.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,912   INFO  Not updated weight backbone_2d.blocks.1.17.running_var: torch.Size([128])\n",
      "2023-07-20 19:17:11,912   INFO  Not updated weight backbone_2d.blocks.1.17.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,912   INFO  Not updated weight backbone_2d.blocks.1.17.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,913   INFO  Not updated weight backbone_2d.blocks.2.4.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,913   INFO  Not updated weight backbone_2d.blocks.2.4.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,914   INFO  Not updated weight backbone_2d.blocks.2.5.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,914   INFO  Not updated weight backbone_2d.blocks.2.5.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,914   INFO  Not updated weight backbone_2d.blocks.2.5.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,914   INFO  Not updated weight backbone_2d.blocks.2.5.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,915   INFO  Not updated weight backbone_2d.blocks.2.5.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,915   INFO  Not updated weight backbone_2d.blocks.2.5.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,915   INFO  Not updated weight backbone_2d.blocks.2.5.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,915   INFO  Not updated weight backbone_2d.blocks.2.5.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,916   INFO  Not updated weight backbone_2d.blocks.2.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,916   INFO  Not updated weight backbone_2d.blocks.2.5.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,917   INFO  Not updated weight backbone_2d.blocks.2.7.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,917   INFO  Not updated weight backbone_2d.blocks.2.7.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,917   INFO  Not updated weight backbone_2d.blocks.2.8.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,917   INFO  Not updated weight backbone_2d.blocks.2.8.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,918   INFO  Not updated weight backbone_2d.blocks.2.8.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,918   INFO  Not updated weight backbone_2d.blocks.2.8.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,919   INFO  Not updated weight backbone_2d.blocks.2.8.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,919   INFO  Not updated weight backbone_2d.blocks.2.8.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,919   INFO  Not updated weight backbone_2d.blocks.2.8.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,919   INFO  Not updated weight backbone_2d.blocks.2.8.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,920   INFO  Not updated weight backbone_2d.blocks.2.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,920   INFO  Not updated weight backbone_2d.blocks.2.8.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,920   INFO  Not updated weight backbone_2d.blocks.2.10.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,920   INFO  Not updated weight backbone_2d.blocks.2.10.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,921   INFO  Not updated weight backbone_2d.blocks.2.11.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,921   INFO  Not updated weight backbone_2d.blocks.2.11.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,922   INFO  Not updated weight backbone_2d.blocks.2.11.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,922   INFO  Not updated weight backbone_2d.blocks.2.11.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,922   INFO  Not updated weight backbone_2d.blocks.2.11.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,922   INFO  Not updated weight backbone_2d.blocks.2.11.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,923   INFO  Not updated weight backbone_2d.blocks.2.11.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,923   INFO  Not updated weight backbone_2d.blocks.2.11.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,924   INFO  Not updated weight backbone_2d.blocks.2.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,924   INFO  Not updated weight backbone_2d.blocks.2.11.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,924   INFO  Not updated weight backbone_2d.blocks.2.16.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,924   INFO  Not updated weight backbone_2d.blocks.2.16.weight: torch.Size([256, 256, 3, 3])\n",
      "2023-07-20 19:17:11,925   INFO  Not updated weight backbone_2d.blocks.2.17.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,925   INFO  Not updated weight backbone_2d.blocks.2.17.weight: torch.Size([256])\n",
      "2023-07-20 19:17:11,925   INFO  Not updated weight backbone_2d.blocks.2.17.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,925   INFO  Not updated weight backbone_2d.blocks.2.17.bias: torch.Size([256])\n",
      "2023-07-20 19:17:11,926   INFO  Not updated weight backbone_2d.blocks.2.17.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,926   INFO  Not updated weight backbone_2d.blocks.2.17.running_mean: torch.Size([256])\n",
      "2023-07-20 19:17:11,927   INFO  Not updated weight backbone_2d.blocks.2.17.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,927   INFO  Not updated weight backbone_2d.blocks.2.17.running_var: torch.Size([256])\n",
      "2023-07-20 19:17:11,927   INFO  Not updated weight backbone_2d.blocks.2.17.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,927   INFO  Not updated weight backbone_2d.blocks.2.17.num_batches_tracked: torch.Size([])\n",
      "2023-07-20 19:17:11,928   INFO  ==> Done (loaded 61/127)\n",
      "2023-07-20 19:17:11,928   INFO  ==> Done (loaded 61/127)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3700022894417224e-16 I-V_data_1.5um_length_5nm_diameter_NA_third_etch_8min_Pb_ED_3h_180C_MAI_no_150nm_Ag_memory_6V_carbon_paste.xlsx\n",
      "I-V_data_1.5um_length_5nm_diameter_NA_third_etch_8min_Pb_ED_3h_180C_MAI_no_150nm_Ag_memory_6V_carbon_paste.xlsx\n",
      "file:I-V_data_1.5um_length_5nm_diameter_NA_third_etch_8min_Pb_ED_3h_180C_MAI_no_150nm_Ag_memory_6V_carbon_paste.xlsx, evaluate-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|██████████| 943/943 [03:19<00:00, 27.59it/s]2023-07-20 19:20:31,875   INFO  Average predicted number of objects(3769 samples): 55.523\n",
      "2023-07-20 19:20:31,875   INFO  Average predicted number of objects(3769 samples): 55.523\n",
      "eval: 100%|██████████| 943/943 [03:30<00:00, 27.59it/s]/home/pai/lib/python3.9/site-packages/numba/core/typed_passes.py:329: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../pcdet/datasets/kitti/kitti_object_eval_python/eval.py\", line 122:\u001b[0m\n",
      "\u001b[1m@numba.jit(nopython=True, parallel=True)\n",
      "\u001b[1mdef d3_box_overlap_kernel(boxes, qboxes, rinc, criterion=-1):\n",
      "\u001b[0m\u001b[1m^\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaPerformanceWarning(msg,\n",
      "eval: 100%|██████████| 943/943 [03:45<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall/roi_0.3': 0.0, 'recall/rcnn_0.3': 0.0, 'recall/roi_0.5': 0.0, 'recall/rcnn_0.5': 0.0, 'recall/roi_0.7': 0.0, 'recall/rcnn_0.7': 0.0, 'Car_aos/easy_R40': 0.0, 'Car_aos/moderate_R40': 4.251296646888003e-05, 'Car_aos/hard_R40': 4.251296646888003e-05, 'Car_3d/easy_R40': 0.0, 'Car_3d/moderate_R40': 0.0, 'Car_3d/hard_R40': 0.0, 'Car_bev/easy_R40': 0.0, 'Car_bev/moderate_R40': 0.0, 'Car_bev/hard_R40': 0.0, 'Car_image/easy_R40': 0.0, 'Car_image/moderate_R40': 4.4467897142789386e-05, 'Car_image/hard_R40': 4.4467897142789386e-05, 'Pedestrian_aos/easy_R40': 0.0, 'Pedestrian_aos/moderate_R40': 0.0, 'Pedestrian_aos/hard_R40': 0.0, 'Pedestrian_3d/easy_R40': 0.0, 'Pedestrian_3d/moderate_R40': 0.0, 'Pedestrian_3d/hard_R40': 0.0, 'Pedestrian_bev/easy_R40': 0.0, 'Pedestrian_bev/moderate_R40': 0.0, 'Pedestrian_bev/hard_R40': 0.0, 'Pedestrian_image/easy_R40': 0.0, 'Pedestrian_image/moderate_R40': 0.0, 'Pedestrian_image/hard_R40': 0.0, 'Cyclist_aos/easy_R40': 0.0, 'Cyclist_aos/moderate_R40': 0.0, 'Cyclist_aos/hard_R40': 0.0, 'Cyclist_3d/easy_R40': 0.0, 'Cyclist_3d/moderate_R40': 0.0, 'Cyclist_3d/hard_R40': 0.0, 'Cyclist_bev/easy_R40': 0.0, 'Cyclist_bev/moderate_R40': 0.0, 'Cyclist_bev/hard_R40': 0.0, 'Cyclist_image/easy_R40': 0.0, 'Cyclist_image/moderate_R40': 0.0, 'Cyclist_image/hard_R40': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "# from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "# from noise import add_noise_to_weights\n",
    "\n",
    "import numba\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "from pcdet.models import build_network, model_fn_decorator\n",
    "from pcdet.utils import common_utils\n",
    "from train_utils.optimization import build_optimizer, build_scheduler\n",
    "from train_utils.train_utils import train_model, model_save\n",
    "from eval_utils import eval_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from hardware_noise.weight_mapping_finalv import weight_mapping as add_noise_to_weights\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--cfg_file', type=str, default='cfgs/kitti_models/pointpillar.yaml', \\\n",
    "                        help='specify the config for training')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=None, required=False, help='batch size for training')\n",
    "    parser.add_argument('--epochs', type=int, default=None, required=False, help='number of epochs to train for')\n",
    "    parser.add_argument('--workers', type=int, default=32, help='number of workers for dataloader')\n",
    "    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    parser.add_argument('--ckpt', type=str, default='checkpoint_epoch_30__.pth', \\\n",
    "                        help='checkpoint to start from')\n",
    "    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    parser.add_argument('--tcp_port', type=int, default=29051, help='tcp port for distrbuted training')\n",
    "    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    parser.add_argument('--fix_random_seed', action='store_true', default=True, help='')\n",
    "    parser.add_argument('--ckpt_save_interval', type=int, default=1, help='number of training epochs')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')\n",
    "    parser.add_argument('--max_ckpt_save_num', type=int, default=30, help='max number of saved checkpoint')\n",
    "    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER,\n",
    "                        help='set extra config keys if needed')\n",
    "\n",
    "    parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "    \n",
    "    # parser.add_argument('--local_rank', type=int, default=-1, metavar='N', help='Local process rank.')\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    cfg_from_yaml_file(args.cfg_file, cfg)\n",
    "    cfg.TAG = Path(args.cfg_file).stem\n",
    "    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1:-1])  # remove 'cfgs' and 'xxxx.yaml'\n",
    "\n",
    "    if args.set_cfgs is not None:\n",
    "        cfg_from_list(args.set_cfgs, cfg)\n",
    "\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "args, cfg = parse_config()\n",
    "\n",
    "\n",
    "\n",
    "if args.launcher == 'none':\n",
    "    dist_train = False  # True\n",
    "    total_gpus = 1\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    print('Using GPU:' + str(np.argmax(memory_gpu)))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(np.argmax(memory_gpu))\n",
    "    os.system('rm tmp')\n",
    "else:\n",
    "    args.is_master = args.local_rank == 0\n",
    "    args.device = torch.cuda.device(args.local_rank)\n",
    "    torch.cuda.manual_seed_all(666)\n",
    "    # dist.init_process_group(backend='nccl', init_method='env://', rank = 0, world_size = 1)\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '5678'\n",
    "    # dist.init_process_group(backend='nccl', init_method='env://', rank = 0, world_size = 1)\n",
    "    torch.cuda.set_device(1)\n",
    "    device = torch.device('cuda', cfg.LOCAL_RANK)\n",
    "    total_gpus, cfg.LOCAL_RANK = getattr(common_utils, 'init_dist_%s' % args.launcher)(\n",
    "        args.tcp_port, args.local_rank, backend='nccl'\n",
    "    )\n",
    "    dist_train = True\n",
    "\n",
    "\n",
    "if args.batch_size is None:\n",
    "    args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "else:\n",
    "    assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "    args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "\n",
    "if args.fix_random_seed:\n",
    "    common_utils.set_random_seed(666)\n",
    "\n",
    "output_dir = cfg.ROOT_DIR / 'output' / cfg.EXP_GROUP_PATH / cfg.TAG / args.extra_tag\n",
    "ckpt_dir = output_dir / 'ckpt'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_file = output_dir / ('log_train_%s.txt' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
    "logger = common_utils.create_logger('./baseline/pointpillar/log.txt', rank=cfg.LOCAL_RANK)\n",
    "\n",
    "file = open('./baseline/pointpillar/result.txt','w')\n",
    "file.write('results\\n')\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''Test on Noises'''  \n",
    "test_set, test_loader, sampler = build_dataloader(\n",
    "                                dataset_cfg=cfg.DATA_CONFIG,\n",
    "                                class_names=cfg.CLASS_NAMES,\n",
    "                                batch_size=args.batch_size,\n",
    "                                dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "                            )\n",
    "\n",
    "\n",
    "model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=test_set)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "logger.info('**********************Start evaluation %s/%s(%s)**********************' %\n",
    "            (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "\n",
    "\n",
    "eval_output_dir = output_dir / 'eval' / 'eval_with_train'\n",
    "eval_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "args.start_epoch = max(args.epochs - 10, 0)  # Only evaluate the last 10 epochs\n",
    "\n",
    "\n",
    "\n",
    "logger.info('----------------Noise Experiment----------------')\n",
    "\n",
    "save_path = './save_path/'\n",
    "f = open(save_path+'3d.txt', \"a+\")\n",
    "f.write(str(datetime.datetime.now())+'\\n')\n",
    "f.close()\n",
    "\n",
    "with open(save_path+'3d.txt', \"a+\") as f:\n",
    "    f.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                                        'file', 'usability', 'sigma', 'n', 'mthd',\n",
    "                                        'Car/easy_R40',\n",
    "                                        'Car/moderate_R40',\n",
    "                                        'Car/hard_R40',\n",
    "                                        'Pedestrian/easy_R40',\n",
    "                                        'Pedestrian/moderate_R40',\n",
    "                                        'Pedestrian/hard_R40',\n",
    "                                        'Cyclist/easy_R40',\n",
    "                                        'Cyclist/moderate_R40',\n",
    "                                        'Cyclist/hard_R40',\n",
    "                                        'easy_R40',\n",
    "                                        'moderate_R40',\n",
    "                                        'hard_R40',\n",
    "                                        'avg', 'time'\n",
    "                                            ))\n",
    "\n",
    "hw_data_files = os.listdir('./hardware_noise/hardware_data/')\n",
    "\n",
    "file2usability = np.load('./hardware_noise/file2usability.npy', allow_pickle=True).item()\n",
    "file2sigma = np.load('./hardware_noise/file2sigma.npy', allow_pickle=True).item()\n",
    "\n",
    "N = 1\n",
    "file2ap_dict = {}\n",
    "\n",
    "for sigma, f_name in sorted(zip(file2sigma.values(), file2sigma.keys())):\n",
    "    print(sigma, f_name)\n",
    "    usability = file2usability[f_name]\n",
    "    if f_name.endswith('xlsx'):\n",
    "        file2ap_dict[f_name] = {}\n",
    "        print(f_name)\n",
    "         \n",
    "        usability = file2usability[f_name]\n",
    "        sigma = file2sigma[f_name]\n",
    "\n",
    "        for n in range(N):\n",
    "            print('file:{}, evaluate-{}'.format(f_name, n))\n",
    "            model.load_params_from_file(filename=args.ckpt, to_cpu=dist, logger=logger)\n",
    "            # add_noise_to_weights('./hardware_noise/hardware_data/'+f_name, model, device='cuda')\n",
    "\n",
    "\n",
    "            # acc1 = eval_utils.eval_simple(p1, p2, sigma, n, cfg, model, test_loader, logger, save_path, dist_test=dist_train, save_to_file=args.save_to_file, result_dir=eval_output_dir)\n",
    "            acc1, ret_dict = eval_utils.eval_simple(f_name, usability, sigma, \n",
    "                            n, cfg, model, test_loader, logger, \n",
    "                            save_path, dist_test=False, \n",
    "                            save_to_file=False, result_dir=save_path)\n",
    "            print(ret_dict)\n",
    "            file2ap_dict[f_name][n] = ret_dict\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d69ba-65f2-4bfb-8b31-85477599fc80",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-20T11:13:37.480344Z",
     "iopub.status.idle": "2023-07-20T11:13:37.480552Z",
     "shell.execute_reply": "2023-07-20T11:13:37.480466Z",
     "shell.execute_reply.started": "2023-07-20T11:13:37.480456Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "file2ap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7312d5bf-cc2f-48ed-86d6-8fec54dea795",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-20T11:13:37.481476Z",
     "iopub.status.idle": "2023-07-20T11:13:37.481762Z",
     "shell.execute_reply": "2023-07-20T11:13:37.481680Z",
     "shell.execute_reply.started": "2023-07-20T11:13:37.481672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for f_name in sorted(hw_data_files):\n",
    "    print(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81a982-fd22-4942-8817-fc12fd7c6e7b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-20T11:13:37.482304Z",
     "iopub.status.idle": "2023-07-20T11:13:37.482595Z",
     "shell.execute_reply": "2023-07-20T11:13:37.482514Z",
     "shell.execute_reply.started": "2023-07-20T11:13:37.482505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
