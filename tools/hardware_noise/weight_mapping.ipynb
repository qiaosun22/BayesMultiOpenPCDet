{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad46dbf-1e2c-4255-9186-50e05c9d0803",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-04T07:26:35.843144Z",
     "iopub.status.busy": "2023-07-04T07:26:35.842990Z",
     "iopub.status.idle": "2023-07-04T07:26:41.172632Z",
     "shell.execute_reply": "2023-07-04T07:26:41.172104Z",
     "shell.execute_reply.started": "2023-07-04T07:26:35.843129Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear1.weight', tensor([[ 0.4246,  0.0939, -0.2650, -0.4600],\n",
      "        [ 0.4172, -0.4526,  0.2788, -0.1434],\n",
      "        [ 0.0403,  0.0310, -0.4260, -0.0195]], device='cuda:0')), ('linear1.bias', tensor([-0.2389, -0.4981, -0.0754], device='cuda:0'))])\n",
      "OrderedDict([('linear1.weight', tensor([[ 0.4830,  0.0862, -0.0885, -0.1241],\n",
      "        [ 0.1199, -1.3043,  0.8529, -0.1091],\n",
      "        [ 0.0021,  0.0171, -0.1740, -0.0153]], device='cuda:0')), ('linear1.bias', tensor([-1.0386, -2.2991, -0.0399], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import norm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def mlesigma(f_name, st_indx, end_indx):\n",
    "    \"\"\"\n",
    "    我们引入 MLE 方法来估计忆阻器电导值与对应电压水平下电导均值的比值的 mu 和 sigma\n",
    "    函数返回 sigma 值，它的值反映了等效噪声水平的强度\n",
    "    '''\n",
    "    关于噪声分布形式设定与转换的说明：\n",
    "    X ~ logNorm(a, b)  # m, v 是对数正态分布本身的均值和方差\n",
    "    # 这说的是 X 是一个值永远为正的随机变量，只有取对数之后才是正态分布\n",
    "    即 logX ~ Norm(mu, sigma)\n",
    "\n",
    "    换算关系：\n",
    "    m = exp(mu + sigma^2/2)  # 注意到 m 是 >0 的\n",
    "    v = exp(2*mu + sigma^2) * exp(sigma^2 - 1)  \n",
    "\n",
    "    因此，一个随机变量 Y 如果服从正态分布，即 Y ~ Norm(mu, sigma)\n",
    "    就一定可以有一个 X = exp(Y), 使得 Y = logX ~ Norm(mu, sigma)\n",
    "\n",
    "    所以，一个正态分布随机变量取指数得到的变量就服从对数正态分布\n",
    "\n",
    "    具体到上面的代码，已知gaussian_kernel 服从正态分布，则 \n",
    "            torch.exp(noise_sigma*gassian_kernel.sample(noise_sigma.size())) \n",
    "    服从对数正态分布，其中我们施加的 mu, sigma 都是对应的正态分布的参数，而非对数正态分布的参数\n",
    "\n",
    "    又由于 (m, v) 和 (mu, sigma) 是一一映射，我们控制谁都一样，\n",
    "    所以，为了方便和统一，今后：\n",
    "    1）需要采样对数正态分布样本，都采取先用 mu, sigma 采样正态分布，再 exp 的方式\n",
    "    2）需要估计对数正态分布参数，都采取先对对数正态分布样本取 log，再用 norm.fit 拟合的方式\n",
    "    '''\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(f_name, engine='openpyxl').iloc[1: 101, :]\n",
    "    data = data.iloc[st_indx: end_indx+1, :]\n",
    "    \n",
    "    conductance = data.drop(['voltage'], axis=1).div(data['voltage'], axis=0)\n",
    "    conductance = conductance[(conductance!=np.inf).all(axis=1)]\n",
    "    \n",
    "    conductance_by_mean = np.array(conductance.div(conductance.mean(axis=1), axis=0)).reshape(1, -1)[0]\n",
    "    samples = np.log(conductance_by_mean)\n",
    "    samples_fine = np.delete(conductance_by_mean, np.isfinite(samples)==False)\n",
    "\n",
    "    # mu_hat, sigma_hat = norm.fit(samples_fine)  # 通过极大似然估计得到 sigma\\hat，不固定均值为 0\n",
    "    mu_hat, sigma_hat = norm.fit(samples_fine, floc=0)  # 通过极大似然估计得到 sigma\\hat，固定均值为 0\n",
    "    \n",
    "    return sigma_hat # 我们只关注 sigma_hat\n",
    "\n",
    "\n",
    "def LCIS(arr):\n",
    "    \"\"\"\n",
    "    求最长连续[递增]*子序列\n",
    "    返回最长连续递增子序列的\n",
    "    ---------\n",
    "    *该[递增]序列并不要求始终严格增，而是最多允许两次递减\n",
    "    \"\"\"\n",
    "    decrease_cnt = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    sub_len = 0\n",
    "    longest_start = 0\n",
    "    longest_end = 0\n",
    "    longest_len = 0\n",
    "    decrease_point_0 = -1\n",
    "    decrease_point_1 = -1\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > arr[i-1]:\n",
    "            end_index += 1\n",
    "        else:\n",
    "            decrease_cnt += 1\n",
    "            if decrease_cnt == 1:\n",
    "                end_index += 1\n",
    "                decrease_point_0 = end_index\n",
    "            elif decrease_cnt == 2:\n",
    "                end_index += 1\n",
    "                decrease_point_1 = end_index\n",
    "            else:\n",
    "                sub_len = end_index - start_index + 1\n",
    "                if longest_len < sub_len:\n",
    "                    longest_len = sub_len\n",
    "                    longest_start = start_index\n",
    "                    longest_end = end_index\n",
    "                start_index = decrease_point_0\n",
    "                decrease_point_0 = decrease_point_1\n",
    "                decrease_point_1 = i\n",
    "                end_index = i\n",
    "                decrease_cnt = 2\n",
    "\n",
    "    sub_len = end_index - start_index + 1\n",
    "    if longest_len < sub_len:\n",
    "        longest_len = sub_len\n",
    "        longest_start = start_index\n",
    "        longest_end = end_index\n",
    "\n",
    "    return longest_start, longest_end\n",
    "\n",
    "\n",
    "def calculate_smoothed_cmean(f_name):\n",
    "    df = pd.read_excel(f_name, engine='openpyxl')\n",
    "    eps = 1e-12\n",
    "    current_cols = df.iloc[:, 1:]\n",
    "    voltage_col = df.iloc[:, :1].values\n",
    "\n",
    "    conductance_cols = current_cols.div(voltage_col + eps)\n",
    "    conductance_mean_col = conductance_cols.mean(axis=1).values\n",
    "    conductance_mean_col_head100 = conductance_mean_col[1: 101]\n",
    "    c_mean_smooth = Kalman1D(conductance_mean_col_head100)\n",
    "    # c_mean_smooth = c_mean_smooth.T[0]\n",
    "    return c_mean_smooth\n",
    "\n",
    "\n",
    "def Kalman1D(observations, damping=0):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "def weight_mapping(f_name, model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Map target weight to true weight. \n",
    "    The difference between the target weight and the true weight is due to two factors:\n",
    "    1.Random heat noises\n",
    "    2.Non-monotonic characteristics of conductance-Q curves\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To achieve the above functionality, we first have to measure the conductance-Q curve. \n",
    "    As the measurement is affected by noises, we measure the curve for several times and get\n",
    "    the mean value. \n",
    "    \"\"\"\n",
    "    c_mean_smooth = calculate_smoothed_cmean(f_name)\n",
    "    \n",
    "    max_index = c_mean_smooth.shape[0]\n",
    "    c_max = c_mean_smooth.max()\n",
    "    c_min = c_mean_smooth.min()\n",
    "\n",
    "    \"\"\"\n",
    "    We calculate the LCIS of the curve to find a monotonicly increasing part of the curve.\n",
    "    \"\"\"\n",
    "\n",
    "    start_index, end_index= LCIS(c_mean_smooth) \n",
    "    mono_len = end_index - start_index\n",
    "\n",
    "    #utility最大的曲线的值\n",
    "    required_len = 65\n",
    "    if mono_len < required_len: # Non-monotonic characteristics \n",
    "        increase_indices = np.arange(start_index, end_index, 1)\n",
    "        end_index = start_index + required_len\n",
    "        if end_index > max_index:\n",
    "            start_index = start_index - end_index + max_index\n",
    "            end_index = max_index\n",
    "        ratio = np.zeros(required_len)\n",
    "        for i in range(required_len):\n",
    "            idx = i + start_index\n",
    "            # 只有非递增区域的theta值受non-monotonic characteristics影响\n",
    "            ratio[i] = 1 if idx in increase_indices else (c_mean_smooth[idx]-c_min)/(c_max-c_min)\n",
    "\n",
    "    noise_sigma = mlesigma(f_name, start_index, end_index)\n",
    "    gassian_kernel = torch.distributions.Normal(0.0, noise_sigma)\n",
    "    with torch.no_grad():\n",
    "        for theta in model.parameters():\n",
    "            abstheta = torch.abs(theta) # 求参数的绝对值\n",
    "            normalized_theta = abstheta / (torch.max(abstheta) + 1e-8) # 归一化\n",
    "\n",
    "            theta_index = normalized_theta * (required_len-1)\n",
    "            theta_index = theta_index.type(torch.LongTensor) # 求各参数对应的下标位置\n",
    "            noise_index = normalized_theta * 100\n",
    "            noise_index = noise_index.type(torch.LongTensor)\n",
    "            noise_index[noise_index >= 100] = 99\n",
    "            \n",
    "            theta_ratio = torch.Tensor(ratio)[theta_index].cuda() # theta = theta * (c_real-c_min) / (c_max-c_min)\n",
    "\n",
    "            mul_ = theta_ratio * torch.exp(gassian_kernel.sample(theta.size()).cuda())\n",
    "            theta.mul_(mul_)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(4, 3)\n",
    "        def forward(self, xb):\n",
    "            xb = xb.view(xb.size(0), -1)\n",
    "            out = self.linear1(xb)\n",
    "            return out\n",
    "\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    print(model.state_dict())\n",
    "\n",
    "    f_name = 'I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx' #'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\n",
    "    weight_mapping(f_name, model, device='cuda')\n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bfd555-2c1d-4083-bbd9-cea1f0451906",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-07-04T07:26:41.174120Z",
     "iopub.status.busy": "2023-07-04T07:26:41.173679Z",
     "iopub.status.idle": "2023-07-04T07:26:41.383215Z",
     "shell.execute_reply": "2023-07-04T07:26:41.382487Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.174104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear1.weight', tensor([[-0.1097,  0.0465, -0.4834,  0.4669],\n",
      "        [-0.0549,  0.0168,  0.0946, -0.0709],\n",
      "        [ 0.4826,  0.1092,  0.4588,  0.4653]], device='cuda:0')), ('linear1.bias', tensor([-0.4774, -0.1961, -0.2825], device='cuda:0'))])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 222\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[1;32m    221\u001b[0m f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m \u001b[43mweights_mapping\u001b[49m(f_name, model, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import norm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def mlesigma(f_name, st_indx, end_indx):\n",
    "    \"\"\"\n",
    "    我们引入 MLE 方法来估计忆阻器电导值与对应电压水平下电导均值的比值的 mu 和 sigma\n",
    "    函数返回 sigma 值，它的值反映了等效噪声水平的强度\n",
    "    '''\n",
    "    关于噪声分布形式设定与转换的说明：\n",
    "    X ~ logNorm(a, b)  # m, v 是对数正态分布本身的均值和方差\n",
    "    # 这说的是 X 是一个值永远为正的随机变量，只有取对数之后才是正态分布\n",
    "    即 logX ~ Norm(mu, sigma)\n",
    "\n",
    "    换算关系：\n",
    "    m = exp(mu + sigma^2/2)  # 注意到 m 是 >0 的\n",
    "    v = exp(2*mu + sigma^2) * exp(sigma^2 - 1)  \n",
    "\n",
    "    因此，一个随机变量 Y 如果服从正态分布，即 Y ~ Norm(mu, sigma)\n",
    "    就一定可以有一个 X = exp(Y), 使得 Y = logX ~ Norm(mu, sigma)\n",
    "\n",
    "    所以，一个正态分布随机变量取指数得到的变量就服从对数正态分布\n",
    "\n",
    "    具体到上面的代码，已知gaussian_kernel 服从正态分布，则 \n",
    "            torch.exp(noise_sigma*gassian_kernel.sample(noise_sigma.size())) \n",
    "    服从对数正态分布，其中我们施加的 mu, sigma 都是对应的正态分布的参数，而非对数正态分布的参数\n",
    "\n",
    "    又由于 (m, v) 和 (mu, sigma) 是一一映射，我们控制谁都一样，\n",
    "    所以，为了方便和统一，今后：\n",
    "    1）需要采样对数正态分布样本，都采取先用 mu, sigma 采样正态分布，再 exp 的方式\n",
    "    2）需要估计对数正态分布参数，都采取先对对数正态分布样本取 log，再用 norm.fit 拟合的方式\n",
    "    '''\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(f_name, engine='openpyxl').iloc[:, 1: 101]\n",
    "    data = data.iloc[:, st_indx: end_indx+1]\n",
    "    \n",
    "    # conductance = data.drop(['voltage'], axis=1).div(data['voltage'], axis=0)\n",
    "    conductance = conductance[(conductance!=np.inf).all(axis=1)]\n",
    "    \n",
    "    conductance_by_mean = np.array(conductance.div(conductance.mean(axis=1), axis=0)).reshape(1, -1)[0]\n",
    "    samples = np.log(conductance_by_mean)\n",
    "    samples_fine = np.delete(conductance_by_mean, np.isfinite(samples)==False)\n",
    "\n",
    "    # mu_hat, sigma_hat = norm.fit(samples_fine)  # 通过极大似然估计得到 sigma\\hat，不固定均值为 0\n",
    "    mu_hat, sigma_hat = norm.fit(samples_fine, floc=0)  # 通过极大似然估计得到 sigma\\hat，固定均值为 0\n",
    "    \n",
    "    return sigma_hat # 我们只关注 sigma_hat\n",
    "\n",
    "\n",
    "def LCIS(arr):\n",
    "    \"\"\"\n",
    "    求最长连续[递增]*子序列\n",
    "    返回最长连续递增子序列的\n",
    "    ---------\n",
    "    *该[递增]序列并不要求始终严格增，而是最多允许两次递减\n",
    "    \"\"\"\n",
    "    decrease_cnt = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    sub_len = 0\n",
    "    longest_start = 0\n",
    "    longest_end = 0\n",
    "    longest_len = 0\n",
    "    decrease_point_0 = -1\n",
    "    decrease_point_1 = -1\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > arr[i-1]:\n",
    "            end_index += 1\n",
    "        else:\n",
    "            decrease_cnt += 1\n",
    "            if decrease_cnt == 1:\n",
    "                end_index += 1\n",
    "                decrease_point_0 = end_index\n",
    "            elif decrease_cnt == 2:\n",
    "                end_index += 1\n",
    "                decrease_point_1 = end_index\n",
    "            else:\n",
    "                sub_len = end_index - start_index + 1\n",
    "                if longest_len < sub_len:\n",
    "                    longest_len = sub_len\n",
    "                    longest_start = start_index\n",
    "                    longest_end = end_index\n",
    "                start_index = decrease_point_0\n",
    "                decrease_point_0 = decrease_point_1\n",
    "                decrease_point_1 = i\n",
    "                end_index = i\n",
    "                decrease_cnt = 2\n",
    "\n",
    "    sub_len = end_index - start_index + 1\n",
    "    if longest_len < sub_len:\n",
    "        longest_len = sub_len\n",
    "        longest_start = start_index\n",
    "        longest_end = end_index\n",
    "\n",
    "    return longest_start, longest_end\n",
    "\n",
    "\n",
    "def calculate_smoothed_cmean(f_name):\n",
    "    eps = 1e-12\n",
    "    current_cols = df.iloc[:, 1:]\n",
    "    voltage_col = df.iloc[:, :1].values\n",
    "\n",
    "    conductance_cols = current_cols.div(voltage_col + eps)\n",
    "    conductance_mean_col = conductance_cols.mean(axis=1).values\n",
    "    conductance_mean_col_head100 = conductance_mean_col[1: 101]\n",
    "    c_mean_smooth = Kalman1D(conductance_mean_col_head100)\n",
    "    # c_mean_smooth = c_mean_smooth.T[0]\n",
    "    return c_mean_smooth\n",
    "\n",
    "\n",
    "def Kalman1D(observations, damping=0):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "def weight_mapping(f_name, model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Map target weight to true weight. \n",
    "    The difference between the target weight and the true weight is due to two factors:\n",
    "    1.Random heat noises\n",
    "    2.Non-monotonic characteristics of conductance-Q curves\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To achieve the above functionality, we first have to measure the conductance-Q curve. \n",
    "    As the measurement is affected by noises, we measure the curve for several times and get\n",
    "    the mean value. \n",
    "    \"\"\"\n",
    "    c_mean_smooth = calculate_smoothed_cmean(f_name)\n",
    "    \n",
    "    max_index = c_mean_smooth.shape[0]\n",
    "    c_max = c_mean_smooth.max()\n",
    "    c_min = c_mean_smooth.min()\n",
    "\n",
    "    \"\"\"\n",
    "    We calculate the LCIS of the curve to find a monotonicly increasing part of the curve.\n",
    "    \"\"\"\n",
    "\n",
    "    start_index, end_index= LCIS(c_mean_smooth) \n",
    "    mono_len = end_index - start_index\n",
    "\n",
    "    #utility最大的曲线的值\n",
    "    required_len = 65\n",
    "    if mono_len < required_len: # Non-monotonic characteristics \n",
    "        increase_indices = np.arange(start_index, end_index, 1)\n",
    "        end_index = start_index + required_len\n",
    "        if end_index > max_index:\n",
    "            start_index = start_index - end_index + max_index\n",
    "            end_index = max_index\n",
    "        ratio = np.zeros(required_len)\n",
    "        for i in range(required_len):\n",
    "            idx = i + start_index\n",
    "            # 只有非递增区域的theta值受non-monotonic characteristics影响\n",
    "            ratio[i] = 1 if idx in increase_indices else (c_mean_smooth[idx]-c_min)/(c_max-c_min)\n",
    "\n",
    "    noise_sigma = mlesigma(f_name, start_index, end_index)\n",
    "    gassian_kernel = torch.distributions.Normal(0.0, noise_sigma)\n",
    "    with torch.no_grad():\n",
    "        for theta in model.parameters():\n",
    "            abstheta = torch.abs(theta) # 求参数的绝对值\n",
    "            normalized_theta = abstheta / (torch.max(abstheta) + 1e-8) # 归一化\n",
    "\n",
    "            theta_index = normalized_theta * (required_len-1)\n",
    "            theta_index = theta_index.type(torch.LongTensor) # 求各参数对应的下标位置\n",
    "            noise_index = normalized_theta * 100\n",
    "            noise_index = noise_index.type(torch.LongTensor)\n",
    "            noise_index[noise_index >= 100] = 99\n",
    "            \n",
    "            theta_ratio = torch.Tensor(ratio)[theta_index].cuda() # theta = theta * (c_real-c_min) / (c_max-c_min)\n",
    "\n",
    "            mul_ = theta_ratio * torch.exp(gassian_kernel.sample(theta.size()).cuda())\n",
    "            theta.mul_(mul_)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(4, 3)\n",
    "        def forward(self, xb):\n",
    "            xb = xb.view(xb.size(0), -1)\n",
    "            out = self.linear1(xb)\n",
    "            return out\n",
    "\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    print(model.state_dict())\n",
    "\n",
    "    f_name = 'I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx' #'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\n",
    "    weights_mapping(f_name, model, device='cuda')\n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a2677-317e-49fe-afe6-44f6efb835a1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-04T07:26:41.383875Z",
     "iopub.status.idle": "2023-07-04T07:26:41.384067Z",
     "shell.execute_reply": "2023-07-04T07:26:41.383982Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.383972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import norm\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "def mlesigma(f_name, st_indx, end_indx):\n",
    "    \"\"\"\n",
    "    我们引入 MLE 方法来估计忆阻器电导值与对应电压水平下电导均值的比值的 mu 和 sigma\n",
    "    函数返回 sigma 值，它的值反映了等效噪声水平的强度\n",
    "    '''\n",
    "    关于噪声分布形式设定与转换的说明：\n",
    "    X ~ logNorm(a, b)  # m, v 是对数正态分布本身的均值和方差\n",
    "    # 这说的是 X 是一个值永远为正的随机变量，只有取对数之后才是正态分布\n",
    "    即 logX ~ Norm(mu, sigma)\n",
    "\n",
    "    换算关系：\n",
    "    m = exp(mu + sigma^2/2)  # 注意到 m 是 >0 的\n",
    "    v = exp(2*mu + sigma^2) * exp(sigma^2 - 1)  \n",
    "\n",
    "    因此，一个随机变量 Y 如果服从正态分布，即 Y ~ Norm(mu, sigma)\n",
    "    就一定可以有一个 X = exp(Y), 使得 Y = logX ~ Norm(mu, sigma)\n",
    "\n",
    "    所以，一个正态分布随机变量取指数得到的变量就服从对数正态分布\n",
    "\n",
    "    具体到上面的代码，已知gaussian_kernel 服从正态分布，则 \n",
    "            torch.exp(noise_sigma*gassian_kernel.sample(noise_sigma.size())) \n",
    "    服从对数正态分布，其中我们施加的 mu, sigma 都是对应的正态分布的参数，而非对数正态分布的参数\n",
    "\n",
    "    又由于 (m, v) 和 (mu, sigma) 是一一映射，我们控制谁都一样，\n",
    "    所以，为了方便和统一，今后：\n",
    "    1）需要采样对数正态分布样本，都采取先用 mu, sigma 采样正态分布，再 exp 的方式\n",
    "    2）需要估计对数正态分布参数，都采取先对对数正态分布样本取 log，再用 norm.fit 拟合的方式\n",
    "    '''\n",
    "    \"\"\"\n",
    "    data = pd.read_excel(f_name, engine='openpyxl').iloc[:, 1: 101]\n",
    "    data = data.iloc[:, st_indx: end_indx+1]\n",
    "    \n",
    "    conductance = data.drop(['voltage'], axis=1).div(data['voltage'], axis=0)\n",
    "    conductance = conductance[(conductance!=np.inf).all(axis=1)]\n",
    "    \n",
    "    conductance_by_mean = np.array(conductance.div(conductance.mean(axis=1), axis=0)).reshape(1, -1)[0]\n",
    "    samples = np.log(conductance_by_mean)\n",
    "    samples_fine = np.delete(conductance_by_mean, np.isfinite(samples)==False)\n",
    "\n",
    "    # mu_hat, sigma_hat = norm.fit(samples_fine)  # 通过极大似然估计得到 sigma\\hat，不固定均值为 0\n",
    "    mu_hat, sigma_hat = norm.fit(samples_fine, floc=0)  # 通过极大似然估计得到 sigma\\hat，固定均值为 0\n",
    "    \n",
    "    return sigma_hat # 我们只关注 sigma_hat\n",
    "\n",
    "\n",
    "def LCIS(arr):\n",
    "    \"\"\"\n",
    "    求最长连续[递增]*子序列\n",
    "    返回最长连续递增子序列的\n",
    "    ---------\n",
    "    *该[递增]序列并不要求始终严格增，而是最多允许两次递减\n",
    "    \"\"\"\n",
    "    decrease_cnt = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    sub_len = 0\n",
    "    longest_start = 0\n",
    "    longest_end = 0\n",
    "    longest_len = 0\n",
    "    decrease_point_0 = -1\n",
    "    decrease_point_1 = -1\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > arr[i-1]:\n",
    "            end_index += 1\n",
    "        else:\n",
    "            decrease_cnt += 1\n",
    "            if decrease_cnt == 1:\n",
    "                end_index += 1\n",
    "                decrease_point_0 = end_index\n",
    "            elif decrease_cnt == 2:\n",
    "                end_index += 1\n",
    "                decrease_point_1 = end_index\n",
    "            else:\n",
    "                sub_len = end_index - start_index + 1\n",
    "                if longest_len < sub_len:\n",
    "                    longest_len = sub_len\n",
    "                    longest_start = start_index\n",
    "                    longest_end = end_index\n",
    "                start_index = decrease_point_0\n",
    "                decrease_point_0 = decrease_point_1\n",
    "                decrease_point_1 = i\n",
    "                end_index = i\n",
    "                decrease_cnt = 2\n",
    "\n",
    "    sub_len = end_index - start_index + 1\n",
    "    if longest_len < sub_len:\n",
    "        longest_len = sub_len\n",
    "        longest_start = start_index\n",
    "        longest_end = end_index\n",
    "\n",
    "    return longest_start, longest_end\n",
    "\n",
    "\n",
    "def calculate_smoothed_cmean(f_name):\n",
    "    eps = 1e-12\n",
    "    current_cols = df.iloc[:, 1:]\n",
    "    voltage_col = df.iloc[:, :1].values\n",
    "\n",
    "    conductance_cols = current_cols.div(voltage_col + eps)\n",
    "    conductance_mean_col = conductance_cols.mean(axis=1).values\n",
    "    conductance_mean_col_head100 = conductance_mean_col[1: 101]\n",
    "    c_mean_smooth = Kalman1D(conductance_mean_col_head100)\n",
    "    # c_mean_smooth = c_mean_smooth.T[0]\n",
    "    return c_mean_smooth\n",
    "\n",
    "\n",
    "def Kalman1D(observations, damping=0):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "def weight_mapping(f_name, model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Map target weight to true weight. \n",
    "    The difference between the target weight and the true weight is due to two factors:\n",
    "    1.Random heat noises\n",
    "    2.Non-monotonic characteristics of conductance-Q curves\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To achieve the above functionality, we first have to measure the conductance-Q curve. \n",
    "    As the measurement is affected by noises, we measure the curve for several times and get\n",
    "    the mean value. \n",
    "    \"\"\"\n",
    "    c_mean_smooth = calculate_smoothed_cmean(f_name)\n",
    "    \n",
    "    max_index = c_mean_smooth.shape[0]\n",
    "    c_max = c_mean_smooth.max()\n",
    "    c_min = c_mean_smooth.min()\n",
    "\n",
    "    \"\"\"\n",
    "    We calculate the LCIS of the curve to find a monotonicly increasing part of the curve.\n",
    "    \"\"\"\n",
    "\n",
    "    start_index, end_index= LCIS(c_mean_smooth) \n",
    "    mono_len = end_index - start_index\n",
    "\n",
    "    #utility最大的曲线的值\n",
    "    required_len = 65\n",
    "    if mono_len < required_len: # Non-monotonic characteristics \n",
    "        increase_indices = np.arange(start_index, end_index, 1)\n",
    "        end_index = start_index + required_len\n",
    "        if end_index > max_index:\n",
    "            start_index = start_index - end_index + max_index\n",
    "            end_index = max_index\n",
    "        ratio = np.zeros(required_len)\n",
    "        for i in range(required_len):\n",
    "            idx = i + start_index\n",
    "            # 只有非递增区域的theta值受non-monotonic characteristics影响\n",
    "            ratio[i] = 1 if idx in increase_indices else (c_mean_smooth[idx]-c_min)/(c_max-c_min)\n",
    "\n",
    "    noise_sigma = mlesigma(f_name, start_index, end_index)\n",
    "    gassian_kernel = torch.distributions.Normal(0.0, noise_sigma)\n",
    "    with torch.no_grad():\n",
    "        for theta in model.parameters():\n",
    "            abstheta = torch.abs(theta) # 求参数的绝对值\n",
    "            normalized_theta = abstheta / (torch.max(abstheta) + 1e-8) # 归一化\n",
    "\n",
    "            theta_index = normalized_theta * (required_len-1)\n",
    "            theta_index = theta_index.type(torch.LongTensor) # 求各参数对应的下标位置\n",
    "            noise_index = normalized_theta * 100\n",
    "            noise_index = noise_index.type(torch.LongTensor)\n",
    "            noise_index[noise_index >= 100] = 99\n",
    "            \n",
    "            theta_ratio = torch.Tensor(ratio)[theta_index].cuda() # theta = theta * (c_real-c_min) / (c_max-c_min)\n",
    "\n",
    "            mul_ = theta_ratio * torch.exp(gassian_kernel.sample(theta.size()).cuda())\n",
    "            theta.mul_(mul_)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(4, 3)\n",
    "        def forward(self, xb):\n",
    "            xb = xb.view(xb.size(0), -1)\n",
    "            out = self.linear1(xb)\n",
    "            return out\n",
    "\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    print(model.state_dict())\n",
    "\n",
    "    f_name = 'I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx' #'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\n",
    "    weights_mapping(f_name, model, device='cuda')\n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ea7b9-bfdc-40bf-9559-7c5f520a00f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e50403-b6da-4d42-a398-de06a6a9a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b097c-17e3-41ea-8ff8-bc2c3ffb695a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa08ac-6905-442b-a3b3-eb0b800c31e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebba7e-3304-4866-b879-53564382c44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed356599-e3a3-4eba-bf97-ffe7c75fff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43d715-04df-4cea-97fc-17638ccdffdd",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-04T07:26:41.384855Z",
     "iopub.status.idle": "2023-07-04T07:26:41.385050Z",
     "shell.execute_reply": "2023-07-04T07:26:41.384965Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.384956Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def mlesigma(f_name):\n",
    "    # df = pd.read_excel('I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx') #0.55\n",
    "    # df = pd.read_excel('I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx') #0.33\n",
    "    # df = pd.read_excel('I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V.xlsx') #0.14\n",
    "\n",
    "    # data = pd.read_excel('../hardware_noise/I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V.xlsx') #0.14\n",
    "    # data = pd.read_excel('../hardware_noise/I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx') #0.33\n",
    "    data = pd.read_excel(f_name, engine='openpyxl') #0.55\n",
    "\n",
    "    conductance = data.drop(['voltage'], axis=1).div(data['voltage'], axis=0)\n",
    "\n",
    "    conductance = conductance[(conductance!=np.inf).all(axis=1)]\n",
    "\n",
    "    # (conductance.div(conductance.mean(axis=1), axis=0)<0).all(axis=0)\n",
    "\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    conductance_by_mean = np.array(conductance.div(conductance.mean(axis=1), axis=0)).reshape(1, -1)[0]\n",
    "    samples = np.log(conductance_by_mean)\n",
    "    samples_fine = np.delete(conductance_by_mean, np.isfinite(samples)==False)\n",
    "\n",
    "    # print(norm.fit(samples_fine))  # 返回极大似然估计\n",
    "    # print(norm.fit(samples_fine, floc=0))  # 返回极大似然估计 固定均值为 0\n",
    "    \n",
    "    return norm.fit(samples_fine)[1]\n",
    "\n",
    "\n",
    "def weights_mapping(f_name, model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Map target weight to true weight. \n",
    "    The difference between the target weight and the true weight is due to two factors:\n",
    "    1.Random heat noises\n",
    "    2.Non-monotonic characteristics of conductance-Q curves\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    To achieve the above functionality, we first have to measure the conductance-Q curve. \n",
    "    As the measurement is affected by noises, we measure the curve for several times and get\n",
    "    the mean value. \n",
    "    \"\"\"\n",
    "    c_mean_smooth, c_std_smooth = calculate(f_name)\n",
    "    max_index = c_mean_smooth.shape[0]\n",
    "    c_max = c_mean_smooth.max()\n",
    "    c_min = c_mean_smooth.min()\n",
    "\n",
    "    \"\"\"\n",
    "    We calculate the LCIS of the curve to find a monotonicly increasing part of the curve.\n",
    "    \"\"\"\n",
    "\n",
    "    start_index, end_index= LCIS(c_mean_smooth) \n",
    "\n",
    "    mono_len = end_index - start_index\n",
    "\n",
    "    #utility最大的曲线的值\n",
    "    required_len = 65\n",
    "    if mono_len < required_len: # Non-monotonic characteristics \n",
    "        increase_indices = np.arange(start_index, end_index, 1)\n",
    "        end_index = start_index + required_len\n",
    "        if end_index > max_index:\n",
    "            start_index = start_index - end_index + max_index\n",
    "            end_index = max_index\n",
    "        ratio = np.zeros(required_len)\n",
    "        for i in range(required_len):\n",
    "            idx = i+start_index\n",
    "            # 只有非递增区域的theta值受non-monotonic characteristics影响\n",
    "            ratio[i] = 1 if idx in increase_indices else (c_mean_smooth[idx]-c_min)/(c_max-c_min)\n",
    "\n",
    "    x = np.arange(start_index, end_index, 1)\n",
    "    y = c_mean_smooth[start_index:end_index]\n",
    "\n",
    "    interp_func_mean = interpolate.splrep(x, y, s=0)\n",
    "    y_std = c_std_smooth[start_index:end_index]\n",
    "    interp_func_std = interpolate.splrep(x, y_std, s=0)\n",
    "    xfit = np.linspace(start_index, end_index, 100)\n",
    "    yfit = interpolate.splev(xfit, interp_func_mean, der=0)\n",
    "    yfit_std = interpolate.splev(xfit, interp_func_std, der=0) #计算取样点的插值结果\n",
    "\n",
    "    noise_sigma = mlesigma(f_name)\n",
    "    gassian_kernel = torch.distributions.Normal(0.0, noise_sigma)\n",
    "    with torch.no_grad():\n",
    "        for theta in model.parameters():\n",
    "            abstheta = torch.abs(theta) # 求参数的绝对值\n",
    "            normalized_theta = abstheta / (torch.max(abstheta)+1e-8) # 归一化\n",
    "\n",
    "            theta_index = normalized_theta*(required_len-1)\n",
    "            theta_index = theta_index.type(torch.LongTensor) # 求各参数对应的下标位置\n",
    "\n",
    "            noise_index = normalized_theta*100\n",
    "            noise_index = noise_index.type(torch.LongTensor)\n",
    "            noise_index[noise_index>=100]=99\n",
    "\n",
    "            theta_ratio = torch.Tensor(ratio)[theta_index].cuda() # theta = theta * (c_real-c_min) / (c_max-c_min)\n",
    "\n",
    "            # noise_sigma = torch.Tensor(np.log(1+np.square(yfit_std/yfit)))[noise_index]\n",
    "            # theta.mul_(to_device(theta_ratio * torch.exp(noise_sigma*gassian_kernel.sample(noise_sigma.size())), device))\n",
    "            # print(noise_sigma.shape)\n",
    "            \n",
    "            mul_ = theta_ratio * torch.exp(gassian_kernel.sample(theta.size()).cuda())\n",
    "            # theta = theta.cuda()\n",
    "            theta.mul_(mul_)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # theta.mul_(to_device(theta_ratio*torch.exp(noise_sigma), device))\n",
    "\n",
    "\n",
    "\n",
    "def calculate(f_name):\n",
    "    # df = pd.read_excel(file)\n",
    "    # df = pd.read_excel('I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx') #0.55\n",
    "    # df = pd.read_excel('I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx') #0.33\n",
    "    # df = pd.read_excel('./hardware_data/I-V_data_0.7um_length_200nm_diameter_NA_third_etch_10min_Pb_ED_1h_180C_MAI_no_100nm_Ag_memory_1V_carbon_paste.xlsx') #0.14\n",
    "    # sunqiao/OpenPCDet/tools/hardware_noise/hardware_data/I-V_data_0.7um_length_200nm_diameter_NA_third_etch_10min_Pb_ED_1h_180C_MAI_no_100nm_Ag_memory_1V_carbon_paste.xlsx\n",
    "    df = pd.read_excel(f_name)\n",
    "    dropcolumn = []\n",
    "    for i in range(len(df.columns)):\n",
    "        if 'Unnamed' in df.columns[i]:\n",
    "            dropcolumn.append(df.columns[i])\n",
    "            \n",
    "\n",
    "    df = df.drop(columns=dropcolumn)\n",
    "\n",
    "    #calculate mean and std current for each row and remove the NAN\n",
    "    voltage = df['voltage']\n",
    "    current_mean_list = []\n",
    "    current_std_list = []\n",
    "    for i in range(len(voltage)):\n",
    "        current_row = df.iloc[[i]].to_numpy()\n",
    "        current_row = current_row[~np.isnan(current_row)]\n",
    "        \n",
    "        low_percentile = np.percentile(current_row, 25)\n",
    "        high_percentile = np.percentile(current_row, 75)\n",
    "        current_row = current_row[(current_row >=low_percentile ) & (current_row <= high_percentile)]\n",
    "        \n",
    "        current_mean = np.mean(current_row)\n",
    "        \n",
    "        \n",
    "        current_std = np.std(current_row)\n",
    "        current_mean_list.append(current_mean)\n",
    "        current_std_list.append(current_std)\n",
    "\n",
    "    current_mean_list = np.array(current_mean_list)\n",
    "    current_std_list = np.array(current_std_list)\n",
    "\n",
    "    voltage_list = df['voltage'].to_numpy()\n",
    "\n",
    "    conductance_mean = current_mean_list/(voltage_list+1e-9)\n",
    "    conductance_std  = current_std_list/(voltage_list+1e-9)\n",
    "\n",
    "    c_mean_smooth = conductance_mean[1:100] \n",
    "\n",
    "    c_std_smooth = conductance_std[1:100]\n",
    "\n",
    "\n",
    "    c_mean_smooth = Kalman1D(c_mean_smooth,damping=1)\n",
    "    c_std_smooth  = Kalman1D(c_std_smooth,damping=1)\n",
    "\n",
    "\n",
    "    return c_mean_smooth, c_std_smooth\n",
    "\n",
    "def Kalman1D(observations,damping=0):\n",
    "    # To return the smoothed time series data\n",
    "    observation_covariance = damping\n",
    "    initial_value_guess = observations[0]\n",
    "    transition_matrix = 1\n",
    "    transition_covariance = 0.1\n",
    "    initial_value_guess\n",
    "    kf = KalmanFilter(\n",
    "            initial_state_mean=initial_value_guess,\n",
    "            initial_state_covariance=observation_covariance,\n",
    "            observation_covariance=observation_covariance,\n",
    "            transition_covariance=transition_covariance,\n",
    "            transition_matrices=transition_matrix\n",
    "        )\n",
    "    pred_state, state_cov = kf.smooth(observations)\n",
    "    return pred_state\n",
    "\n",
    "def LCIS(arr): # 求最长连续[递增]子序列，该[递增]序列中最多允许两次递减\n",
    "    decrease_cnt = 0\n",
    "    start_index = 0\n",
    "    end_index = 0\n",
    "    sub_len = 0\n",
    "    longest_start = 0\n",
    "    longest_end = 0\n",
    "    longest_len = 0\n",
    "    decrease_point_0 = -1\n",
    "    decrease_point_1 = -1\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] > arr[i-1]:\n",
    "            end_index += 1\n",
    "        else:\n",
    "            decrease_cnt += 1\n",
    "            if decrease_cnt == 1:\n",
    "                end_index += 1\n",
    "                decrease_point_0 = end_index\n",
    "\n",
    "            elif decrease_cnt == 2:\n",
    "                end_index += 1\n",
    "                decrease_point_1 = end_index\n",
    "                \n",
    "            else:\n",
    "                sub_len = end_index - start_index + 1\n",
    "                if longest_len < sub_len:\n",
    "                    longest_len = sub_len\n",
    "                    longest_start = start_index\n",
    "                    longest_end = end_index\n",
    "                start_index = decrease_point_0\n",
    "                decrease_point_0 = decrease_point_1\n",
    "                decrease_point_1 = i\n",
    "                end_index = i\n",
    "                decrease_cnt = 2\n",
    "\n",
    "    sub_len = end_index - start_index + 1\n",
    "    if longest_len < sub_len:\n",
    "        longest_len = sub_len\n",
    "        longest_start = start_index\n",
    "        longest_end = end_index\n",
    "\n",
    "    return longest_start, longest_end\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "# def main():\n",
    "#     weights_mapping(0, 1, model=model, device='cpu')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.linear1 = nn.Linear(4, 3)\n",
    "        def forward(self, xb):\n",
    "            xb = xb.view(xb.size(0), -1)\n",
    "            out = self.linear1(xb)\n",
    "            return out\n",
    "\n",
    "    model = Net()\n",
    "    model.cuda()\n",
    "    print(model.state_dict())\n",
    "\n",
    "    f_name = 'I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx' #'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\n",
    "    weights_mapping(f_name, model, device='cuda')\n",
    "    print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1eedc-02fc-4550-9cb8-f5c76134b754",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-04T07:26:41.385602Z",
     "iopub.status.idle": "2023-07-04T07:26:41.385905Z",
     "shell.execute_reply": "2023-07-04T07:26:41.385822Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.385813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(4, 3)\n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(xb.size(0), -1)\n",
    "        out = self.linear1(xb)\n",
    "        return out\n",
    "\n",
    "model = Net()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a70b4-099c-4505-8c35-4014fe9bdeee",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-04T07:26:41.386418Z",
     "iopub.status.idle": "2023-07-04T07:26:41.386702Z",
     "shell.execute_reply": "2023-07-04T07:26:41.386613Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.386604Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e94c58-d7a7-47c3-8fb4-589e38bb86b9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2023-07-04T07:26:41.387121Z",
     "iopub.status.idle": "2023-07-04T07:26:41.387287Z",
     "shell.execute_reply": "2023-07-04T07:26:41.387210Z",
     "shell.execute_reply.started": "2023-07-04T07:26:41.387202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights_mapping(0, 1, model=model, device='cpu')\n",
    "f_name = 'I-V_data_30min_AAO_5min second etch_15min_Pb_ED_3h_180C_MAI_200nm_Ag_memory_6V.xlsx' #'I-V_data_25min_AAO_10min_Pb_ED_1h_180C_MAI_memory_8V_2.xlsx'\n",
    "\n",
    "weights_mapping(f_name, model, device='cuda')\n",
    "model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
