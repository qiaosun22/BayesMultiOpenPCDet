{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262d683d-4c38-4ecc-92b0-e8fbaa386662",
   "metadata": {},
   "source": [
    "重训 ERM，并在 ERM 上校验是否会训崩\n",
    "\n",
    "已完成：调整学习率，降低代码复杂度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f7a3b7-423e-48d0-aef4-3d6f53af4196",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2023-11-15T03:12:26.277174Z",
     "iopub.status.busy": "2023-11-15T03:12:26.276958Z",
     "iopub.status.idle": "2023-11-15T03:23:43.954400Z",
     "shell.execute_reply": "2023-11-15T03:23:43.953228Z",
     "shell.execute_reply.started": "2023-11-15T03:12:26.277153Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:12:33,946   INFO  Database filter by min points Car: 14357 => 13532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU:0\n",
      "=============\n",
      "0.23 0.77 0.68\n",
      "=============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:12:33,950   INFO  Database filter by min points Pedestrian: 2207 => 2168\n",
      "2023-11-15 11:12:33,951   INFO  Database filter by min points Cyclist: 734 => 705\n",
      "2023-11-15 11:12:33,973   INFO  Database filter by difficulty Car: 13532 => 10759\n",
      "2023-11-15 11:12:33,978   INFO  Database filter by difficulty Pedestrian: 2168 => 2075\n",
      "2023-11-15 11:12:33,980   INFO  Database filter by difficulty Cyclist: 705 => 581\n",
      "2023-11-15 11:12:33,985   INFO  Loading KITTI dataset\n",
      "2023-11-15 11:12:34,085   INFO  Total samples for KITTI dataset: 3712\n",
      "/home/pai/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2023-11-15 11:12:35,889   INFO  **********************Start training cfgs/kitti_models/pointpillar_bayes(default)**********************\n",
      "epochs:   0%|          | 0/791 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_epoch: 10/800, cur_it: 1/464, lr: 4.96922e-05, loss: 0.922749\n",
      "cur_epoch: 10/800, cur_it: 2/464, lr: 4.87764e-05, loss: 0.968092\n",
      "cur_epoch: 10/800, cur_it: 3/464, lr: 4.72752e-05, loss: 0.967601\n",
      "cur_epoch: 10/800, cur_it: 4/464, lr: 4.52254e-05, loss: 0.952636\n",
      "cur_epoch: 10/800, cur_it: 5/464, lr: 4.26777e-05, loss: 0.971318\n",
      "cur_epoch: 10/800, cur_it: 6/464, lr: 3.96946e-05, loss: 0.977248\n",
      "cur_epoch: 10/800, cur_it: 7/464, lr: 3.63498e-05, loss: 0.975253\n",
      "cur_epoch: 10/800, cur_it: 8/464, lr: 3.27254e-05, loss: 0.974787\n",
      "cur_epoch: 10/800, cur_it: 9/464, lr: 2.89109e-05, loss: 0.972776\n",
      "cur_epoch: 10/800, cur_it: 10/464, lr: 2.5e-05, loss: 0.96646\n",
      "cur_epoch: 10/800, cur_it: 11/464, lr: 2.10891e-05, loss: 0.958635\n",
      "cur_epoch: 10/800, cur_it: 12/464, lr: 1.72746e-05, loss: 0.954543\n",
      "cur_epoch: 10/800, cur_it: 13/464, lr: 1.36502e-05, loss: 0.952143\n",
      "cur_epoch: 10/800, cur_it: 14/464, lr: 1.03054e-05, loss: 0.952076\n",
      "cur_epoch: 10/800, cur_it: 15/464, lr: 7.32233e-06, loss: 0.94796\n",
      "cur_epoch: 10/800, cur_it: 16/464, lr: 4.77458e-06, loss: 0.944023\n",
      "cur_epoch: 10/800, cur_it: 17/464, lr: 2.72484e-06, loss: 0.941961\n",
      "cur_epoch: 10/800, cur_it: 18/464, lr: 1.22359e-06, loss: 0.940479\n",
      "cur_epoch: 10/800, cur_it: 19/464, lr: 3.07791e-07, loss: 0.937471\n",
      "cur_epoch: 10/800, cur_it: 20/464, lr: 0.0, loss: 0.936438\n",
      "cur_epoch: 10/800, cur_it: 21/464, lr: 3.07791e-07, loss: 0.93111\n",
      "cur_epoch: 10/800, cur_it: 22/464, lr: 1.22359e-06, loss: 0.930092\n",
      "cur_epoch: 10/800, cur_it: 23/464, lr: 2.72484e-06, loss: 0.924354\n",
      "cur_epoch: 10/800, cur_it: 24/464, lr: 4.77458e-06, loss: 0.924413\n",
      "cur_epoch: 10/800, cur_it: 25/464, lr: 7.32233e-06, loss: 0.923927\n",
      "cur_epoch: 10/800, cur_it: 26/464, lr: 1.03054e-05, loss: 0.923195\n",
      "cur_epoch: 10/800, cur_it: 27/464, lr: 1.36502e-05, loss: 0.923494\n",
      "cur_epoch: 10/800, cur_it: 28/464, lr: 1.72746e-05, loss: 0.920246\n",
      "cur_epoch: 10/800, cur_it: 29/464, lr: 2.10891e-05, loss: 0.919431\n",
      "cur_epoch: 10/800, cur_it: 30/464, lr: 2.5e-05, loss: 0.917635\n",
      "cur_epoch: 10/800, cur_it: 31/464, lr: 2.89109e-05, loss: 0.914657\n",
      "cur_epoch: 10/800, cur_it: 32/464, lr: 3.27254e-05, loss: 0.913528\n",
      "cur_epoch: 10/800, cur_it: 33/464, lr: 3.63498e-05, loss: 0.913283\n",
      "cur_epoch: 10/800, cur_it: 34/464, lr: 3.96946e-05, loss: 0.912727\n",
      "cur_epoch: 10/800, cur_it: 35/464, lr: 4.26777e-05, loss: 0.912796\n",
      "cur_epoch: 10/800, cur_it: 36/464, lr: 4.52254e-05, loss: 0.910369\n",
      "cur_epoch: 10/800, cur_it: 37/464, lr: 4.72752e-05, loss: 0.912634\n",
      "cur_epoch: 10/800, cur_it: 38/464, lr: 4.87764e-05, loss: 0.911941\n",
      "cur_epoch: 10/800, cur_it: 39/464, lr: 4.96922e-05, loss: 0.913609\n",
      "cur_epoch: 10/800, cur_it: 40/464, lr: 5e-05, loss: 0.915806\n",
      "cur_epoch: 10/800, cur_it: 41/464, lr: 4.96922e-05, loss: 0.916975\n",
      "cur_epoch: 10/800, cur_it: 42/464, lr: 4.87764e-05, loss: 0.91917\n",
      "cur_epoch: 10/800, cur_it: 43/464, lr: 4.72752e-05, loss: 0.919188\n",
      "cur_epoch: 10/800, cur_it: 44/464, lr: 4.52254e-05, loss: 0.920113\n",
      "cur_epoch: 10/800, cur_it: 45/464, lr: 4.26777e-05, loss: 0.921013\n",
      "cur_epoch: 10/800, cur_it: 46/464, lr: 3.96946e-05, loss: 0.921945\n",
      "cur_epoch: 10/800, cur_it: 47/464, lr: 3.63498e-05, loss: 0.922974\n",
      "cur_epoch: 10/800, cur_it: 48/464, lr: 3.27254e-05, loss: 0.921596\n",
      "cur_epoch: 10/800, cur_it: 49/464, lr: 2.89109e-05, loss: 0.92713\n",
      "cur_epoch: 10/800, cur_it: 50/464, lr: 2.5e-05, loss: 0.927082\n",
      "cur_epoch: 10/800, cur_it: 51/464, lr: 2.10891e-05, loss: 0.926618\n",
      "cur_epoch: 10/800, cur_it: 52/464, lr: 1.72746e-05, loss: 0.925529\n",
      "cur_epoch: 10/800, cur_it: 53/464, lr: 1.36502e-05, loss: 0.926362\n",
      "cur_epoch: 10/800, cur_it: 54/464, lr: 1.03054e-05, loss: 0.924724\n",
      "cur_epoch: 10/800, cur_it: 55/464, lr: 7.32233e-06, loss: 0.924589\n",
      "cur_epoch: 10/800, cur_it: 56/464, lr: 4.77458e-06, loss: 0.924992\n",
      "cur_epoch: 10/800, cur_it: 57/464, lr: 2.72484e-06, loss: 0.923812\n",
      "cur_epoch: 10/800, cur_it: 58/464, lr: 1.22359e-06, loss: 0.922055\n",
      "cur_epoch: 10/800, cur_it: 59/464, lr: 3.07791e-07, loss: 0.921235\n",
      "cur_epoch: 10/800, cur_it: 60/464, lr: 0.0, loss: 0.920525\n",
      "cur_epoch: 10/800, cur_it: 61/464, lr: 3.07791e-07, loss: 0.922775\n",
      "cur_epoch: 10/800, cur_it: 62/464, lr: 1.22359e-06, loss: 0.923369\n",
      "cur_epoch: 10/800, cur_it: 63/464, lr: 2.72484e-06, loss: 0.923834\n",
      "cur_epoch: 10/800, cur_it: 64/464, lr: 4.77458e-06, loss: 0.924795\n",
      "cur_epoch: 10/800, cur_it: 65/464, lr: 7.32233e-06, loss: 0.923862\n",
      "cur_epoch: 10/800, cur_it: 66/464, lr: 1.03054e-05, loss: 0.923976\n",
      "cur_epoch: 10/800, cur_it: 67/464, lr: 1.36502e-05, loss: 0.923928\n",
      "cur_epoch: 10/800, cur_it: 68/464, lr: 1.72746e-05, loss: 0.923348\n",
      "cur_epoch: 10/800, cur_it: 69/464, lr: 2.10891e-05, loss: 0.922273\n",
      "cur_epoch: 10/800, cur_it: 70/464, lr: 2.5e-05, loss: 0.921962\n",
      "cur_epoch: 10/800, cur_it: 71/464, lr: 2.89109e-05, loss: 0.922513\n",
      "cur_epoch: 10/800, cur_it: 72/464, lr: 3.27254e-05, loss: 0.922479\n",
      "cur_epoch: 10/800, cur_it: 73/464, lr: 3.63498e-05, loss: 0.921991\n",
      "cur_epoch: 10/800, cur_it: 74/464, lr: 3.96946e-05, loss: 0.919899\n",
      "cur_epoch: 10/800, cur_it: 75/464, lr: 4.26777e-05, loss: 0.919552\n",
      "cur_epoch: 10/800, cur_it: 76/464, lr: 4.52254e-05, loss: 0.91981\n",
      "cur_epoch: 10/800, cur_it: 77/464, lr: 4.72752e-05, loss: 0.920862\n",
      "cur_epoch: 10/800, cur_it: 78/464, lr: 4.87764e-05, loss: 0.921885\n",
      "cur_epoch: 10/800, cur_it: 79/464, lr: 4.96922e-05, loss: 0.9225\n",
      "cur_epoch: 10/800, cur_it: 80/464, lr: 5e-05, loss: 0.922996\n",
      "cur_epoch: 10/800, cur_it: 81/464, lr: 4.96922e-05, loss: 0.922782\n",
      "cur_epoch: 10/800, cur_it: 82/464, lr: 4.87764e-05, loss: 0.923528\n",
      "cur_epoch: 10/800, cur_it: 83/464, lr: 4.72752e-05, loss: 0.922989\n",
      "cur_epoch: 10/800, cur_it: 84/464, lr: 4.52254e-05, loss: 0.923541\n",
      "cur_epoch: 10/800, cur_it: 85/464, lr: 4.26777e-05, loss: 0.924161\n",
      "cur_epoch: 10/800, cur_it: 86/464, lr: 3.96946e-05, loss: 0.923339\n",
      "cur_epoch: 10/800, cur_it: 87/464, lr: 3.63498e-05, loss: 0.922291\n",
      "cur_epoch: 10/800, cur_it: 88/464, lr: 3.27254e-05, loss: 0.921823\n",
      "cur_epoch: 10/800, cur_it: 89/464, lr: 2.89109e-05, loss: 0.922332\n",
      "cur_epoch: 10/800, cur_it: 90/464, lr: 2.5e-05, loss: 0.923975\n",
      "cur_epoch: 10/800, cur_it: 91/464, lr: 2.10891e-05, loss: 0.924244\n",
      "cur_epoch: 10/800, cur_it: 92/464, lr: 1.72746e-05, loss: 0.925151\n",
      "cur_epoch: 10/800, cur_it: 93/464, lr: 1.36502e-05, loss: 0.924453\n",
      "cur_epoch: 10/800, cur_it: 94/464, lr: 1.03054e-05, loss: 0.92359\n",
      "cur_epoch: 10/800, cur_it: 95/464, lr: 7.32233e-06, loss: 0.923729\n",
      "cur_epoch: 10/800, cur_it: 96/464, lr: 4.77458e-06, loss: 0.924169\n",
      "cur_epoch: 10/800, cur_it: 97/464, lr: 2.72484e-06, loss: 0.924152\n",
      "cur_epoch: 10/800, cur_it: 98/464, lr: 1.22359e-06, loss: 0.924008\n",
      "cur_epoch: 10/800, cur_it: 99/464, lr: 3.07791e-07, loss: 0.923986\n",
      "cur_epoch: 10/800, cur_it: 100/464, lr: 0.0, loss: 0.924073\n",
      "cur_epoch: 10/800, cur_it: 101/464, lr: 3.07791e-07, loss: 0.924842\n",
      "cur_epoch: 10/800, cur_it: 102/464, lr: 1.22359e-06, loss: 0.925047\n",
      "cur_epoch: 10/800, cur_it: 103/464, lr: 2.72484e-06, loss: 0.924917\n",
      "cur_epoch: 10/800, cur_it: 104/464, lr: 4.77458e-06, loss: 0.92452\n",
      "cur_epoch: 10/800, cur_it: 105/464, lr: 7.32233e-06, loss: 0.924425\n",
      "cur_epoch: 10/800, cur_it: 106/464, lr: 1.03054e-05, loss: 0.924754\n",
      "cur_epoch: 10/800, cur_it: 107/464, lr: 1.36502e-05, loss: 0.924531\n",
      "cur_epoch: 10/800, cur_it: 108/464, lr: 1.72746e-05, loss: 0.924197\n",
      "cur_epoch: 10/800, cur_it: 109/464, lr: 2.10891e-05, loss: 0.923933\n",
      "cur_epoch: 10/800, cur_it: 110/464, lr: 2.5e-05, loss: 0.924142\n",
      "cur_epoch: 10/800, cur_it: 111/464, lr: 2.89109e-05, loss: 0.923904\n",
      "cur_epoch: 10/800, cur_it: 112/464, lr: 3.27254e-05, loss: 0.923747\n",
      "cur_epoch: 10/800, cur_it: 113/464, lr: 3.63498e-05, loss: 0.923479\n",
      "cur_epoch: 10/800, cur_it: 114/464, lr: 3.96946e-05, loss: 0.923391\n",
      "cur_epoch: 10/800, cur_it: 115/464, lr: 4.26777e-05, loss: 0.92306\n",
      "cur_epoch: 10/800, cur_it: 116/464, lr: 4.52254e-05, loss: 0.922521\n",
      "cur_epoch: 10/800, cur_it: 117/464, lr: 4.72752e-05, loss: 0.921722\n",
      "cur_epoch: 10/800, cur_it: 118/464, lr: 4.87764e-05, loss: 0.921934\n",
      "cur_epoch: 10/800, cur_it: 119/464, lr: 4.96922e-05, loss: 0.922771\n",
      "cur_epoch: 10/800, cur_it: 120/464, lr: 5e-05, loss: 0.922928\n",
      "cur_epoch: 10/800, cur_it: 121/464, lr: 4.96922e-05, loss: 0.923083\n",
      "cur_epoch: 10/800, cur_it: 122/464, lr: 4.87764e-05, loss: 0.922419\n",
      "cur_epoch: 10/800, cur_it: 123/464, lr: 4.72752e-05, loss: 0.921745\n",
      "cur_epoch: 10/800, cur_it: 124/464, lr: 4.52254e-05, loss: 0.921626\n",
      "cur_epoch: 10/800, cur_it: 125/464, lr: 4.26777e-05, loss: 0.922189\n",
      "cur_epoch: 10/800, cur_it: 126/464, lr: 3.96946e-05, loss: 0.921474\n",
      "cur_epoch: 10/800, cur_it: 127/464, lr: 3.63498e-05, loss: 0.921943\n",
      "cur_epoch: 10/800, cur_it: 128/464, lr: 3.27254e-05, loss: 0.92149\n",
      "cur_epoch: 10/800, cur_it: 129/464, lr: 2.89109e-05, loss: 0.921213\n",
      "cur_epoch: 10/800, cur_it: 130/464, lr: 2.5e-05, loss: 0.922262\n",
      "cur_epoch: 10/800, cur_it: 131/464, lr: 2.10891e-05, loss: 0.922382\n",
      "cur_epoch: 10/800, cur_it: 132/464, lr: 1.72746e-05, loss: 0.922244\n",
      "cur_epoch: 10/800, cur_it: 133/464, lr: 1.36502e-05, loss: 0.922567\n",
      "cur_epoch: 10/800, cur_it: 134/464, lr: 1.03054e-05, loss: 0.922281\n",
      "cur_epoch: 10/800, cur_it: 135/464, lr: 7.32233e-06, loss: 0.923184\n",
      "cur_epoch: 10/800, cur_it: 136/464, lr: 4.77458e-06, loss: 0.923162\n",
      "cur_epoch: 10/800, cur_it: 137/464, lr: 2.72484e-06, loss: 0.923825\n",
      "cur_epoch: 10/800, cur_it: 138/464, lr: 1.22359e-06, loss: 0.924047\n",
      "cur_epoch: 10/800, cur_it: 139/464, lr: 3.07791e-07, loss: 0.923565\n",
      "cur_epoch: 10/800, cur_it: 140/464, lr: 0.0, loss: 0.923196\n",
      "cur_epoch: 10/800, cur_it: 141/464, lr: 3.07791e-07, loss: 0.92331\n",
      "cur_epoch: 10/800, cur_it: 142/464, lr: 1.22359e-06, loss: 0.923211\n",
      "cur_epoch: 10/800, cur_it: 143/464, lr: 2.72484e-06, loss: 0.923936\n",
      "cur_epoch: 10/800, cur_it: 144/464, lr: 4.77458e-06, loss: 0.924089\n",
      "cur_epoch: 10/800, cur_it: 145/464, lr: 7.32233e-06, loss: 0.923608\n",
      "cur_epoch: 10/800, cur_it: 146/464, lr: 1.03054e-05, loss: 0.922922\n",
      "cur_epoch: 10/800, cur_it: 147/464, lr: 1.36502e-05, loss: 0.922759\n",
      "cur_epoch: 10/800, cur_it: 148/464, lr: 1.72746e-05, loss: 0.922409\n",
      "cur_epoch: 10/800, cur_it: 149/464, lr: 2.10891e-05, loss: 0.922605\n",
      "cur_epoch: 10/800, cur_it: 150/464, lr: 2.5e-05, loss: 0.921828\n",
      "cur_epoch: 10/800, cur_it: 151/464, lr: 2.89109e-05, loss: 0.921531\n",
      "cur_epoch: 10/800, cur_it: 152/464, lr: 3.27254e-05, loss: 0.921585\n",
      "cur_epoch: 10/800, cur_it: 153/464, lr: 3.63498e-05, loss: 0.921441\n",
      "cur_epoch: 10/800, cur_it: 154/464, lr: 3.96946e-05, loss: 0.920173\n",
      "cur_epoch: 10/800, cur_it: 155/464, lr: 4.26777e-05, loss: 0.919959\n",
      "cur_epoch: 10/800, cur_it: 156/464, lr: 4.52254e-05, loss: 0.92146\n",
      "cur_epoch: 10/800, cur_it: 157/464, lr: 4.72752e-05, loss: 0.921431\n",
      "cur_epoch: 10/800, cur_it: 158/464, lr: 4.87764e-05, loss: 0.921176\n",
      "cur_epoch: 10/800, cur_it: 159/464, lr: 4.96922e-05, loss: 0.921034\n",
      "cur_epoch: 10/800, cur_it: 160/464, lr: 5e-05, loss: 0.921147\n",
      "cur_epoch: 10/800, cur_it: 161/464, lr: 4.96922e-05, loss: 0.920953\n",
      "cur_epoch: 10/800, cur_it: 162/464, lr: 4.87764e-05, loss: 0.920714\n",
      "cur_epoch: 10/800, cur_it: 163/464, lr: 4.72752e-05, loss: 0.920468\n",
      "cur_epoch: 10/800, cur_it: 164/464, lr: 4.52254e-05, loss: 0.919892\n",
      "cur_epoch: 10/800, cur_it: 165/464, lr: 4.26777e-05, loss: 0.920212\n",
      "cur_epoch: 10/800, cur_it: 166/464, lr: 3.96946e-05, loss: 0.919754\n",
      "cur_epoch: 10/800, cur_it: 167/464, lr: 3.63498e-05, loss: 0.919753\n",
      "cur_epoch: 10/800, cur_it: 168/464, lr: 3.27254e-05, loss: 0.919427\n",
      "cur_epoch: 10/800, cur_it: 169/464, lr: 2.89109e-05, loss: 0.919481\n",
      "cur_epoch: 10/800, cur_it: 170/464, lr: 2.5e-05, loss: 0.920083\n",
      "cur_epoch: 10/800, cur_it: 171/464, lr: 2.10891e-05, loss: 0.919961\n",
      "cur_epoch: 10/800, cur_it: 172/464, lr: 1.72746e-05, loss: 0.920361\n",
      "cur_epoch: 10/800, cur_it: 173/464, lr: 1.36502e-05, loss: 0.920495\n",
      "cur_epoch: 10/800, cur_it: 174/464, lr: 1.03054e-05, loss: 0.920696\n",
      "cur_epoch: 10/800, cur_it: 175/464, lr: 7.32233e-06, loss: 0.920971\n",
      "cur_epoch: 10/800, cur_it: 176/464, lr: 4.77458e-06, loss: 0.920458\n",
      "cur_epoch: 10/800, cur_it: 177/464, lr: 2.72484e-06, loss: 0.920544\n",
      "cur_epoch: 10/800, cur_it: 178/464, lr: 1.22359e-06, loss: 0.919712\n",
      "cur_epoch: 10/800, cur_it: 179/464, lr: 3.07791e-07, loss: 0.920537\n",
      "cur_epoch: 10/800, cur_it: 180/464, lr: 0.0, loss: 0.920382\n",
      "cur_epoch: 10/800, cur_it: 181/464, lr: 3.07791e-07, loss: 0.920424\n",
      "cur_epoch: 10/800, cur_it: 182/464, lr: 1.22359e-06, loss: 0.920324\n",
      "cur_epoch: 10/800, cur_it: 183/464, lr: 2.72484e-06, loss: 0.92009\n",
      "cur_epoch: 10/800, cur_it: 184/464, lr: 4.77458e-06, loss: 0.92016\n",
      "cur_epoch: 10/800, cur_it: 185/464, lr: 7.32233e-06, loss: 0.920295\n",
      "cur_epoch: 10/800, cur_it: 186/464, lr: 1.03054e-05, loss: 0.91993\n",
      "cur_epoch: 10/800, cur_it: 187/464, lr: 1.36502e-05, loss: 0.919318\n",
      "cur_epoch: 10/800, cur_it: 188/464, lr: 1.72746e-05, loss: 0.919373\n",
      "cur_epoch: 10/800, cur_it: 189/464, lr: 2.10891e-05, loss: 0.919123\n",
      "cur_epoch: 10/800, cur_it: 190/464, lr: 2.5e-05, loss: 0.918748\n",
      "cur_epoch: 10/800, cur_it: 191/464, lr: 2.89109e-05, loss: 0.917953\n",
      "cur_epoch: 10/800, cur_it: 192/464, lr: 3.27254e-05, loss: 0.917749\n",
      "cur_epoch: 10/800, cur_it: 193/464, lr: 3.63498e-05, loss: 0.918369\n",
      "cur_epoch: 10/800, cur_it: 194/464, lr: 3.96946e-05, loss: 0.918713\n",
      "cur_epoch: 10/800, cur_it: 195/464, lr: 4.26777e-05, loss: 0.919136\n",
      "cur_epoch: 10/800, cur_it: 196/464, lr: 4.52254e-05, loss: 0.918977\n",
      "cur_epoch: 10/800, cur_it: 197/464, lr: 4.72752e-05, loss: 0.91896\n",
      "cur_epoch: 10/800, cur_it: 198/464, lr: 4.87764e-05, loss: 0.918538\n",
      "cur_epoch: 10/800, cur_it: 199/464, lr: 4.96922e-05, loss: 0.918619\n",
      "cur_epoch: 10/800, cur_it: 200/464, lr: 5e-05, loss: 0.918262\n",
      "cur_epoch: 10/800, cur_it: 201/464, lr: 4.96922e-05, loss: 0.918157\n",
      "cur_epoch: 10/800, cur_it: 202/464, lr: 4.87764e-05, loss: 0.917874\n",
      "cur_epoch: 10/800, cur_it: 203/464, lr: 4.72752e-05, loss: 0.917767\n",
      "cur_epoch: 10/800, cur_it: 204/464, lr: 4.52254e-05, loss: 0.917971\n",
      "cur_epoch: 10/800, cur_it: 205/464, lr: 4.26777e-05, loss: 0.918368\n",
      "cur_epoch: 10/800, cur_it: 206/464, lr: 3.96946e-05, loss: 0.91845\n",
      "cur_epoch: 10/800, cur_it: 207/464, lr: 3.63498e-05, loss: 0.91848\n",
      "cur_epoch: 10/800, cur_it: 208/464, lr: 3.27254e-05, loss: 0.918507\n",
      "cur_epoch: 10/800, cur_it: 209/464, lr: 2.89109e-05, loss: 0.918875\n",
      "cur_epoch: 10/800, cur_it: 210/464, lr: 2.5e-05, loss: 0.918829\n",
      "cur_epoch: 10/800, cur_it: 211/464, lr: 2.10891e-05, loss: 0.918492\n",
      "cur_epoch: 10/800, cur_it: 212/464, lr: 1.72746e-05, loss: 0.919014\n",
      "cur_epoch: 10/800, cur_it: 213/464, lr: 1.36502e-05, loss: 0.918959\n",
      "cur_epoch: 10/800, cur_it: 214/464, lr: 1.03054e-05, loss: 0.918735\n",
      "cur_epoch: 10/800, cur_it: 215/464, lr: 7.32233e-06, loss: 0.91867\n",
      "cur_epoch: 10/800, cur_it: 216/464, lr: 4.77458e-06, loss: 0.918527\n",
      "cur_epoch: 10/800, cur_it: 217/464, lr: 2.72484e-06, loss: 0.918328\n",
      "cur_epoch: 10/800, cur_it: 218/464, lr: 1.22359e-06, loss: 0.918523\n",
      "cur_epoch: 10/800, cur_it: 219/464, lr: 3.07791e-07, loss: 0.918859\n",
      "cur_epoch: 10/800, cur_it: 220/464, lr: 0.0, loss: 0.919129\n",
      "cur_epoch: 10/800, cur_it: 221/464, lr: 3.07791e-07, loss: 0.919292\n",
      "cur_epoch: 10/800, cur_it: 222/464, lr: 1.22359e-06, loss: 0.919755\n",
      "cur_epoch: 10/800, cur_it: 223/464, lr: 2.72484e-06, loss: 0.919511\n",
      "cur_epoch: 10/800, cur_it: 224/464, lr: 4.77458e-06, loss: 0.919183\n",
      "cur_epoch: 10/800, cur_it: 225/464, lr: 7.32233e-06, loss: 0.919112\n",
      "cur_epoch: 10/800, cur_it: 226/464, lr: 1.03054e-05, loss: 0.919344\n",
      "cur_epoch: 10/800, cur_it: 227/464, lr: 1.36502e-05, loss: 0.919643\n",
      "cur_epoch: 10/800, cur_it: 228/464, lr: 1.72746e-05, loss: 0.9196\n",
      "cur_epoch: 10/800, cur_it: 229/464, lr: 2.10891e-05, loss: 0.919623\n",
      "cur_epoch: 10/800, cur_it: 230/464, lr: 2.5e-05, loss: 0.919526\n",
      "cur_epoch: 10/800, cur_it: 231/464, lr: 2.89109e-05, loss: 0.919506\n",
      "cur_epoch: 10/800, cur_it: 232/464, lr: 3.27254e-05, loss: 0.919639\n",
      "cur_epoch: 10/800, cur_it: 233/464, lr: 3.63498e-05, loss: 0.919801\n",
      "cur_epoch: 10/800, cur_it: 234/464, lr: 3.96946e-05, loss: 0.919599\n",
      "cur_epoch: 10/800, cur_it: 235/464, lr: 4.26777e-05, loss: 0.919695\n",
      "cur_epoch: 10/800, cur_it: 236/464, lr: 4.52254e-05, loss: 0.919891\n",
      "cur_epoch: 10/800, cur_it: 237/464, lr: 4.72752e-05, loss: 0.919905\n",
      "cur_epoch: 10/800, cur_it: 238/464, lr: 4.87764e-05, loss: 0.919877\n",
      "cur_epoch: 10/800, cur_it: 239/464, lr: 4.96922e-05, loss: 0.919743\n",
      "cur_epoch: 10/800, cur_it: 240/464, lr: 5e-05, loss: 0.919606\n",
      "cur_epoch: 10/800, cur_it: 241/464, lr: 4.96922e-05, loss: 0.919482\n",
      "cur_epoch: 10/800, cur_it: 242/464, lr: 4.87764e-05, loss: 0.919413\n",
      "cur_epoch: 10/800, cur_it: 243/464, lr: 4.72752e-05, loss: 0.919721\n",
      "cur_epoch: 10/800, cur_it: 244/464, lr: 4.52254e-05, loss: 0.919985\n",
      "cur_epoch: 10/800, cur_it: 245/464, lr: 4.26777e-05, loss: 0.919805\n",
      "cur_epoch: 10/800, cur_it: 246/464, lr: 3.96946e-05, loss: 0.919784\n",
      "cur_epoch: 10/800, cur_it: 247/464, lr: 3.63498e-05, loss: 0.919409\n",
      "cur_epoch: 10/800, cur_it: 248/464, lr: 3.27254e-05, loss: 0.919674\n",
      "cur_epoch: 10/800, cur_it: 249/464, lr: 2.89109e-05, loss: 0.920016\n",
      "cur_epoch: 10/800, cur_it: 250/464, lr: 2.5e-05, loss: 0.920113\n",
      "cur_epoch: 10/800, cur_it: 251/464, lr: 2.10891e-05, loss: 0.920247\n",
      "cur_epoch: 10/800, cur_it: 252/464, lr: 1.72746e-05, loss: 0.920619\n",
      "cur_epoch: 10/800, cur_it: 253/464, lr: 1.36502e-05, loss: 0.920627\n",
      "cur_epoch: 10/800, cur_it: 254/464, lr: 1.03054e-05, loss: 0.920593\n",
      "cur_epoch: 10/800, cur_it: 255/464, lr: 7.32233e-06, loss: 0.920472\n",
      "cur_epoch: 10/800, cur_it: 256/464, lr: 4.77458e-06, loss: 0.920498\n",
      "cur_epoch: 10/800, cur_it: 257/464, lr: 2.72484e-06, loss: 0.920233\n",
      "cur_epoch: 10/800, cur_it: 258/464, lr: 1.22359e-06, loss: 0.920642\n",
      "cur_epoch: 10/800, cur_it: 259/464, lr: 3.07791e-07, loss: 0.920702\n",
      "cur_epoch: 10/800, cur_it: 260/464, lr: 0.0, loss: 0.920654\n",
      "cur_epoch: 10/800, cur_it: 261/464, lr: 3.07791e-07, loss: 0.920113\n",
      "cur_epoch: 10/800, cur_it: 262/464, lr: 1.22359e-06, loss: 0.920346\n",
      "cur_epoch: 10/800, cur_it: 263/464, lr: 2.72484e-06, loss: 0.920214\n",
      "cur_epoch: 10/800, cur_it: 264/464, lr: 4.77458e-06, loss: 0.92024\n",
      "cur_epoch: 10/800, cur_it: 265/464, lr: 7.32233e-06, loss: 0.920119\n",
      "cur_epoch: 10/800, cur_it: 266/464, lr: 1.03054e-05, loss: 0.920049\n",
      "cur_epoch: 10/800, cur_it: 267/464, lr: 1.36502e-05, loss: 0.919794\n",
      "cur_epoch: 10/800, cur_it: 268/464, lr: 1.72746e-05, loss: 0.919601\n",
      "cur_epoch: 10/800, cur_it: 269/464, lr: 2.10891e-05, loss: 0.919441\n",
      "cur_epoch: 10/800, cur_it: 270/464, lr: 2.5e-05, loss: 0.919273\n",
      "cur_epoch: 10/800, cur_it: 271/464, lr: 2.89109e-05, loss: 0.919917\n",
      "cur_epoch: 10/800, cur_it: 272/464, lr: 3.27254e-05, loss: 0.919954\n",
      "cur_epoch: 10/800, cur_it: 273/464, lr: 3.63498e-05, loss: 0.920173\n",
      "cur_epoch: 10/800, cur_it: 274/464, lr: 3.96946e-05, loss: 0.919818\n",
      "cur_epoch: 10/800, cur_it: 275/464, lr: 4.26777e-05, loss: 0.919852\n",
      "cur_epoch: 10/800, cur_it: 276/464, lr: 4.52254e-05, loss: 0.919882\n",
      "cur_epoch: 10/800, cur_it: 277/464, lr: 4.72752e-05, loss: 0.919623\n",
      "cur_epoch: 10/800, cur_it: 278/464, lr: 4.87764e-05, loss: 0.919771\n",
      "cur_epoch: 10/800, cur_it: 279/464, lr: 4.96922e-05, loss: 0.919794\n",
      "cur_epoch: 10/800, cur_it: 280/464, lr: 5e-05, loss: 0.919488\n",
      "cur_epoch: 10/800, cur_it: 281/464, lr: 4.96922e-05, loss: 0.919627\n",
      "cur_epoch: 10/800, cur_it: 282/464, lr: 4.87764e-05, loss: 0.919647\n",
      "cur_epoch: 10/800, cur_it: 283/464, lr: 4.72752e-05, loss: 0.919609\n",
      "cur_epoch: 10/800, cur_it: 284/464, lr: 4.52254e-05, loss: 0.919663\n",
      "cur_epoch: 10/800, cur_it: 285/464, lr: 4.26777e-05, loss: 0.91994\n",
      "cur_epoch: 10/800, cur_it: 286/464, lr: 3.96946e-05, loss: 0.919742\n",
      "cur_epoch: 10/800, cur_it: 287/464, lr: 3.63498e-05, loss: 0.919621\n",
      "cur_epoch: 10/800, cur_it: 288/464, lr: 3.27254e-05, loss: 0.920015\n",
      "cur_epoch: 10/800, cur_it: 289/464, lr: 2.89109e-05, loss: 0.919984\n",
      "cur_epoch: 10/800, cur_it: 290/464, lr: 2.5e-05, loss: 0.919482\n",
      "cur_epoch: 10/800, cur_it: 291/464, lr: 2.10891e-05, loss: 0.919469\n",
      "cur_epoch: 10/800, cur_it: 292/464, lr: 1.72746e-05, loss: 0.919067\n",
      "cur_epoch: 10/800, cur_it: 293/464, lr: 1.36502e-05, loss: 0.91906\n",
      "cur_epoch: 10/800, cur_it: 294/464, lr: 1.03054e-05, loss: 0.918788\n",
      "cur_epoch: 10/800, cur_it: 295/464, lr: 7.32233e-06, loss: 0.918712\n",
      "cur_epoch: 10/800, cur_it: 296/464, lr: 4.77458e-06, loss: 0.919429\n",
      "cur_epoch: 10/800, cur_it: 297/464, lr: 2.72484e-06, loss: 0.91936\n",
      "cur_epoch: 10/800, cur_it: 298/464, lr: 1.22359e-06, loss: 0.919036\n",
      "cur_epoch: 10/800, cur_it: 299/464, lr: 3.07791e-07, loss: 0.918726\n",
      "cur_epoch: 10/800, cur_it: 300/464, lr: 0.0, loss: 0.91859\n",
      "cur_epoch: 10/800, cur_it: 301/464, lr: 3.07791e-07, loss: 0.918471\n",
      "cur_epoch: 10/800, cur_it: 302/464, lr: 1.22359e-06, loss: 0.918603\n",
      "cur_epoch: 10/800, cur_it: 303/464, lr: 2.72484e-06, loss: 0.918715\n",
      "cur_epoch: 10/800, cur_it: 304/464, lr: 4.77458e-06, loss: 0.918529\n",
      "cur_epoch: 10/800, cur_it: 305/464, lr: 7.32233e-06, loss: 0.918577\n",
      "cur_epoch: 10/800, cur_it: 306/464, lr: 1.03054e-05, loss: 0.918424\n",
      "cur_epoch: 10/800, cur_it: 307/464, lr: 1.36502e-05, loss: 0.918486\n",
      "cur_epoch: 10/800, cur_it: 308/464, lr: 1.72746e-05, loss: 0.918325\n",
      "cur_epoch: 10/800, cur_it: 309/464, lr: 2.10891e-05, loss: 0.918246\n",
      "cur_epoch: 10/800, cur_it: 310/464, lr: 2.5e-05, loss: 0.918265\n",
      "cur_epoch: 10/800, cur_it: 311/464, lr: 2.89109e-05, loss: 0.918168\n",
      "cur_epoch: 10/800, cur_it: 312/464, lr: 3.27254e-05, loss: 0.918426\n",
      "cur_epoch: 10/800, cur_it: 313/464, lr: 3.63498e-05, loss: 0.918291\n",
      "cur_epoch: 10/800, cur_it: 314/464, lr: 3.96946e-05, loss: 0.918425\n",
      "cur_epoch: 10/800, cur_it: 315/464, lr: 4.26777e-05, loss: 0.918417\n",
      "cur_epoch: 10/800, cur_it: 316/464, lr: 4.52254e-05, loss: 0.9184\n",
      "cur_epoch: 10/800, cur_it: 317/464, lr: 4.72752e-05, loss: 0.918415\n",
      "cur_epoch: 10/800, cur_it: 318/464, lr: 4.87764e-05, loss: 0.917883\n",
      "cur_epoch: 10/800, cur_it: 319/464, lr: 4.96922e-05, loss: 0.917991\n",
      "cur_epoch: 10/800, cur_it: 320/464, lr: 5e-05, loss: 0.917971\n",
      "cur_epoch: 10/800, cur_it: 321/464, lr: 4.96922e-05, loss: 0.917658\n",
      "cur_epoch: 10/800, cur_it: 322/464, lr: 4.87764e-05, loss: 0.917732\n",
      "cur_epoch: 10/800, cur_it: 323/464, lr: 4.72752e-05, loss: 0.917848\n",
      "cur_epoch: 10/800, cur_it: 324/464, lr: 4.52254e-05, loss: 0.9178\n",
      "cur_epoch: 10/800, cur_it: 325/464, lr: 4.26777e-05, loss: 0.91794\n",
      "cur_epoch: 10/800, cur_it: 326/464, lr: 3.96946e-05, loss: 0.917762\n",
      "cur_epoch: 10/800, cur_it: 327/464, lr: 3.63498e-05, loss: 0.917655\n",
      "cur_epoch: 10/800, cur_it: 328/464, lr: 3.27254e-05, loss: 0.917704\n",
      "cur_epoch: 10/800, cur_it: 329/464, lr: 2.89109e-05, loss: 0.917738\n",
      "cur_epoch: 10/800, cur_it: 330/464, lr: 2.5e-05, loss: 0.917842\n",
      "cur_epoch: 10/800, cur_it: 331/464, lr: 2.10891e-05, loss: 0.918121\n",
      "cur_epoch: 10/800, cur_it: 332/464, lr: 1.72746e-05, loss: 0.918102\n",
      "cur_epoch: 10/800, cur_it: 333/464, lr: 1.36502e-05, loss: 0.918009\n",
      "cur_epoch: 10/800, cur_it: 334/464, lr: 1.03054e-05, loss: 0.917762\n",
      "cur_epoch: 10/800, cur_it: 335/464, lr: 7.32233e-06, loss: 0.918157\n",
      "cur_epoch: 10/800, cur_it: 336/464, lr: 4.77458e-06, loss: 0.917884\n",
      "cur_epoch: 10/800, cur_it: 337/464, lr: 2.72484e-06, loss: 0.917582\n",
      "cur_epoch: 10/800, cur_it: 338/464, lr: 1.22359e-06, loss: 0.917378\n",
      "cur_epoch: 10/800, cur_it: 339/464, lr: 3.07791e-07, loss: 0.917381\n",
      "cur_epoch: 10/800, cur_it: 340/464, lr: 0.0, loss: 0.917419\n",
      "cur_epoch: 10/800, cur_it: 341/464, lr: 3.07791e-07, loss: 0.917549\n",
      "cur_epoch: 10/800, cur_it: 342/464, lr: 1.22359e-06, loss: 0.917574\n",
      "cur_epoch: 10/800, cur_it: 343/464, lr: 2.72484e-06, loss: 0.917465\n",
      "cur_epoch: 10/800, cur_it: 344/464, lr: 4.77458e-06, loss: 0.917476\n",
      "cur_epoch: 10/800, cur_it: 345/464, lr: 7.32233e-06, loss: 0.917406\n",
      "cur_epoch: 10/800, cur_it: 346/464, lr: 1.03054e-05, loss: 0.917324\n",
      "cur_epoch: 10/800, cur_it: 347/464, lr: 1.36502e-05, loss: 0.917362\n",
      "cur_epoch: 10/800, cur_it: 348/464, lr: 1.72746e-05, loss: 0.917295\n",
      "cur_epoch: 10/800, cur_it: 349/464, lr: 2.10891e-05, loss: 0.917172\n",
      "cur_epoch: 10/800, cur_it: 350/464, lr: 2.5e-05, loss: 0.91737\n",
      "cur_epoch: 10/800, cur_it: 351/464, lr: 2.89109e-05, loss: 0.917809\n",
      "cur_epoch: 10/800, cur_it: 352/464, lr: 3.27254e-05, loss: 0.917587\n",
      "cur_epoch: 10/800, cur_it: 353/464, lr: 3.63498e-05, loss: 0.917937\n",
      "cur_epoch: 10/800, cur_it: 354/464, lr: 3.96946e-05, loss: 0.917751\n",
      "cur_epoch: 10/800, cur_it: 355/464, lr: 4.26777e-05, loss: 0.917786\n",
      "cur_epoch: 10/800, cur_it: 356/464, lr: 4.52254e-05, loss: 0.917887\n",
      "cur_epoch: 10/800, cur_it: 357/464, lr: 4.72752e-05, loss: 0.917839\n",
      "cur_epoch: 10/800, cur_it: 358/464, lr: 4.87764e-05, loss: 0.917687\n",
      "cur_epoch: 10/800, cur_it: 359/464, lr: 4.96922e-05, loss: 0.917567\n",
      "cur_epoch: 10/800, cur_it: 360/464, lr: 5e-05, loss: 0.917332\n",
      "cur_epoch: 10/800, cur_it: 361/464, lr: 4.96922e-05, loss: 0.917484\n",
      "cur_epoch: 10/800, cur_it: 362/464, lr: 4.87764e-05, loss: 0.917421\n",
      "cur_epoch: 10/800, cur_it: 363/464, lr: 4.72752e-05, loss: 0.917212\n",
      "cur_epoch: 10/800, cur_it: 364/464, lr: 4.52254e-05, loss: 0.917284\n",
      "cur_epoch: 10/800, cur_it: 365/464, lr: 4.26777e-05, loss: 0.917264\n",
      "cur_epoch: 10/800, cur_it: 366/464, lr: 3.96946e-05, loss: 0.91769\n",
      "cur_epoch: 10/800, cur_it: 367/464, lr: 3.63498e-05, loss: 0.917691\n",
      "cur_epoch: 10/800, cur_it: 368/464, lr: 3.27254e-05, loss: 0.917763\n",
      "cur_epoch: 10/800, cur_it: 369/464, lr: 2.89109e-05, loss: 0.917885\n",
      "cur_epoch: 10/800, cur_it: 370/464, lr: 2.5e-05, loss: 0.917869\n",
      "cur_epoch: 10/800, cur_it: 371/464, lr: 2.10891e-05, loss: 0.918023\n",
      "cur_epoch: 10/800, cur_it: 372/464, lr: 1.72746e-05, loss: 0.917922\n",
      "cur_epoch: 10/800, cur_it: 373/464, lr: 1.36502e-05, loss: 0.917962\n",
      "cur_epoch: 10/800, cur_it: 374/464, lr: 1.03054e-05, loss: 0.917895\n",
      "cur_epoch: 10/800, cur_it: 375/464, lr: 7.32233e-06, loss: 0.917915\n",
      "cur_epoch: 10/800, cur_it: 376/464, lr: 4.77458e-06, loss: 0.917736\n",
      "cur_epoch: 10/800, cur_it: 377/464, lr: 2.72484e-06, loss: 0.917681\n",
      "cur_epoch: 10/800, cur_it: 378/464, lr: 1.22359e-06, loss: 0.917807\n",
      "cur_epoch: 10/800, cur_it: 379/464, lr: 3.07791e-07, loss: 0.91762\n",
      "cur_epoch: 10/800, cur_it: 380/464, lr: 0.0, loss: 0.917483\n",
      "cur_epoch: 10/800, cur_it: 381/464, lr: 3.07791e-07, loss: 0.917266\n",
      "cur_epoch: 10/800, cur_it: 382/464, lr: 1.22359e-06, loss: 0.916985\n",
      "cur_epoch: 10/800, cur_it: 383/464, lr: 2.72484e-06, loss: 0.916832\n",
      "cur_epoch: 10/800, cur_it: 384/464, lr: 4.77458e-06, loss: 0.916653\n",
      "cur_epoch: 10/800, cur_it: 385/464, lr: 7.32233e-06, loss: 0.916902\n",
      "cur_epoch: 10/800, cur_it: 386/464, lr: 1.03054e-05, loss: 0.916703\n",
      "cur_epoch: 10/800, cur_it: 387/464, lr: 1.36502e-05, loss: 0.916312\n",
      "cur_epoch: 10/800, cur_it: 388/464, lr: 1.72746e-05, loss: 0.916307\n",
      "cur_epoch: 10/800, cur_it: 389/464, lr: 2.10891e-05, loss: 0.9162\n",
      "cur_epoch: 10/800, cur_it: 390/464, lr: 2.5e-05, loss: 0.916224\n",
      "cur_epoch: 10/800, cur_it: 391/464, lr: 2.89109e-05, loss: 0.91601\n",
      "cur_epoch: 10/800, cur_it: 392/464, lr: 3.27254e-05, loss: 0.916015\n",
      "cur_epoch: 10/800, cur_it: 393/464, lr: 3.63498e-05, loss: 0.916037\n",
      "cur_epoch: 10/800, cur_it: 394/464, lr: 3.96946e-05, loss: 0.915921\n",
      "cur_epoch: 10/800, cur_it: 395/464, lr: 4.26777e-05, loss: 0.915907\n",
      "cur_epoch: 10/800, cur_it: 396/464, lr: 4.52254e-05, loss: 0.915854\n",
      "cur_epoch: 10/800, cur_it: 397/464, lr: 4.72752e-05, loss: 0.915837\n",
      "cur_epoch: 10/800, cur_it: 398/464, lr: 4.87764e-05, loss: 0.915834\n",
      "cur_epoch: 10/800, cur_it: 399/464, lr: 4.96922e-05, loss: 0.915818\n",
      "cur_epoch: 10/800, cur_it: 400/464, lr: 5e-05, loss: 0.915773\n",
      "cur_epoch: 10/800, cur_it: 401/464, lr: 4.96922e-05, loss: 0.915935\n",
      "cur_epoch: 10/800, cur_it: 402/464, lr: 4.87764e-05, loss: 0.916123\n",
      "cur_epoch: 10/800, cur_it: 403/464, lr: 4.72752e-05, loss: 0.915949\n",
      "cur_epoch: 10/800, cur_it: 404/464, lr: 4.52254e-05, loss: 0.915966\n",
      "cur_epoch: 10/800, cur_it: 405/464, lr: 4.26777e-05, loss: 0.915929\n",
      "cur_epoch: 10/800, cur_it: 406/464, lr: 3.96946e-05, loss: 0.916042\n",
      "cur_epoch: 10/800, cur_it: 407/464, lr: 3.63498e-05, loss: 0.91616\n",
      "cur_epoch: 10/800, cur_it: 408/464, lr: 3.27254e-05, loss: 0.916192\n",
      "cur_epoch: 10/800, cur_it: 409/464, lr: 2.89109e-05, loss: 0.916118\n",
      "cur_epoch: 10/800, cur_it: 410/464, lr: 2.5e-05, loss: 0.915998\n",
      "cur_epoch: 10/800, cur_it: 411/464, lr: 2.10891e-05, loss: 0.916052\n",
      "cur_epoch: 10/800, cur_it: 412/464, lr: 1.72746e-05, loss: 0.916215\n",
      "cur_epoch: 10/800, cur_it: 413/464, lr: 1.36502e-05, loss: 0.916082\n",
      "cur_epoch: 10/800, cur_it: 414/464, lr: 1.03054e-05, loss: 0.915947\n",
      "cur_epoch: 10/800, cur_it: 415/464, lr: 7.32233e-06, loss: 0.915809\n",
      "cur_epoch: 10/800, cur_it: 416/464, lr: 4.77458e-06, loss: 0.915697\n",
      "cur_epoch: 10/800, cur_it: 417/464, lr: 2.72484e-06, loss: 0.915711\n",
      "cur_epoch: 10/800, cur_it: 418/464, lr: 1.22359e-06, loss: 0.915625\n",
      "cur_epoch: 10/800, cur_it: 419/464, lr: 3.07791e-07, loss: 0.915646\n",
      "cur_epoch: 10/800, cur_it: 420/464, lr: 0.0, loss: 0.915424\n",
      "cur_epoch: 10/800, cur_it: 421/464, lr: 3.07791e-07, loss: 0.91528\n",
      "cur_epoch: 10/800, cur_it: 422/464, lr: 1.22359e-06, loss: 0.915265\n",
      "cur_epoch: 10/800, cur_it: 423/464, lr: 2.72484e-06, loss: 0.915147\n",
      "cur_epoch: 10/800, cur_it: 424/464, lr: 4.77458e-06, loss: 0.915116\n",
      "cur_epoch: 10/800, cur_it: 425/464, lr: 7.32233e-06, loss: 0.915319\n",
      "cur_epoch: 10/800, cur_it: 426/464, lr: 1.03054e-05, loss: 0.915369\n",
      "cur_epoch: 10/800, cur_it: 427/464, lr: 1.36502e-05, loss: 0.915119\n",
      "cur_epoch: 10/800, cur_it: 428/464, lr: 1.72746e-05, loss: 0.914981\n",
      "cur_epoch: 10/800, cur_it: 429/464, lr: 2.10891e-05, loss: 0.915438\n",
      "cur_epoch: 10/800, cur_it: 430/464, lr: 2.5e-05, loss: 0.915115\n",
      "cur_epoch: 10/800, cur_it: 431/464, lr: 2.89109e-05, loss: 0.915168\n",
      "cur_epoch: 10/800, cur_it: 432/464, lr: 3.27254e-05, loss: 0.914912\n",
      "cur_epoch: 10/800, cur_it: 433/464, lr: 3.63498e-05, loss: 0.914907\n",
      "cur_epoch: 10/800, cur_it: 434/464, lr: 3.96946e-05, loss: 0.914859\n",
      "cur_epoch: 10/800, cur_it: 435/464, lr: 4.26777e-05, loss: 0.914835\n",
      "cur_epoch: 10/800, cur_it: 436/464, lr: 4.52254e-05, loss: 0.914728\n",
      "cur_epoch: 10/800, cur_it: 437/464, lr: 4.72752e-05, loss: 0.914627\n",
      "cur_epoch: 10/800, cur_it: 438/464, lr: 4.87764e-05, loss: 0.914557\n",
      "cur_epoch: 10/800, cur_it: 439/464, lr: 4.96922e-05, loss: 0.914375\n",
      "cur_epoch: 10/800, cur_it: 440/464, lr: 5e-05, loss: 0.914145\n",
      "cur_epoch: 10/800, cur_it: 441/464, lr: 4.96922e-05, loss: 0.914066\n",
      "cur_epoch: 10/800, cur_it: 442/464, lr: 4.87764e-05, loss: 0.914201\n",
      "cur_epoch: 10/800, cur_it: 443/464, lr: 4.72752e-05, loss: 0.914276\n",
      "cur_epoch: 10/800, cur_it: 444/464, lr: 4.52254e-05, loss: 0.913967\n",
      "cur_epoch: 10/800, cur_it: 445/464, lr: 4.26777e-05, loss: 0.913722\n",
      "cur_epoch: 10/800, cur_it: 446/464, lr: 3.96946e-05, loss: 0.913756\n",
      "cur_epoch: 10/800, cur_it: 447/464, lr: 3.63498e-05, loss: 0.913474\n",
      "cur_epoch: 10/800, cur_it: 448/464, lr: 3.27254e-05, loss: 0.913396\n",
      "cur_epoch: 10/800, cur_it: 449/464, lr: 2.89109e-05, loss: 0.913417\n",
      "cur_epoch: 10/800, cur_it: 450/464, lr: 2.5e-05, loss: 0.913367\n",
      "cur_epoch: 10/800, cur_it: 451/464, lr: 2.10891e-05, loss: 0.913106\n",
      "cur_epoch: 10/800, cur_it: 452/464, lr: 1.72746e-05, loss: 0.913131\n",
      "cur_epoch: 10/800, cur_it: 453/464, lr: 1.36502e-05, loss: 0.913283\n",
      "cur_epoch: 10/800, cur_it: 454/464, lr: 1.03054e-05, loss: 0.913294\n",
      "cur_epoch: 10/800, cur_it: 455/464, lr: 7.32233e-06, loss: 0.912943\n",
      "cur_epoch: 10/800, cur_it: 456/464, lr: 4.77458e-06, loss: 0.912711\n",
      "cur_epoch: 10/800, cur_it: 457/464, lr: 2.72484e-06, loss: 0.912917\n",
      "cur_epoch: 10/800, cur_it: 458/464, lr: 1.22359e-06, loss: 0.912936\n",
      "cur_epoch: 10/800, cur_it: 459/464, lr: 3.07791e-07, loss: 0.912825\n",
      "cur_epoch: 10/800, cur_it: 460/464, lr: 0.0, loss: 0.912948\n",
      "cur_epoch: 10/800, cur_it: 461/464, lr: 3.07791e-07, loss: 0.913033\n",
      "cur_epoch: 10/800, cur_it: 462/464, lr: 1.22359e-06, loss: 0.912878\n",
      "cur_epoch: 10/800, cur_it: 463/464, lr: 2.72484e-06, loss: 0.912779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 11:23:04,743   INFO  **********************Start testing**********************\n",
      "2023-11-15 11:23:04,757   INFO  Loading KITTI dataset\n",
      "2023-11-15 11:23:04,891   INFO  Total samples for KITTI dataset: 3769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur_epoch: 10/800, cur_it: 464/464, lr: 4.77458e-06, loss: 0.912663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epochs:   0%|          | 0/791 [10:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 25782) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 25782) is killed by signal: Killed. ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 207\u001b[0m\n\u001b[1;32m    199\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**********************Start testing**********************\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    200\u001b[0m test_set, test_loader, sampler \u001b[38;5;241m=\u001b[39m build_dataloader(\n\u001b[1;32m    201\u001b[0m                                 dataset_cfg\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mDATA_CONFIG,\n\u001b[1;32m    202\u001b[0m                                 class_names\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mCLASS_NAMES,\n\u001b[1;32m    203\u001b[0m                                 batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    204\u001b[0m                                 dist\u001b[38;5;241m=\u001b[39mdist_train, workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mworkers, logger\u001b[38;5;241m=\u001b[39mlogger, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m                             )\n\u001b[0;32m--> 207\u001b[0m acc1, acc2, ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc1, acc2, ret)\n\u001b[1;32m    210\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./save_path/0722/ckpt_epoch\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cur_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, acc1, acc2)\n",
      "File \u001b[0;32m/mnt/workspace/sunqiao/OpenPCDet/tools/eval_utils/eval_utils_multinomial_finalv_half.py:28\u001b[0m, in \u001b[0;36meval_\u001b[0;34m(cfg, model, dataloader)\u001b[0m\n\u001b[1;32m     24\u001b[0m det_annos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     29\u001b[0m     load_data_to_gpu(batch_dict)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1315\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1315\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1317\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/home/pai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1175\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 25782) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pcdet.models_multinomial_half.detectors.pointpillar import PointPillar\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from test import repeat_eval_ckpt, eval_single_ckpt\n",
    "from noise import add_noise_to_weights\n",
    "import numba\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from pcdet.config import cfg, cfg_from_list, cfg_from_yaml_file, log_config_to_file\n",
    "from pcdet.datasets import build_dataloader\n",
    "# from pcdet.models import build_network, model_fn_decorator\n",
    "from pcdet.utils import common_utils\n",
    "\n",
    "from pcdet.models.detectors.pointpillar import PointPillar\n",
    "\n",
    "from train_utils.optimization import build_optimizer, build_scheduler\n",
    "# from train_utils.train_utils import train_model\n",
    "# from eval_utils import eval_utils_multinomial\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from eval_utils import eval_utils_multinomial_finalv_half as eval_utils\n",
    "\n",
    "\n",
    "\n",
    "def load_data_to_gpu(batch_dict):\n",
    "    for key, val in batch_dict.items():\n",
    "        if not isinstance(val, np.ndarray):\n",
    "            continue\n",
    "        elif key in ['frame_id', 'metadata', 'calib']:\n",
    "            continue\n",
    "        elif key in ['images']:\n",
    "            batch_dict[key] = image_to_tensor(val).float().cuda().contiguous()\n",
    "        elif key in ['image_shape']:\n",
    "            batch_dict[key] = torch.from_numpy(val).int().cuda()\n",
    "        else:\n",
    "            batch_dict[key] = torch.from_numpy(val).float().cuda()\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    parser.add_argument('--cfg_file', type=str, default='./cfgs/kitti_models/pointpillar_bayes.yaml', help='specify the config for training')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, required=False, help='batch size for training')\n",
    "    parser.add_argument('--workers', type=int, default=32, help='number of workers for dataloader')\n",
    "    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n",
    "    parser.add_argument('--pretrained_model', type=str, default=True, help='pretrained_model')\n",
    "    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm'], default='none')\n",
    "    parser.add_argument('--tcp_port', type=int, default=18888, help='tcp port for distrbuted training')\n",
    "    parser.add_argument('--sync_bn', action='store_true', default=False, help='whether to use sync bn')\n",
    "    parser.add_argument('--fix_random_seed', action='store_true', default=True, help='')\n",
    "    parser.add_argument('--ckpt_save_interval', type=int, default=80, help='number of training epochs')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='local rank for distributed training')\n",
    "    parser.add_argument('--max_ckpt_save_num', type=int, default=81, help='max number of saved checkpoint')\n",
    "    parser.add_argument('--merge_all_iters_to_one_epoch', action='store_true', default=False, help='')\n",
    "    parser.add_argument('--set', dest='set_cfgs', default=None, nargs=argparse.REMAINDER, help='set extra config keys if needed')\n",
    "    parser.add_argument('--max_waiting_mins', type=int, default=0, help='max waiting minutes')\n",
    "    parser.add_argument('--start_epoch', type=int, default=0, help='')\n",
    "    parser.add_argument('--save_to_file', action='store_true', default=False, help='')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=800, required=False, help='number of epochs to train for')\n",
    "    parser.add_argument('--ckpt', type=str, default='./save_path/0722/ckpt_epoch8-26.959004-38.293910.pth', help='checkpoint to start from')\n",
    "    # sunqiao/OpenPCDet/tools/save_path/0722/ckpt_epoch84.pth\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    cfg_from_yaml_file(args.cfg_file, cfg)\n",
    "    cfg.TAG = Path(args.cfg_file).stem\n",
    "    cfg.EXP_GROUP_PATH = '/'.join(args.cfg_file.split('/')[1: -1])  # remove 'cfgs' and 'xxxx.yaml'\n",
    "\n",
    "    if args.set_cfgs is not None:\n",
    "        cfg_from_list(args.set_cfgs, cfg)\n",
    "\n",
    "    return args, cfg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "    args, cfg = parse_config()\n",
    "    dist_train = False\n",
    "    total_gpus = 1\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_gpu = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    print('Using GPU:' + str(np.argmax(memory_gpu)))\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(np.argmax(memory_gpu))\n",
    "    os.system('rm tmp')\n",
    "\n",
    "\n",
    "    if args.batch_size is None:\n",
    "        args.batch_size = cfg.OPTIMIZATION.BATCH_SIZE_PER_GPU\n",
    "    else:\n",
    "        assert args.batch_size % total_gpus == 0, 'Batch size should match the number of gpus'\n",
    "        args.batch_size = args.batch_size // total_gpus\n",
    "\n",
    "    args.epochs = cfg.OPTIMIZATION.NUM_EPOCHS if args.epochs is None else args.epochs\n",
    "    \n",
    "    common_utils.set_random_seed(666)\n",
    "    save_path = './save_path/logger/'#/bayes/pointpillar/'+time.strftime('%m%d-%H%M',time.localtime(time.time()))+'/'\n",
    "\n",
    "    logger = common_utils.create_logger(save_path+'log.txt', rank=cfg.LOCAL_RANK)\n",
    "    \n",
    "    print(\"=============\")\n",
    "    p1 = 0.23\n",
    "    p2 = 0.77\n",
    "    p3 = 0.68\n",
    "    print(p1, p2, p3)\n",
    "    print(\"=============\")\n",
    "    \n",
    "\n",
    "    train_set, train_loader, train_sampler = build_dataloader(\n",
    "        dataset_cfg=cfg.DATA_CONFIG,\n",
    "        class_names=cfg.CLASS_NAMES,\n",
    "        batch_size=args.batch_size,\n",
    "        dist=dist_train, workers=args.workers,\n",
    "        logger=logger,\n",
    "        training=True,\n",
    "        merge_all_iters_to_one_epoch=args.merge_all_iters_to_one_epoch,\n",
    "        total_epochs=args.epochs\n",
    "    )\n",
    "\n",
    "    model = PointPillar(\n",
    "        model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=train_set\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(args.ckpt, map_location=torch.device('cuda:0')), strict=False)#['model_state']\n",
    "    # torch.load('./checkpoint_epoch_33_multinomial.pth')['model_state']\n",
    "    \n",
    "    model.cuda()\n",
    "\n",
    "    optim_cfg = cfg.OPTIMIZATION\n",
    "    optim_cfg.LR = 0.00005\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=optim_cfg.LR, weight_decay=optim_cfg.WEIGHT_DECAY)\n",
    "\n",
    "    start_epoch = 9\n",
    "    last_epoch = -1\n",
    "\n",
    "    model.train()  # before wrap to DistributedDataParallel to support fixed some parameters\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            m.eval()\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 20, eta_min=0, last_epoch=-1)\n",
    "    \n",
    "\n",
    "#     # -----------------------start training---------------------------\n",
    "    logger.info('**********************Start training %s/%s(%s)**********************'\n",
    "                % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "    \n",
    "    optim_cfg=cfg.OPTIMIZATION\n",
    "    total_epochs=args.epochs\n",
    "    rank=cfg.LOCAL_RANK\n",
    "    \n",
    "    with tqdm.trange(start_epoch, total_epochs, desc='epochs', dynamic_ncols=True, leave=(rank==0)) as tbar:\n",
    "        total_it_each_epoch = len(train_loader)\n",
    "        for cur_epoch in tbar:\n",
    "            dataloader_iter = iter(train_loader)\n",
    "            loss_list = []\n",
    "\n",
    "            for cur_it in range(total_it_each_epoch):\n",
    "                batch = next(dataloader_iter)\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                load_data_to_gpu(batch)\n",
    "                ret_dict, tb_dict, disp_dict = model(batch)\n",
    "\n",
    "                loss = ret_dict['loss'].mean()\n",
    "        \n",
    "                loss_list.append(loss)\n",
    "                loss_avg = sum(loss_list) / len(loss_list)\n",
    "\n",
    "                loss.backward()\n",
    "                # clip_grad_norm_(model.parameters(), optim_cfg.GRAD_NORM_CLIP)\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                print('cur_epoch: {}/{}, cur_it: {}/{}, lr: {:.6}, loss: {:.6}'.format(cur_epoch+1, total_epochs, cur_it+1, total_it_each_epoch, optimizer.param_groups[0]['lr'], loss_avg))\n",
    "                \n",
    "            logger.info('**********************Start testing**********************')\n",
    "            test_set, test_loader, sampler = build_dataloader(\n",
    "                                            dataset_cfg=cfg.DATA_CONFIG,\n",
    "                                            class_names=cfg.CLASS_NAMES,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "                                        )\n",
    "\n",
    "            acc1, acc2, ret = eval_utils.eval_(cfg, model, test_loader)\n",
    "            print(acc1, acc2, ret)\n",
    "\n",
    "            filename = './save_path/0722/ckpt_epoch{}-{:.6f}-{:.6f}.pth'.format(cur_epoch+1, acc1, acc2)\n",
    "            last_filename = filename\n",
    "            torch.save(model.state_dict(), filename)\n",
    "\n",
    "        \n",
    "    # filename = './save_path/0722/ckpt_epoch{}.pth'.format(0+1)\n",
    "    # torch.save(model.state_dict(), filename)        \n",
    "    \n",
    "#     logger.info('**********************Start testing %s/%s(%s)**********************'\n",
    "#                 % (cfg.EXP_GROUP_PATH, cfg.TAG, args.extra_tag))\n",
    "#     test_set, test_loader, sampler = build_dataloader(\n",
    "#                                     dataset_cfg=cfg.DATA_CONFIG,\n",
    "#                                     class_names=cfg.CLASS_NAMES,\n",
    "#                                     batch_size=args.batch_size,\n",
    "#                                     dist=dist_train, workers=args.workers, logger=logger, training=False\n",
    "#                                 )\n",
    "\n",
    "#     acc1, acc2, ret = eval_utils.eval_(cfg, model, test_loader)\n",
    "#     print(acc1, ac2, ret)\n",
    "\n",
    "    # logger.info('----------------Bayes Optimization----------------')\n",
    "    # Bounded region of parameter space\n",
    "    # pbounds = {'p1': (0.1, 0.9), 'p2': (0.1, 0.9)}\n",
    "    \n",
    "\n",
    "#     optimizer = BayesianOptimization(\n",
    "#         f=opt_function,\n",
    "#         pbounds=pbounds,\n",
    "#         verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "#         random_state=1,\n",
    "#     )\n",
    "#     optimizer.probe(\n",
    "#         params={'p1': 0.11, 'p2': 0.11},\n",
    "#         lazy=True,\n",
    "#     )\n",
    "\n",
    "#     logger_bayes = JSONLogger(path=save_path+\"logs2.json\")\n",
    "#     optimizer.subscribe(Events.OPTIMIZATION_STEP, logger_bayes)\n",
    "    \n",
    "    \n",
    "#     n = 0\n",
    "#     optimizer.maximize(\n",
    "#         init_points=3,\n",
    "#         n_iter=10,\n",
    "#     )\n",
    "    print(\"=======end========\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
